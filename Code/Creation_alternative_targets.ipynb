{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* CELEX: Nimm die SAMPA ground truth, konvertiere nach IPA mit dem selbstgebauten dict basierend auf der WIKI Tabelle. Dann nimm das IPA -> Buchstaben dict um die alternativen Schreibweisen zu generieren und gehe die manuell durch und sortiere aus\n",
    "* ChildLex: Nimm alle Wörter aus dem Korpus und schaue nach welche Wörter mit SAMPA im CELEX Korpus existieren. Diejenigen die es gibt: Easy, da SAMPA existiert und sogar schon in IPA umgewandelt und sogar alternative writings da. Diejenigen die es nicht gibt (vermutlich wenige): Nimm den Online Konverter um Text in SAMPA umzuwandeln (das ist die ground truth! Achte darauf, dass Alphabet so wie beim CELEX Korpus). Wandel diese Wörter dann in IPA um und generiere alternative Schreibweisen\n",
    "* Fibelwörter: Gleicher Ansatz wie bei ChildLex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jannisborn/anaconda3/envs/tf/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# Task: Come up with alternative spellings for all orthographic words in a database\n",
    "# Tool 1: IPA -> Graphem Konverter Wiki\n",
    "# Tool 2: SAMPA -> IPA Konverter Wiki\n",
    "\n",
    "# Idea: Take the phonetic sequence (SAMPA) of every word, convert it to an IPA sequence and\n",
    "        # then convert that to all grapheme sequences\n",
    "    \n",
    "# Needs: A dictionary for SAMPA -> IPA (ideally non-ambiguous). A dictionary for IPA -> Text (ambigu.)\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excluded 3873 words because they were too long (more than 15 phons)\n",
      "Size of dataset is 33395 samples\n"
     ]
    }
   ],
   "source": [
    "path = '/Users/jannisborn/Desktop/LDS_Data/celex2/german/gpl/gpl.cd'\n",
    "t=0\n",
    "with open(path, 'r') as file:\n",
    "\n",
    "    raw_data = file.read().splitlines()\n",
    "    words = []\n",
    "    phons = []\n",
    "\n",
    "    for ind,raw_line in enumerate(raw_data):\n",
    "\n",
    "        line = raw_line.split(\"\\\\\")\n",
    "\n",
    "        \n",
    "        if line[-2]: # Use only words that HAVE a SAMPA transcript (reduces from 51k to 37345)\n",
    "\n",
    "            # exclude foreign words that have the 'æ' tone (SAMPA '{' ) like in\n",
    "            # exclude foreign words that have the 'ɑ' tone (SAMPA 'A' )\n",
    "            if not 'A' in line[-2] and not '{' in line[-2] and not '~' in line[-2]: \n",
    "                if not ('tS' in line[-2] and not 'tsch' in line[1]):\n",
    "                    if len(line[-2]) <= 15:\n",
    "                        words.append(line[1].lower()) # Make spellings lowercase only\n",
    "                        phons.append(line[-2]) # Using SAMPA notation\n",
    "                    else:\n",
    "                        t+=1\n",
    "print(\"Excluded\",t, \"words because they were too long (more than 15 phons)\" )\n",
    "print(\"Size of dataset is\", len(words), \"samples\")\n",
    "\n",
    "\n",
    "## Helper Method:\n",
    "\n",
    "def str_to_num_dataset(X,Y):\n",
    "    \"\"\"\n",
    "    This method receives 2 lists of strings (input X and output Y) and converts it to padded, numerical arrays.\n",
    "    It returns the numerical dataset as well as the dictionaries to retrieve the strings.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Define dictionaries \n",
    "    # Dictionary assignining a unique integer to each input character\n",
    "    try:\n",
    "        u_characters = set(' '.join(X)) \n",
    "    except TypeError:\n",
    "        # Exception for TIMIT dataset (one phoneme is repr. by seq. of chars)\n",
    "        print(\"TypeError occurred.\")\n",
    "        u_characters = set([quant for seq in X for quant in seq])\n",
    "\n",
    "    char2numX = dict(zip(u_characters, range(len(u_characters))))\n",
    "\n",
    "    # Dictionary assignining a unique integer to each phoneme\n",
    "    try:\n",
    "        v_characters = set(' '.join(Y)) \n",
    "    except TypeError:\n",
    "        print(\"TypeError occurred.\")\n",
    "        v_characters = set([quant for seq in Y for quant in seq])\n",
    "    char2numY = dict(zip(v_characters, range(1,len(v_characters)+1))) # Using 0 causes trouble for tf.edit_distance\n",
    "    \n",
    "    # 2. Padding\n",
    "    # Pad inputs\n",
    "    char2numX['<GO>'] = len(char2numX) \n",
    "    char2numX['<PAD>']  = len(char2numX) \n",
    "    mx_l_X = max([len(word) for word in X]) # longest input sequence\n",
    "    # Padd all X for the final form for the LSTM\n",
    "    x = [[char2numX['<PAD>']]*(mx_l_X - len(word)) +[char2numX[char] for char in word] for word in X]\n",
    "    x = np.array(x) \n",
    "\n",
    "    # Pad targets\n",
    "    char2numY['<GO>'] = len(char2numY) # Define number denoting the response onset\n",
    "    char2numY['<PAD>'] = len(char2numY)  \n",
    "    mx_l_Y = max([len(phon_seq) for phon_seq in Y]) # longest output sequence\n",
    "\n",
    "    y = [[char2numY['<GO>']] + [char2numY['<PAD>']]*(mx_l_Y - len(ph_sq)) + [char2numY[phon] for phon in ph_sq] for ph_sq in Y]\n",
    "    y = np.array(y)\n",
    "\n",
    "    return ((x,y) , (char2numX,char2numY))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "((phons_num, words_num), (phon_dict, word_dict)) = str_to_num_dataset(phons,words)\n",
    "#print(len(word_dict),word_dict)\n",
    "#print(len(phon_dict),phon_dict)\n",
    "#print(words_num.shape, phons_num.shape)\n",
    "#print(words_num[321,:], phons_num[321,:])\n",
    "\n",
    "np.savez(\"data/celex.npz\", words=words_num, phons=phons_num, word_dict=word_dict, phon_dict=phon_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of phonetic dict is  40  and the keys are: \n",
      " ['S', ':', 'U', 'Z', '|', '+', 'a', 'o', '#', 'Y', '/', 'r', 'v', 'N', 'O', 'j', 'I', 'e', 'g', 't', 'k', 'f', 'p', 'n', 'E', 'u', 'x', 'b', 'd', 'i', ' ', 'h', 'm', 'z', 's', '@', 'y', 'l', '<GO>', '<PAD>']\n"
     ]
    }
   ],
   "source": [
    "data = np.load('data/celex.npz')\n",
    "sampa_dict = {key:data['phon_dict'].item().get(key) for key in data['phon_dict'].item()}\n",
    "sampa_keys = list(sampa_dict.keys()) # Has 43 keys originally, 40 after excluding {, ~, A\n",
    "print(\"Length of phonetic dict is \", len(sampa_dict), \" and the keys are: \\n\", sampa_keys) # Has 43 keys\n",
    "\n",
    "\n",
    "# Step 1: Make a sampa_ipa dict. How to: Go to SAMPA -> IPA tabelle, for every SAMPA char, check whether it \n",
    "# is in the CELEX korpus. If yes, look up example word from wiki in korpus and check whether it is the right sampa\n",
    "# sign. If yes, look up on wiktionary example word in IPA and check whether output sign is correct.\n",
    "\n",
    "sampa_ipa = dict()\n",
    "\n",
    "# Vowels\n",
    "sampa_ipa['i'] = 'i'\n",
    "sampa_ipa[':'] = 'ː'\n",
    "sampa_ipa['I'] = 'ɪ'\n",
    "sampa_ipa['e'] = 'e'\n",
    "sampa_ipa['E'] = 'ɛ'\n",
    "sampa_ipa['y'] = 'y'   # meaning a real ü like in kühl\n",
    "sampa_ipa['@'] = 'ə'\n",
    "sampa_ipa['a'] = 'a'\n",
    "sampa_ipa['u'] = 'u'\n",
    "sampa_ipa['U'] = 'ʊ'\n",
    "sampa_ipa['o'] = 'o'\n",
    "sampa_ipa['O'] = 'ɔ'\n",
    "\n",
    "\n",
    "# consonants\n",
    "sampa_ipa['p'] = 'p'\n",
    "sampa_ipa['b'] = 'b'\n",
    "sampa_ipa['t'] = 't'\n",
    "sampa_ipa['d'] = 'd'\n",
    "sampa_ipa['k'] = 'k'\n",
    "sampa_ipa['g'] = 'g'\n",
    "sampa_ipa['f'] = 'f'\n",
    "sampa_ipa['v'] = 'v'\n",
    "sampa_ipa['s'] = 's'\n",
    "sampa_ipa['z'] = 'z'\n",
    "sampa_ipa['S'] = 'ʃ'\n",
    "sampa_ipa['x'] = 'x'\n",
    "sampa_ipa['h'] = 'h'\n",
    "sampa_ipa['m'] = 'm'\n",
    "sampa_ipa['n'] = 'n'\n",
    "sampa_ipa['N'] = 'ŋ'\n",
    "sampa_ipa['l'] = 'l'\n",
    "sampa_ipa['r'] = 'r'\n",
    "sampa_ipa['j'] = 'j'\n",
    "sampa_ipa['Z'] = 'ʒ'\n",
    "sampa_ipa['+'] = ''  # meaning a bit unclear\n",
    "sampa_ipa['#'] = 'ˈ' # following syllabus carries primary intonation\n",
    "sampa_ipa['|'] = 'ø' # meaning a bit unclear\n",
    "sampa_ipa['/'] = 'œ' # usually SAMPA uses 9 instead of / for this \n",
    "sampa_ipa['Y'] = 'ʏ' # meaning more a 'oü' like in Müll\n",
    "\n",
    "# These are 37 keys only, so 6 are missing. Remaining ones are:\n",
    "\n",
    "# <GO>          not needed for alt. writing creation\n",
    "# <PAD>         not needed for alt. writing creation\n",
    "#    (SPACE)    not needed\n",
    "# {             excluded some foreign words\n",
    "# A             excluded some foreign words\n",
    "# ~             excluded some foreign words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now convert the SAMPA words into IPA words:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of IPA samples is 33395 . Some samples are: \n",
      "\n",
      " WORD         ===>        SAMPA        ===>        IPA\n",
      "strafen  =>  Stra:f  =>  ʃtraːf\n",
      "strafanstalt  =>  Stra:f#anStalt  =>  ʃtraːfˈanʃtalt\n",
      "strafarbeit  =>  Stra:f+@#arbait  =>  ʃtraːfəˈarbait\n",
      "strafbar  =>  Stra:f#ba:r  =>  ʃtraːfˈbaːr\n",
      "strafbefehl  =>  Stra:f+@#b@fe:l  =>  ʃtraːfəˈbəfeːl\n",
      "strafe  =>  Stra:f+@  =>  ʃtraːfə\n",
      "straferlass  =>  Stra:f+@#Er#las  =>  ʃtraːfəˈɛrˈlas\n",
      "straff  =>  Straf  =>  ʃtraf\n",
      "straffen  =>  Straf  =>  ʃtraf\n",
      "straffaellig  =>  Stra:f+@#fal+Ix  =>  ʃtraːfəˈfalɪx\n"
     ]
    }
   ],
   "source": [
    "ipa = []\n",
    "for samp in phons:\n",
    "    s = []\n",
    "    for char in samp:\n",
    "        s.append(sampa_ipa[char])\n",
    "    ipa.append(''.join(s))\n",
    "print(\"Amount of IPA samples is\", len(ipa), \". Some samples are: \\n\")\n",
    "print(\" WORD         ===>        SAMPA        ===>        IPA\")\n",
    "for k in range(25100, 25110):\n",
    "    print(words[k],\" => \", phons[k],\" => \", ipa[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now define the second dictionary, mapping IPA signs to graphemes (according to [here](https://de.wiktionary.org/wiki/Verzeichnis:Deutsch/Phoneme_und_Grapheme) ) \n",
    "\n",
    "Go to every phon and write down all graphemes (copying table, excluding very weird graphemes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_values(['i', 'ː', 'ɪ', 'e', 'ɛ', 'y', 'ə', 'a', 'u', 'ʊ', 'o', 'ɔ', 'p', 'b', 't', 'd', 'k', 'g', 'f', 'v', 's', 'z', 'ʃ', 'x', 'h', 'm', 'n', 'ŋ', 'l', 'r', 'j', 'ʒ', '', 'ˈ', 'ø', 'œ', 'ʏ'])\n"
     ]
    }
   ],
   "source": [
    "print(sampa_ipa.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37 46\n",
      "ː\n",
      "\n",
      "ˈ\n",
      "ø\n"
     ]
    }
   ],
   "source": [
    "ipa_graph = dict()\n",
    "ipa_graph['t'] = ['t', 'd', 'tt'] # excluded dt (mostly in Stadt) and th (Methode) -> hard but not do more than 3 opts.\n",
    "ipa_graph['ə'] = ['e']\n",
    "ipa_graph['n'] = ['n', 'nn']\n",
    "ipa_graph['s'] = ['s', 'ss'] # excluded t for Patience, ce for Farce, zz for Jazz, ß (no occ.), c for Sauce, z for Quiz\n",
    "ipa_graph['a'] = ['a', 'ah']\n",
    "ipa_graph['r'] = ['r', 'rr'] # excluded rrh for Zirrhose/Myrrhe, rh for Rhythmus\n",
    "ipa_graph['l'] = ['l', 'll']\n",
    "ipa_graph['ɛ'] = ['e', 'ae']\n",
    "ipa_graph['f'] = ['f', 'v', 'ff'] # excluded ph for Physik\n",
    "ipa_graph['g'] = ['g', 'gh'] # excluded gg for Bagger\n",
    "ipa_graph['ɪ'] = ['i']\n",
    "ipa_graph['k'] = ['k', 'ck', 'c'] # Excluded cch for Zucchini, gg for Flaggschiff, qu for Boutique, kk for Akkordeon\n",
    "                  #  qu chars are usually kv ipa (tracked below), g for Krieg, ch for Chor\n",
    "ipa_graph['m'] = ['m', 'mm']\n",
    "ipa_graph['b'] = ['b', 'bb']\n",
    "ipa_graph['ʃ'] = ['sch', 's'] # excluded sk for Ski, sh for Sheriff, Show and ch for Recherche \n",
    "ipa_graph['d'] = ['d','dd']\n",
    "ipa_graph['p'] = ['p', 'b', 'pp'] # excluded bb for abebben or schrubben\n",
    "ipa_graph['ŋ'] = ['ng','n']\n",
    "ipa_graph['ɔ'] = ['o'] # excluded ch for Chauffeur (very rare exception)\n",
    "ipa_graph['v'] = ['w', 'v']\n",
    "ipa_graph['ʊ'] = ['u']\n",
    "ipa_graph['z'] = ['s'] # excluded zz for Blizzard, Puzzle and z for zoomen, bulldozer (since only in foreign words)\n",
    "ipa_graph['h'] = ['h']\n",
    "ipa_graph['i'] = ['i'] # excluding y (Baby/Party/Hockey) only 10 words in corpus\n",
    "ipa_graph['ʏ'] = ['ue', 'u'] # The corpus is weird here and writes Druck as drʏk, i.e. \"Drück\" rather than drʊk\n",
    "ipa_graph['x'] = ['ch']\n",
    "ipa_graph['e'] = ['e'] # excluded ee for Kaffee since IPA would be eː\n",
    "ipa_graph['j'] = ['j', 'y']\n",
    "ipa_graph['u'] = ['u'] # excluded ou like in Boutique\n",
    "ipa_graph['o'] = ['o'] # not needed anyways since o always followed by ː\n",
    "ipa_graph['œ'] = ['oe']\n",
    "ipa_graph['y'] = ['y']\n",
    "ipa_graph['ʒ'] = ['g', 'j'] # no wiki entry, self generated. For Garage or Jury\n",
    "\n",
    "\n",
    "# 2 character keys:\n",
    "ipa_graph['ts'] = ['z', 'ts', 'tz'] # excluded zz for Pizza/Skizze, c for circa, Penicillin, tts for trittst\n",
    "            # t for Aktion, Negation, Infektion, Proportion, ...\n",
    "ipa_graph['aː'] = ['a', 'ah', 'aa']\n",
    "ipa_graph['ai'] = ['ei', 'ai'] # excluded ail for Detail, aill for Medaillon, aille for Medaille and y for Nylon\n",
    "ipa_graph['iː'] = ['ie', 'i', 'ih'] # excluded ieh for Entziehung\n",
    "ipa_graph['eː'] = ['e', 'ee', 'eh'] # excluded et like in Bidet\n",
    "ipa_graph['ɛː'] = ['ae', 'aeh']\n",
    "ipa_graph['uː'] = ['u', 'uh'] # excluded ou like in Ragout, Limousine and oo like on zoomen/Cartoon\n",
    "ipa_graph['oː'] = ['o', 'oh', 'oo'] # excluded au for aubergine/sauce and eau for plateau, Niveau\n",
    "ipa_graph['yː'] = ['ue', 'ueh', 'y'] # excluded uet like in Debüt and u like in deja-vu\n",
    "ipa_graph['ɔy'] = ['eu', 'aeu'] # instead of what wiki calls ɔɪ̯, excluded oi for Boiler and oy for Boykott\n",
    "ipa_graph['ks'] = ['chs', 'x', 'ks'] #excluded gs like in legst/bugsieren, ggs like in eggst (?), cks like in zwecks, gs (legst)\n",
    "ipa_graph['øː'] = ['oe', 'oeh'] # excluded eu like in Ingenieur and eue like in Queue (?)\n",
    "ipa_graph['kv'] = ['qu']\n",
    "\n",
    "\n",
    "print(len(sampa_ipa), len(ipa_graph))\n",
    "# We had 36 keys in sampa_ipa dict, now we have 46 already in ipa_graph due to 2-phoneme-groups\n",
    "# But still there are one-char-values in sampa_ipa which are not keys in ipa_graph. Let us print them:\n",
    "for key in sampa_ipa.values():\n",
    "    if key not in ipa_graph.keys():\n",
    "        print(key)\n",
    "        \n",
    "# Okay 4 are missing:\n",
    "ipa_graph['ː'] = [''] # not needed anyways since ː always occurs after vowel\n",
    "ipa_graph['ˈ'] = [''] # Just a pronounciation symbol, does not carry meaning for spelling\n",
    "# Then the empty string '' is not needed as key\n",
    "# Then ø only occurs followed by a ː\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pretty tough to draw the boundary between which phons rarely but \"regularly\" translate to grapheme strings ([k] to kk like in Akkordeon, Mokka is still regular?) whereas [ʃ] (sch) to ch like in Champagner, Recherche or Lunch is irregular?\n",
    "\n",
    "## Use the IPA->Graphem dict to create alternative writings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_word(word, ipa_graph):\n",
    "    \"\"\"\n",
    "    Splits up an IPA word into a list of lists each with the possible replacement grapheme for each phoneme\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    WORD       {list} in IPA notation\n",
    "    IPA_GRAPH  {dict} mapping IPA symbols to possible grapheme sequences\n",
    "    \n",
    "    Returns:\n",
    "    ---------\n",
    "    CHARS      {list} containing lists with possible grapheme sequences\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    chars = []\n",
    "    single_key = True\n",
    "    for ind in range(len(word)-1):\n",
    "        \n",
    "        if single_key: \n",
    "            if word[ind:ind+2] in ipa_graph:\n",
    "                chars.append(ipa_graph[word[ind:ind+2]])\n",
    "                single_key = False\n",
    "            else:\n",
    "                chars.append(ipa_graph[word[ind]])\n",
    "        else:\n",
    "            single_key = True\n",
    "            \n",
    "    chars.append(ipa_graph[word[ind+1]])\n",
    "    \n",
    "    return chars\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The cell below generates the alternative writings (takes some time...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0, 1, 0]\n",
      "{'[1 0 0 1 0]': 12, (1, 0, 0, 1, 0): 121}\n"
     ]
    }
   ],
   "source": [
    "a = np.array([1,0,0,1,0])\n",
    "d = dict()\n",
    "d[str(a)] = 12\n",
    "\n",
    "e = np.random.random((2,2,2,2,2))\n",
    "f = [np.asscalar(aa) for aa in a]\n",
    "print(f)\n",
    "d[tuple(a)] = 121\n",
    "print(d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.9 µs ± 1.93 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "d[np.str(a)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "633 ns ± 28.2 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "e[a[0],a[1],a[2],a[3], a[4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.04 µs ± 37 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "d[tuple(a)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently examining word  0\n",
      "0 0\n",
      "2 2\n",
      "6 4\n",
      "143 11\n",
      "191 13\n",
      "288 21\n",
      "575 44\n",
      "576 56\n",
      "2304 180\n",
      "Currently examining word  200\n",
      "5183 282\n",
      "Currently examining word  400\n",
      "5184 478\n",
      "Currently examining word  600\n",
      "Currently examining word  800\n",
      "10367 970\n",
      "Currently examining word  1000\n",
      "Currently examining word  1200\n",
      "Currently examining word  1400\n",
      "Currently examining word  1600\n",
      "Currently examining word  1800\n",
      "11663 1900\n",
      "Currently examining word  2000\n",
      "Currently examining word  2200\n",
      "Currently examining word  2400\n",
      "Currently examining word  2600\n",
      "Currently examining word  2800\n",
      "Currently examining word  3000\n",
      "Currently examining word  3200\n",
      "Currently examining word  3400\n",
      "Currently examining word  3600\n",
      "Currently examining word  3800\n",
      "Currently examining word  4000\n",
      "Currently examining word  4200\n",
      "Currently examining word  4400\n",
      "Currently examining word  4600\n",
      "Currently examining word  4800\n",
      "Currently examining word  5000\n",
      "Currently examining word  5200\n",
      "Currently examining word  5400\n",
      "Currently examining word  5600\n",
      "Currently examining word  5800\n",
      "23328 5891\n",
      "Currently examining word  6000\n",
      "Currently examining word  6200\n",
      "Currently examining word  6400\n",
      "Currently examining word  6600\n",
      "Currently examining word  6800\n",
      "Currently examining word  7000\n",
      "Currently examining word  7200\n",
      "Currently examining word  7400\n",
      "31103 7517\n",
      "Currently examining word  7600\n",
      "Currently examining word  7800\n",
      "Currently examining word  8000\n",
      "Currently examining word  8200\n",
      "Currently examining word  8400\n",
      "Currently examining word  8600\n",
      "Currently examining word  8800\n",
      "Currently examining word  9000\n",
      "Currently examining word  9200\n",
      "Currently examining word  9400\n",
      "Currently examining word  9600\n",
      "Currently examining word  9800\n",
      "Currently examining word  10000\n",
      "Currently examining word  10200\n",
      "Currently examining word  10400\n",
      "Currently examining word  10600\n",
      "Currently examining word  10800\n",
      "Currently examining word  11000\n",
      "Currently examining word  11200\n",
      "Currently examining word  11400\n",
      "Currently examining word  11600\n",
      "Currently examining word  11800\n",
      "Currently examining word  12000\n",
      "Currently examining word  12200\n",
      "Currently examining word  12400\n",
      "Currently examining word  12600\n",
      "Currently examining word  12800\n",
      "31104 12922\n",
      "Currently examining word  13000\n",
      "Currently examining word  13200\n",
      "Currently examining word  13400\n",
      "Currently examining word  13600\n",
      "Currently examining word  13800\n",
      "Currently examining word  14000\n",
      "Currently examining word  14200\n",
      "Currently examining word  14400\n",
      "Currently examining word  14600\n",
      "Currently examining word  14800\n",
      "Currently examining word  15000\n",
      "Currently examining word  15200\n",
      "Currently examining word  15400\n",
      "Currently examining word  15600\n",
      "Currently examining word  15800\n",
      "Currently examining word  16000\n",
      "Currently examining word  16200\n",
      "Currently examining word  16400\n",
      "Currently examining word  16600\n",
      "Currently examining word  16800\n",
      "Currently examining word  17000\n",
      "Currently examining word  17200\n",
      "Currently examining word  17400\n",
      "Currently examining word  17600\n",
      "Currently examining word  17800\n",
      "Currently examining word  18000\n",
      "Currently examining word  18200\n",
      "Currently examining word  18400\n",
      "Currently examining word  18600\n",
      "Currently examining word  18800\n",
      "Currently examining word  19000\n",
      "Currently examining word  19200\n",
      "Currently examining word  19400\n",
      "Currently examining word  19600\n",
      "Currently examining word  19800\n",
      "Currently examining word  20000\n",
      "Currently examining word  20200\n",
      "Currently examining word  20400\n",
      "Currently examining word  20600\n",
      "Currently examining word  20800\n",
      "Currently examining word  21000\n",
      "Currently examining word  21200\n",
      "Currently examining word  21400\n",
      "Currently examining word  21600\n",
      "Currently examining word  21800\n",
      "Currently examining word  22000\n",
      "Currently examining word  22200\n",
      "Currently examining word  22400\n",
      "Currently examining word  22600\n",
      "Currently examining word  22800\n",
      "Currently examining word  23000\n",
      "Currently examining word  23200\n",
      "Currently examining word  23400\n",
      "Currently examining word  23600\n",
      "Currently examining word  23800\n",
      "Currently examining word  24000\n",
      "Currently examining word  24200\n",
      "Currently examining word  24400\n",
      "Currently examining word  24600\n",
      "Currently examining word  24800\n",
      "Currently examining word  25000\n",
      "Currently examining word  25200\n",
      "Currently examining word  25400\n",
      "Currently examining word  25600\n",
      "Currently examining word  25800\n",
      "Currently examining word  26000\n",
      "Currently examining word  26200\n",
      "Currently examining word  26400\n",
      "Currently examining word  26600\n",
      "Currently examining word  26800\n",
      "Currently examining word  27000\n",
      "Currently examining word  27200\n",
      "Currently examining word  27400\n",
      "Currently examining word  27600\n",
      "Currently examining word  27800\n",
      "Currently examining word  28000\n",
      "Currently examining word  28200\n",
      "Currently examining word  28400\n",
      "Currently examining word  28600\n",
      "Currently examining word  28800\n",
      "Currently examining word  29000\n",
      "Currently examining word  29200\n",
      "Currently examining word  29400\n",
      "Currently examining word  29600\n",
      "Currently examining word  29800\n",
      "Currently examining word  30000\n",
      "Currently examining word  30200\n",
      "Currently examining word  30400\n",
      "Currently examining word  30600\n",
      "Currently examining word  30800\n",
      "Currently examining word  31000\n",
      "Currently examining word  31200\n",
      "Currently examining word  31400\n",
      "Currently examining word  31600\n",
      "Currently examining word  31800\n",
      "Currently examining word  32000\n",
      "Currently examining word  32200\n",
      "Currently examining word  32400\n",
      "Currently examining word  32600\n",
      "Currently examining word  32800\n",
      "Currently examining word  33000\n",
      "Currently examining word  33200\n",
      "DONE! Alternative writings generated for  33394 words. Resulting list has 33395 entries.\n"
     ]
    }
   ],
   "source": [
    "all_writings = []\n",
    "m = 0\n",
    "\n",
    "for ind,ip in enumerate(ipa):\n",
    "    if ind % 200 == 0:\n",
    "        print(\"Currently examining word \", ind)\n",
    "        \n",
    "    word_lists = split_word(ip, ipa_graph)\n",
    "    alt_write_raw = list(itertools.product(*word_lists))\n",
    "    alt_write = [''.join(a) for a in alt_write_raw]\n",
    "    try:\n",
    "        alt_write.remove(words[ind])\n",
    "    except ValueError:\n",
    "        _ = 1\n",
    "        \n",
    "    all_writings.append(alt_write)\n",
    "    \n",
    "    if len(alt_write) > m:\n",
    "        print(m,ind)\n",
    "        m = len(alt_write)\n",
    "        \n",
    "        \n",
    "print(\"DONE! Alternative writings generated for \", ind, \"words. Resulting list has\", len(all_writings), 'entries.')\n",
    "np.save('celex_all_writings', all_writings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract childlex data from downloaded CSV and clean it up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of words in childlex 9769\n",
      "Set of characters in childlex words is ['a', 'e', 'c', 'h', 'z', 'n', 'd', 'f', 'm', 'l', 'i', 'r', 'g', 's', 't', 'u', 'o', 'b', 'k', 'p', 'q', 'w', 'v', 'x', 'j', 'y']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "path = 'data/childlex_6-8_lemmata.csv'\n",
    "forbidden = \"_,.!?*:=&%- /\\\\–•»«…›‹()[]{}’'\"\n",
    "\n",
    "\n",
    "def rem_um(word):\n",
    "    \"\"\"\n",
    "    Converts a word with German umlauts (ü,ä,ö) into a word without\n",
    "    \"\"\"\n",
    "        \n",
    "    umlautfree = str()\n",
    "    for char in word:\n",
    "        if char == 'ä':\n",
    "            umlautfree += 'ae'\n",
    "        elif char == 'ü':\n",
    "            umlautfree += 'ue'\n",
    "        elif char == 'ö':\n",
    "            umlautfree += 'oe'\n",
    "        elif char == 'ß':\n",
    "            umlautfree += 'ss'\n",
    "        else:\n",
    "            umlautfree += char\n",
    "    return umlautfree\n",
    "\n",
    "\n",
    "with open(path, 'r') as csvfile:\n",
    "    \n",
    "    raw_file = csv.reader(csvfile,delimiter=';')\n",
    "    raw_data = []\n",
    "    \n",
    "    \n",
    "    for line in raw_file:\n",
    "        umlautfree = rem_um(line[0].lower())\n",
    "        \n",
    "       # data cleanup\n",
    "        has_digit = any(char.isdigit() for char in umlautfree)\n",
    "        has_sonderz = any(char in forbidden for char in umlautfree)\n",
    "        \n",
    "        if not has_digit and not has_sonderz:\n",
    "        \n",
    "            raw_data.append(umlautfree)\n",
    "        \n",
    "        \n",
    "    \n",
    "childlex_words = raw_data\n",
    "print(\"Amount of words in childlex\", len(childlex_words))\n",
    "    \n",
    "chars = []\n",
    "for word in childlex_words:\n",
    "    for char in word:\n",
    "        if char not in chars:\n",
    "            chars.append(char)\n",
    "    \n",
    "print(\"Set of characters in childlex words is\", chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look up phonetic sequences in CELEX Database. \n",
    "#### Problem: We do not have phonetic sequences yet. Idea: Retrieve as many phonetic sequences as possible from the CELEX databsae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "childlex_phons = []\n",
    "for ind,word in enumerate(childlex_words):\n",
    "    if word in words:\n",
    "        childlex_phons.append(phons[ind])\n",
    "    else:\n",
    "        childlex_phons.append('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From the 9769 CHILDLEX database 3821 were not retrieved\n"
     ]
    }
   ],
   "source": [
    "a =  childlex_phons.count('')\n",
    "print(\"From the\", len(childlex_phons),\"CHILDLEX database\", a,\"were not retrieved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def extract_celex(path):\n",
    "    \"\"\"\n",
    "    Reads in data from the CELEX corpus\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    PATH        {str} the path to the desired celex file, i.e. gpl.cd \n",
    "                    (contains orthography and phonology)\n",
    "    \n",
    "    Returns:\n",
    "    -----------\n",
    "    WORDS       {list} of words (length 51728) for gpl.cd\n",
    "    PHONS       {list} of phoneme sequences (length 51728) for gpl.cd\n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    with open(path, 'r') as file:\n",
    "\n",
    "        raw_data = file.read().splitlines()\n",
    "        words = []\n",
    "        phons = []\n",
    "        \n",
    "        for ind,raw_line in enumerate(raw_data):\n",
    "            \n",
    "            \n",
    "            line = raw_line.split(\"\\\\\")\n",
    "            words.append(line[1])\n",
    "            phons.append(line[-2]) # Using SAMPA notation\n",
    "                \n",
    "    return words, phons\n",
    "                               \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/Users/jannisborn/Desktop/LDS_Data/celex2/german/gpl/gpl.cd\"\n",
    "words, phons = extract_celex(path)\n",
    "\n",
    "print(words[30], phons[30])\n",
    "\n",
    "((w,p) , (word_dict, phon_dict)) = str_to_num_dataset(words,phons)\n",
    "\n",
    "print(w.shape, p.shape, len(word_dict), len(phon_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('celex.npz', words=w, phons=p, word_dict=word_dict, phon_dict=phon_dict)\n",
    "data = np.load('data/celex.npz')\n",
    "print(data['phons'].shape)\n",
    "print(data['words'].shape)\n",
    "print(data['phon_dict'])\n",
    "print(data['word_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "\"\"\"# Check which words are double in dataset\n",
    "for ind,word in enumerate(ipa):\n",
    "    ipa2 = ipa[:]\n",
    "    del ipa2[ind]\n",
    "    if word in ipa2:\n",
    "        print(word, words[ind],phons[ind])\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def BAS_json(path):\n",
    "    \"\"\"\n",
    "    This method receives a path for the BAS-SprecherInnen corpus and iterates through all JSON files in all subfolders.\n",
    "    It creates and returns a list of words and a list of pronounciations\n",
    "    \"\"\"\n",
    "    \n",
    "    import json, os\n",
    "\n",
    "    words = []\n",
    "    prons = []\n",
    "    ind = 0\n",
    "    # Read in filenames\n",
    "    for dirpath, dirnames, filenames in os.walk(path):\n",
    "        for filename in [f for f in filenames if f.endswith(\".json\")]:\n",
    "\n",
    "            if filename == 'SprecherInnen_DBconfig.json':\n",
    "                continue\n",
    "\n",
    "            # Open the json\n",
    "            with open(os.path.join(dirpath,filename)) as json_file:\n",
    "                data = json.load(json_file)\n",
    "\n",
    "                for item in data['levels'][1]['items']:\n",
    "                    words.append(item['labels'][0]['value'])\n",
    "                    prons.append(item['labels'][1]['value'])\n",
    "\n",
    "    return words,prons\n",
    "\n",
    "\n",
    "def clean_corpus_BAS_Sprecherinnen(words,prons):\n",
    "    \"\"\"\n",
    "    This method receives a list of words and a list of pronunciations of the BAS-Sprech. corpus and returns a cleaned dataset.\n",
    "    Clearning means:    1) Removing multiple occurrences of words       2) Remove misspellings and ambiguities\n",
    "                        3) Remove capitalization at begin of sentence   \n",
    "    Homophon words (Meer, mehr) are kept!\n",
    "\n",
    "    This method required manual inspection (once for each corpus).\n",
    "    Returns a condensed list of words and pronounciations (strings) that can be converted in numerical values next.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # First, we remove multiple occurrences.\n",
    "    # We cannot use set(words), set(prons) since some words are homophon 8(results in diff. lengths)\n",
    "    all_tups = []\n",
    "    for (w,p) in zip(words,prons):\n",
    "        all_tups.append(tuple((w,p)))\n",
    "    set_tup = set(all_tups)\n",
    "    print('Amount of non-unique words in corpus is ', len(all_tups))\n",
    "    unique_tups = dict(set_tup)\n",
    "    print('Amount of unique words in corpus is ', len(set_tup))\n",
    "\n",
    "    # Now we have removed multiple occurrences and we have a dict of tuples (word, pron)\n",
    "\n",
    "    def find_poss_mistakes(unique_tups):\n",
    "        \"\"\"\n",
    "        Receives a list of hopefully unique tupels (word,pron) and collect the tupels\n",
    "        which may have incorrect spelling/pronounciations.\n",
    "        \"\"\"\n",
    "        possible_mistakes = []\n",
    "        for key, val in unique_tups.items():\n",
    "            for keyy,vall in unique_tups.items():\n",
    "                if key != keyy and val == vall:\n",
    "                    # Detect multiple spellings of same pronounciation\n",
    "                    possible_mistakes.append((key,val, keyy, vall))\n",
    "                if key == keyy and val != vall:\n",
    "                    # Detect multiple pronounciations of same spelling\n",
    "                    possible_mistakes.append((key,val, keyy, vall))\n",
    "                    \n",
    "        return possible_mistakes\n",
    "        \n",
    "    poss_mist = find_poss_mistakes(unique_tups)\n",
    "    \"\"\"\n",
    "    print(\"+++ Possible mistakes are +++\")\n",
    "    for k in range(len(poss_mist)):\n",
    "        print(poss_mist[k][0],' -> ',poss_mist[k][1], \n",
    "              poss_mist[k][2],' -> ',poss_mist[k][2])\n",
    "    \"\"\"\n",
    "        \n",
    "    # Remove mistakes (after manual inspection)\n",
    "    unique_tups.pop('BäckerInnen') # removing as a duplicate of Bäckerinnen\n",
    "    unique_tups.pop('nu') # Duplicate of Nu\n",
    "    unique_tups.pop('Abonentinnen') # Misspelled\n",
    "    unique_tups.pop('Mit') # Duplicate of mit\n",
    "    unique_tups.pop('A') # Duplicate of ah\n",
    "    unique_tups.pop('Bei') # Duplicate of bei\n",
    "    unique_tups.pop('backwaren') # Duplicate of Backwaren\n",
    "    unique_tups.pop('-vertreterinnen') # Duplicate of Vertreterinnen\n",
    "    unique_tups.pop('leu') # Duplicate of Leu\n",
    "    unique_tups.pop('teil') # Duplicate of Teilt\n",
    "    unique_tups.pop('Un') # Duplicate of un\n",
    "    unique_tups.pop('Ver') # Duplicate of ver\n",
    "    unique_tups.pop('AutorInnen') # Duplicate of Autorinnen\n",
    "    unique_tups.pop('FreundInnen') # Duplicate of Freundinnen\n",
    "    unique_tups.pop('-pflegerin') # Duplicate of Pflegerin\n",
    "    unique_tups.pop('Neu') # Duplicate of neu\n",
    "    unique_tups.pop('re') # Duplicate of Re\n",
    "    unique_tups.pop('-kolleginnen') # Duplicate of Koleginnen\n",
    "    unique_tups.pop('-trinkerinnen') # Duplicate of Trinkerinnen\n",
    "    unique_tups.pop('Twitter-NutzerInnen') # Duplicate of Twitter-Nutzerinnen\n",
    "    unique_tups.pop('kommissionen') # Duplicate of Koleginnen\n",
    "    # Remaining: (Ihnen, ihnen), (dass,das), (Meer, mehr), (Ihres, ihres), (mal, Mal)\n",
    "\n",
    "    wordss = list(unique_tups.keys())\n",
    "    pronss = list(unique_tups.values())\n",
    "    print('After clearning ', len(wordss), ' different words remain')\n",
    "\n",
    "    return wordss, pronss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/jannisborn/Desktop/LDS_Data/BAS_SprecherInnen'\n",
    "words, prons = BAS_json(path)\n",
    "words, prons = clean_corpus_BAS_Sprecherinnen(words,prons)\n",
    "\n",
    "((x,y) , (char2numX, char2numY)) = str_to_num_dataset(words,prons)\n",
    "np.savez('BAS_G2P.npz', inputs=x,targets=y, inp_dict=char2numX,\n",
    "        tar_dict=char2numY)\n",
    "\n",
    "((x,y) , (char2numX, char2numY)) = str_to_num_dataset(prons,words)\n",
    "np.savez('BAS_P2G.npz', inputs=x,targets=y, inp_dict=char2numX,\n",
    "        tar_dict=char2numY)\n",
    "\n",
    "print(type(x), type(y), type(char2numX),type(char2numY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insight 1: If you have k different classes, then k is the third \n",
    "#    dimension of the logits and then the target matrix must not \n",
    "#    contain values higher than k-1 (i.e. labels are 0, ..., k)\n",
    "\n",
    "# Insight 2: sequence_loss expects unnormalized logits (BEFORE softmax!)\n",
    "# -> This makes testing for me harder, since I cannot simply use values\n",
    "# with a sum over 1 (will get normalized)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
