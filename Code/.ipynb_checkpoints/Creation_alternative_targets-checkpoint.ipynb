{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* CELEX: Nimm die SAMPA ground truth, konvertiere nach IPA mit dem selbstgebauten dict basierend auf der WIKI Tabelle. Dann nimm das IPA -> Buchstaben dict um die alternativen Schreibweisen zu generieren und gehe die manuell durch und sortiere aus\n",
    "* ChildLex: Nimm alle Wörter aus dem Korpus und schaue nach welche Wörter mit SAMPA im CELEX Korpus existieren. Diejenigen die es gibt: Easy, da SAMPA existiert und sogar schon in IPA umgewandelt und sogar alternative writings da. Diejenigen die es nicht gibt (vermutlich wenige): Nimm den Online Konverter um Text in SAMPA umzuwandeln (das ist die ground truth! Achte darauf, dass Alphabet so wie beim CELEX Korpus). Wandel diese Wörter dann in IPA um und generiere alternative Schreibweisen\n",
    "* Fibelwörter: Gleicher Ansatz wie bei ChildLex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jannisborn/anaconda3/envs/tf/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# Task: Come up with alternative spellings for all orthographic words in a database\n",
    "# Tool 1: IPA -> Graphem Konverter Wiki\n",
    "# Tool 2: SAMPA -> IPA Konverter Wiki\n",
    "\n",
    "# Idea: Take the phonetic sequence (SAMPA) of every word, convert it to an IPA sequence and\n",
    "        # then convert that to all grapheme sequences\n",
    "    \n",
    "# Needs: A dictionary for SAMPA -> IPA (ideally non-ambiguous). A dictionary for IPA -> Text (ambigu.)\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/jannisborn/Desktop/LDS_Data/celex2/german/gpl/gpl.cd'\n",
    "\n",
    "def extract_celex(path):\n",
    "    \"\"\"\n",
    "    Reads in data from the CELEX corpus\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    PATH        {str} the path to the desired celex file, i.e. gpl.cd \n",
    "                    (contains orthography and phonology)\n",
    "\n",
    "    Returns:\n",
    "    -----------\n",
    "    2 Tuples, each with 2 variables. \n",
    "        First tuple:\n",
    "    W           {np.array} of words (length 51728) for gpl.cd\n",
    "    P           {np.array} of phoneme sequences (length 51728) for gpl.cd\n",
    "        Second tuple:\n",
    "    WORD_DICT   {dict} allowing to map the numerical array W back to strings\n",
    "    PHON_DICT   {dict} doing the same for the phonetical arrays P\n",
    "\n",
    "    \n",
    "    Call via:\n",
    "    path = \"/Users/jannisborn/Desktop/LDS_Data/celex2/german/gpl/gpl.cd\"\n",
    "    ((w,p) , (word_dict, phon_dict)) = extract_celex(path)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    with open(path, 'r') as file:\n",
    "\n",
    "        raw_data = file.read().splitlines()\n",
    "        words = []\n",
    "        phons = []\n",
    "        m = 0\n",
    "        t = 0\n",
    "        for ind,raw_line in enumerate(raw_data):\n",
    "            \n",
    "            line = raw_line.split(\"\\\\\")\n",
    "            \n",
    "            \n",
    "\n",
    "            if line[-2]: # Use only words that HAVE a SAMPA transcript (reduces from 51k to 37345)\n",
    "\n",
    "            # exclude foreign words that have the 'æ' tone (SAMPA '{' ) like in PoINte   - 18 words\n",
    "            # exclude foreign words that have the 'ɑ' tone (SAMPA 'A' ) like in NuANce   - 28 words\n",
    "            # exclude foreign words that have a nasal vowel (SAMPA '~' ) like in Jargon  - 22 words\n",
    "                if not 'A' in line[-2] and not '{' in line[-2] and not '~' in line[-2]: \n",
    "\n",
    "                    if not ('tS' in line[-2] and not 'tsch' in line[1]): # exclude 9 foreign words like 'Image', 'Match', 'Punch', 'Sketch'\n",
    "                        \n",
    "                        if not ('e' in line[-2] and not 'e:' in line[-2]) and not 'Z' in line[-2]: # exclude aerosol and Z laut (Garage, Jury)\n",
    "                            \n",
    "                            if not 'aero' in line[1].lower(): # exclude words with aero\n",
    "\n",
    "                                if len(line[-2]) < 15 : # exclude words with more than 10 phons\n",
    "\n",
    "                                    if len(line[-2]) > m:\n",
    "                                        m = len(line[-2])\n",
    "                                        print(line[1],line[-2])\n",
    "\n",
    "\n",
    "                                    words.append(line[1].lower()) # All words are lowercase only\n",
    "                                    phons.append(line[-2]) # Using SAMPA notation\n",
    "                            \n",
    "                        else:\n",
    "                            t+=1\n",
    "                            \n",
    "    print(\"Excluded\",t, \"words because they were too long (more than 15 phons)\" )\n",
    "    print(\"Size of dataset is\", len(words), \"samples\")\n",
    "\n",
    "    return words,phons\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def str_to_num_dataset(X,Y,pads=10):\n",
    "    \"\"\"\n",
    "    This method receives 2 lists of strings (input X and output Y) and converts it to padded, numerical arrays.\n",
    "    It returns the numerical dataset as well as the dictionaries to retrieve the strings.\n",
    "    \n",
    "    PADS    {int} specifiying how many additional fields should be padded (to allow long words to have longer alt. writings)\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Define dictionaries \n",
    "    # Dictionary assignining a unique integer to each input character\n",
    "    try:\n",
    "        u_characters = set(' '.join(X)) \n",
    "    except TypeError:\n",
    "        # Exception for TIMIT dataset (one phoneme is repr. by seq. of chars)\n",
    "        print(\"TypeError occurred.\")\n",
    "        u_characters = set([quant for seq in X for quant in seq])\n",
    "\n",
    "    char2numX = dict(zip(u_characters, range(1,len(u_characters)+1)))\n",
    "\n",
    "    # Dictionary assignining a unique integer to each phoneme\n",
    "    try:\n",
    "        v_characters = set(' '.join(Y)) \n",
    "    except TypeError:\n",
    "        print(\"TypeError occurred.\")\n",
    "        v_characters = set([quant for seq in Y for quant in seq])\n",
    "    char2numY = dict(zip(v_characters, range(1,len(v_characters)+1))) # Using 0 causes trouble for tf.edit_distance\n",
    "    \n",
    "    # 2. Padding\n",
    "    # Pad inputs\n",
    "    char2numX['<GO>'] = len(char2numX) + 1\n",
    "    char2numX['<PAD>'] = len(char2numX) + 1\n",
    "    mx_l_X = max([len(word) for word in X]) # longest input sequence\n",
    "    # Padd all X for the final form for the LSTM\n",
    "    x = [[char2numX['<GO>']] + [char2numX['<PAD>']]*(mx_l_X - len(word)) +[char2numX[char] for char in word] for word in X]\n",
    "    x = np.array(x) \n",
    "\n",
    "    # Pad targets\n",
    "    char2numY['<GO>'] = len(char2numY) + 1 # Define number denoting the response onset\n",
    "    char2numY['<PAD>'] = len(char2numY) + 1 \n",
    "    mx_l_Y = max([len(phon_seq) for phon_seq in Y]) # longest output sequence\n",
    "\n",
    "    y = [[char2numY['<GO>']] + pads*[char2numY['<PAD>']] + [char2numY['<PAD>']]*(mx_l_Y - len(ph_sq)) + [char2numY[phon] for phon in ph_sq] for ph_sq in Y]\n",
    "    y = np.array(y)\n",
    "\n",
    "    return ((x,y) , (char2numX,char2numY))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A a:\n",
      "aalen a:l\n",
      "aalglatt a:l#glat\n",
      "Aasgeier a:z#gai@r\n",
      "abaenderlich ap#End@r#lIx\n",
      "abbauwuerdig ap#bau#vYrdIx\n",
      "Abbrucharbeit ap#brEx#arbait\n",
      "Excluded 87 words because they were too long (more than 15 phons)\n",
      "Size of dataset is 31877 samples\n",
      "29 {'n': 1, 'r': 2, 'd': 3, 'y': 4, 'k': 5, 'x': 6, 'z': 7, 'f': 8, 'v': 9, 'j': 10, 'e': 11, 'c': 12, 'q': 13, 'h': 14, 'w': 15, 'a': 16, 'm': 17, 'l': 18, 'g': 19, 'o': 20, 't': 21, ' ': 22, 'u': 23, 's': 24, 'i': 25, 'b': 26, 'p': 27, '<GO>': 28, '<PAD>': 29}\n",
      "39 {'n': 1, 'r': 2, 'd': 3, 'y': 4, 'U': 5, 'k': 6, 'E': 7, ':': 8, 'x': 9, 'z': 10, 'f': 11, 'v': 12, 'Y': 13, '+': 14, 'j': 15, '|': 16, 'e': 17, '/': 18, 'S': 19, 'h': 20, 'a': 21, 'I': 22, '#': 23, 'm': 24, 'l': 25, 'g': 26, 'N': 27, 'o': 28, 't': 29, ' ': 30, 'u': 31, 's': 32, '@': 33, 'i': 34, 'b': 35, 'O': 36, 'p': 37, '<GO>': 38, '<PAD>': 39}\n",
      "(31877, 28) (31877, 15)\n",
      "[28 29 29 29 29 29 29 29 29 29 29 29 29 29 29 29 29 29 29 16 26 18 23 12\n",
      " 14 24 11  1] [38 39 39 39 39 39 39 39 21 37 23 25  5  6 32]\n"
     ]
    }
   ],
   "source": [
    "words,phons = extract_celex(path)\n",
    "((phons_num, words_num), (phon_dict, word_dict)) = str_to_num_dataset(phons,words)\n",
    "    \n",
    "print(len(word_dict),word_dict)\n",
    "print(len(phon_dict),phon_dict)\n",
    "print(words_num.shape, phons_num.shape)\n",
    "print(words_num[321,:], phons_num[321,:])\n",
    "np.savez(\"data/celex.npz\", words=words_num, phons=phons_num, word_dict=word_dict, phon_dict=phon_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of phonetic dict is  39  and the keys are: \n",
      " ['n', 'r', 'd', 'y', 'U', 'k', 'E', ':', 'x', 'z', 'f', 'v', 'Y', '+', 'j', '|', 'e', '/', 'S', 'h', 'a', 'I', '#', 'm', 'l', 'g', 'N', 'o', 't', ' ', 'u', 's', '@', 'i', 'b', 'O', 'p', '<GO>', '<PAD>']\n"
     ]
    }
   ],
   "source": [
    "#data = np.load('data/celex_no_alt.npz')\n",
    "#sampa_dict = {key:data['phon_dict'].item().get(key) for key in data['phon_dict'].item()}\n",
    "sampa_dict = phon_dict\n",
    "sampa_keys = list(sampa_dict.keys()) # Has 43 keys originally, 40 after excluding {, ~, A\n",
    "print(\"Length of phonetic dict is \", len(sampa_dict), \" and the keys are: \\n\", sampa_keys) # Has 43 keys\n",
    "\n",
    "\n",
    "# Step 1: Make a sampa_ipa dict. How to: Go to SAMPA -> IPA tabelle, for every SAMPA char, check whether it \n",
    "# is in the CELEX korpus. If yes, look up example word from wiki in korpus and check whether it is the right sampa\n",
    "# sign. If yes, look up on wiktionary example word in IPA and check whether output sign is correct.\n",
    "\n",
    "sampa_ipa = dict()\n",
    "\n",
    "# Vowels\n",
    "sampa_ipa['i'] = 'i'\n",
    "sampa_ipa[':'] = 'ː'\n",
    "sampa_ipa['I'] = 'ɪ'\n",
    "sampa_ipa['e'] = 'e'\n",
    "sampa_ipa['E'] = 'ɛ'\n",
    "sampa_ipa['y'] = 'y'   # meaning a real ü like in kühl\n",
    "sampa_ipa['@'] = 'ə'\n",
    "sampa_ipa['a'] = 'a'\n",
    "sampa_ipa['u'] = 'u'\n",
    "sampa_ipa['U'] = 'ʊ'\n",
    "sampa_ipa['o'] = 'o'\n",
    "sampa_ipa['O'] = 'ɔ'\n",
    "\n",
    "\n",
    "# consonants\n",
    "sampa_ipa['p'] = 'p'\n",
    "sampa_ipa['b'] = 'b'\n",
    "sampa_ipa['t'] = 't'\n",
    "sampa_ipa['d'] = 'd'\n",
    "sampa_ipa['k'] = 'k'\n",
    "sampa_ipa['g'] = 'g'\n",
    "sampa_ipa['f'] = 'f'\n",
    "sampa_ipa['v'] = 'v'\n",
    "sampa_ipa['s'] = 's'\n",
    "sampa_ipa['z'] = 'z'\n",
    "sampa_ipa['S'] = 'ʃ'\n",
    "sampa_ipa['x'] = 'x'\n",
    "sampa_ipa['h'] = 'h'\n",
    "sampa_ipa['m'] = 'm'\n",
    "sampa_ipa['n'] = 'n'\n",
    "sampa_ipa['N'] = 'ŋ'\n",
    "sampa_ipa['l'] = 'l'\n",
    "sampa_ipa['r'] = 'r'\n",
    "sampa_ipa['j'] = 'j'\n",
    "sampa_ipa['Z'] = 'ʒ'\n",
    "sampa_ipa['+'] = ''  # meaning a bit unclear\n",
    "sampa_ipa['#'] = 'ˈ' # following syllabus carries primary intonation\n",
    "sampa_ipa['|'] = 'ø' # meaning a bit unclear\n",
    "sampa_ipa['/'] = 'œ' # usually SAMPA uses 9 instead of / for this \n",
    "sampa_ipa['Y'] = 'ʏ' # meaning more a 'oü' like in Müll\n",
    "\n",
    "# These are 37 keys only, so 6 are missing. Remaining ones are:\n",
    "\n",
    "# <GO>          not needed for alt. writing creation\n",
    "# <PAD>         not needed for alt. writing creation\n",
    "#    (SPACE)    not needed\n",
    "# {             excluded some foreign words\n",
    "# A             excluded some foreign words\n",
    "# ~             excluded some foreign words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now convert the SAMPA words into IPA words:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of IPA samples is 31877 . Some samples are: \n",
      "\n",
      " WORD         ===>        SAMPA        ===>        IPA\n",
      "glueckselig  =>  glYk#ze:lIg  =>  glʏkˈzeːlɪg\n",
      "gluecksfall  =>  glYk+s#fal  =>  glʏksˈfal\n",
      "glueckskind  =>  glYk+s#kInd  =>  glʏksˈkɪnd\n",
      "gluecksspiel  =>  glYk+s#Spi:l  =>  glʏksˈʃpiːl\n",
      "glueckstreffer  =>  glYk+s#trEf+@r  =>  glʏksˈtrɛfər\n",
      "glueckwunsch  =>  glYk#vUnS  =>  glʏkˈvʊnʃ\n",
      "gluehen  =>  gly:  =>  glyː\n",
      "gluehbirne  =>  gly:#bIrn@  =>  glyːˈbɪrnə\n",
      "gluehheiss  =>  gly:#hais  =>  glyːˈhais\n",
      "gluehlampe  =>  gly:#lamp@  =>  glyːˈlampə\n"
     ]
    }
   ],
   "source": [
    "ipa = []\n",
    "for samp in phons:\n",
    "    s = []\n",
    "    for char in samp:\n",
    "        s.append(sampa_ipa[char])\n",
    "    ipa.append(''.join(s))\n",
    "print(\"Amount of IPA samples is\", len(words), \". Some samples are: \\n\")\n",
    "print(\" WORD         ===>        SAMPA        ===>        IPA\")\n",
    "for k in range(10000, 10010):\n",
    "    print(words[k],\" => \", phons[k],\" => \", ipa[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now define the second dictionary, mapping IPA signs to graphemes (according to [here](https://de.wiktionary.org/wiki/Verzeichnis:Deutsch/Phoneme_und_Grapheme) ) \n",
    "\n",
    "Go to every phon and write down all graphemes (copying table, excluding very weird graphemes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_values(['i', 'ː', 'ɪ', 'e', 'ɛ', 'y', 'ə', 'a', 'u', 'ʊ', 'o', 'ɔ', 'p', 'b', 't', 'd', 'k', 'g', 'f', 'v', 's', 'z', 'ʃ', 'x', 'h', 'm', 'n', 'ŋ', 'l', 'r', 'j', 'ʒ', '', 'ˈ', 'ø', 'œ', 'ʏ'])\n"
     ]
    }
   ],
   "source": [
    "print(sampa_ipa.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37 46\n",
      "ː\n",
      "\n",
      "ˈ\n",
      "ø\n"
     ]
    }
   ],
   "source": [
    "ipa_graph = dict()\n",
    "ipa_graph['t'] = ['t', 'd', 'tt'] # excluded dt (mostly in Stadt) and th (Methode) -> hard but not do more than 3 opts.\n",
    "ipa_graph['ə'] = ['e']\n",
    "ipa_graph['n'] = ['n', 'nn']\n",
    "ipa_graph['s'] = ['s', 'ss'] # excluded t for Patience, ce for Farce, zz for Jazz, ß (no occ.), c for Sauce, z for Quiz\n",
    "ipa_graph['a'] = ['a', 'ah']\n",
    "ipa_graph['r'] = ['r', 'rr'] # excluded rrh for Zirrhose/Myrrhe, rh for Rhythmus\n",
    "ipa_graph['l'] = ['l', 'll']\n",
    "ipa_graph['ɛ'] = ['e', 'ae']\n",
    "ipa_graph['f'] = ['f', 'v', 'ff'] # excluded ph for Physik\n",
    "ipa_graph['g'] = ['g', 'gh'] # excluded gg for Bagger\n",
    "ipa_graph['ɪ'] = ['i']\n",
    "ipa_graph['k'] = ['k', 'ck', 'c'] # Excluded cch for Zucchini, gg for Flaggschiff, qu for Boutique, kk for Akkordeon\n",
    "                  #  qu chars are usually kv ipa (tracked below), g for Krieg, ch for Chor\n",
    "ipa_graph['m'] = ['m', 'mm']\n",
    "ipa_graph['b'] = ['b', 'bb']\n",
    "ipa_graph['ʃ'] = ['sch', 's'] # excluded sk for Ski, sh for Sheriff, Show and ch for Recherche \n",
    "ipa_graph['d'] = ['d','dd']\n",
    "ipa_graph['p'] = ['p', 'b', 'pp'] # excluded bb for abebben or schrubben\n",
    "ipa_graph['ŋ'] = ['ng','n']\n",
    "ipa_graph['ɔ'] = ['o'] # excluded ch for Chauffeur (very rare exception)\n",
    "ipa_graph['v'] = ['w', 'v']\n",
    "ipa_graph['ʊ'] = ['u']\n",
    "ipa_graph['z'] = ['s'] # excluded zz for Blizzard, Puzzle and z for zoomen, bulldozer (since only in foreign words)\n",
    "ipa_graph['h'] = ['h']\n",
    "ipa_graph['i'] = ['i'] # excluding y (Baby/Party/Hockey) only 10 words in corpus\n",
    "ipa_graph['ʏ'] = ['ue', 'u'] # The corpus is weird here and writes Druck as drʏk, i.e. \"Drück\" rather than drʊk\n",
    "ipa_graph['x'] = ['ch']\n",
    "ipa_graph['e'] = ['e'] # excluded ee for Kaffee since IPA would be eː\n",
    "ipa_graph['j'] = ['j', 'y']\n",
    "ipa_graph['u'] = ['u'] # excluded ou like in Boutique\n",
    "ipa_graph['o'] = ['o'] # not needed anyways since o always followed by ː\n",
    "ipa_graph['œ'] = ['oe']\n",
    "ipa_graph['y'] = ['y']\n",
    "ipa_graph['ʒ'] = ['g', 'j'] # no wiki entry, self generated. For Garage or Jury\n",
    "\n",
    "\n",
    "# 2 character keys:\n",
    "ipa_graph['ts'] = ['z', 'ts', 'tz'] # excluded zz for Pizza/Skizze, c for circa, Penicillin, tts for trittst\n",
    "            # t for Aktion, Negation, Infektion, Proportion, ...\n",
    "ipa_graph['aː'] = ['a', 'ah', 'aa']\n",
    "ipa_graph['ai'] = ['ei', 'ai'] # excluded ail for Detail, aill for Medaillon, aille for Medaille and y for Nylon\n",
    "ipa_graph['iː'] = ['ie', 'i', 'ih'] # excluded ieh for Entziehung\n",
    "ipa_graph['eː'] = ['e', 'ee', 'eh'] # excluded et like in Bidet\n",
    "ipa_graph['ɛː'] = ['ae', 'aeh']\n",
    "ipa_graph['uː'] = ['u', 'uh'] # excluded ou like in Ragout, Limousine and oo like on zoomen/Cartoon\n",
    "ipa_graph['oː'] = ['o', 'oh', 'oo'] # excluded au for aubergine/sauce and eau for plateau, Niveau\n",
    "ipa_graph['yː'] = ['ue', 'ueh', 'y'] # excluded uet like in Debüt and u like in deja-vu\n",
    "ipa_graph['ɔy'] = ['eu', 'aeu'] # instead of what wiki calls ɔɪ̯, excluded oi for Boiler and oy for Boykott\n",
    "ipa_graph['ks'] = ['chs', 'x', 'ks'] #excluded gs like in legst/bugsieren, ggs like in eggst (?), cks like in zwecks, gs (legst)\n",
    "ipa_graph['øː'] = ['oe', 'oeh'] # excluded eu like in Ingenieur and eue like in Queue (?)\n",
    "ipa_graph['kv'] = ['qu']\n",
    "\n",
    "\n",
    "print(len(sampa_ipa), len(ipa_graph))\n",
    "# We had 36 keys in sampa_ipa dict, now we have 46 already in ipa_graph due to 2-phoneme-groups\n",
    "# But still there are one-char-values in sampa_ipa which are not keys in ipa_graph. Let us print them:\n",
    "for key in sampa_ipa.values():\n",
    "    if key not in ipa_graph.keys():\n",
    "        print(key)\n",
    "        \n",
    "# Okay 4 are missing:\n",
    "ipa_graph['ː'] = [''] # not needed anyways since ː always occurs after vowel\n",
    "ipa_graph['ˈ'] = [''] # Just a pronounciation symbol, does not carry meaning for spelling\n",
    "# Then the empty string '' is not needed as key\n",
    "# Then ø only occurs followed by a ː\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DO again with Jäger source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37 48\n",
      "e\n",
      "o\n",
      "ʒ\n",
      "\n",
      "ø\n"
     ]
    }
   ],
   "source": [
    "ipa_graph2 = dict()\n",
    "ipa_graph2['t'] = ['t', 'd', 'tt','dt','th'] \n",
    "ipa_graph2['n'] = ['n', 'nn']\n",
    "ipa_graph2['s'] = ['s', 'ss'] \n",
    "ipa_graph2['a'] = ['a']\n",
    "ipa_graph2['r'] = ['r', 'rr','rh'] # our IPA uses r instead of ʁ\n",
    "ipa_graph2['l'] = ['l', 'll']\n",
    "ipa_graph2['ɛ'] = ['e', 'ae']\n",
    "ipa_graph2['f'] = ['f', 'v', 'ff','ph']\n",
    "ipa_graph2['g'] = ['g', 'gg'] \n",
    "ipa_graph2['ɪ'] = ['i','ie']\n",
    "ipa_graph2['k'] = ['k', 'ck', 'c','g','ch'] # Excluded cch for Zucchini, gg for Flaggschiff, qu for Boutique, kk for Akkordeon\n",
    "ipa_graph2['m'] = ['m', 'mm']\n",
    "ipa_graph2['b'] = ['b', 'bb']\n",
    "ipa_graph2['ʃ'] = ['sch', 's'] # excluded sk for Ski, sh for Sheriff, Show and ch for Recherche \n",
    "ipa_graph2['d'] = ['d','dd']\n",
    "ipa_graph2['p'] = ['p', 'b', 'pp'] # excluded bb for abebben or schrubben\n",
    "ipa_graph2['ŋ'] = ['ng','n']\n",
    "ipa_graph2['ɔ'] = ['o'] # excluded au for Chauffeur (very rare exception)\n",
    "ipa_graph2['v'] = ['w', 'v']\n",
    "ipa_graph2['ʊ'] = ['u']\n",
    "ipa_graph2['z'] = ['s'] \n",
    "ipa_graph2['h'] = ['h']\n",
    "ipa_graph2['ʏ'] = ['ue', 'u','y'] # The corpus is weird here and writes Druck as drʏk, i.e. \"Drück\" rather than drʊk\n",
    "ipa_graph2['x'] = ['ch']\n",
    "ipa_graph2['j'] = ['j']\n",
    "ipa_graph2['œ'] = ['oe']\n",
    "\n",
    "\n",
    "# 2 character keys:\n",
    "ipa_graph2['ts'] = ['z', 'tz'] # excluded zz for Pizza/Skizze, c for circa, Penicillin, tts for trittst\n",
    "            # t for Aktion, Negation, Infektion, Proportion, ...\n",
    "ipa_graph2['aː'] = ['a', 'ah', 'aa']\n",
    "\n",
    "ipa_graph2['aɪ'] = ['ei', 'ai'] # excluded ail for Detail, aill for Medaillon, aille for Medaille and y for Nylon\n",
    "ipa_graph2['iː'] = ['ie', 'i', 'ih','ieh'] # excluded ieh for Entziehung\n",
    "ipa_graph2['eː'] = ['e', 'ee', 'eh'] # excluded et like in Bidet\n",
    "ipa_graph2['ɛː'] = ['ae', 'aeh']\n",
    "ipa_graph2['uː'] = ['u', 'uh'] # excluded ou like in Ragout, Limousine and oo like on zoomen/Cartoon\n",
    "ipa_graph2['oː'] = ['o', 'oh', 'oo'] # excluded au for aubergine/sauce and eau for plateau, Niveau\n",
    "ipa_graph2['yː'] = ['ue', 'ueh'] # excluded uet like in Debüt and u like in deja-vu\n",
    "ipa_graph2['ɔy'] = ['eu', 'aeu'] # instead of what table calls 'ɔɪ'\n",
    "ipa_graph2['ks'] = ['chs', 'x', 'ks','cks','gs']\n",
    "ipa_graph2['øː'] = ['oe', 'oeh'] # excluded eu like in Ingenieur and eue like in Queue (?)\n",
    "ipa_graph2['aʊ'] = ['au']\n",
    "ipa_graph2['pf'] = ['pf']\n",
    "\n",
    "# The Jäger table itself is not sufficient. E.g. there are occurrences of y without yː and \n",
    "# since only the latter has a dict entry, we would get a key error. \n",
    "# Thus we add some dict entries manually\n",
    "ipa_graph2['y'] = ['y']\n",
    "ipa_graph2['ə'] = ['e']\n",
    "ipa_graph2['i'] = ['i'] \n",
    "ipa_graph2['u'] = ['u'] \n",
    "ipa_graph2['kv'] = ['qu']\n",
    "ipa_graph2['ˈ'] = [''] # Just a pronounciation symbol, does not carry meaning for spelling\n",
    "ipa_graph2['ː'] = ['']\n",
    "\n",
    "\n",
    "print(len(sampa_ipa), len(ipa_graph))\n",
    "# We had 36 keys in sampa_ipa dict, now we have 46 already in ipa_graph due to 2-phoneme-groups\n",
    "# But still there are one-char-values in sampa_ipa which are not keys in ipa_graph. Let us print them:\n",
    "for key in sampa_ipa.values():\n",
    "    if key not in ipa_graph2.keys():\n",
    "        print(key)\n",
    "        \n",
    "# Okay 4 are missing:\n",
    "#ipa_graph['ː'] = [''] # not needed anyways since ː always occurs after vowel\n",
    "# Then the empty string '' is not needed as key\n",
    "# Then ø only occurs followed by a ː\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pretty tough to draw the boundary between which phons rarely but \"regularly\" translate to grapheme strings ([k] to kk like in Akkordeon, Mokka is still regular?) whereas [ʃ] (sch) to ch like in Champagner, Recherche or Lunch is irregular?\n",
    "\n",
    "## Use the IPA->Graphem dict to create alternative writings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_word(word, ipa_graph2):\n",
    "    \"\"\"\n",
    "    Splits up an IPA word into a list of lists each with the possible replacement grapheme for each phoneme\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    WORD       {list} in IPA notation\n",
    "    IPA_GRAPH  {dict} mapping IPA symbols to possible grapheme sequences\n",
    "    \n",
    "    Returns:\n",
    "    ---------\n",
    "    CHARS      {list} containing lists with possible grapheme sequences\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    chars = []\n",
    "    single_key = True\n",
    "    for ind in range(len(word)-1):\n",
    "        \n",
    "        if single_key: \n",
    "            if word[ind:ind+2] in ipa_graph2:\n",
    "                chars.append(ipa_graph2[word[ind:ind+2]])\n",
    "                single_key = False\n",
    "            else:\n",
    "                chars.append(ipa_graph2[word[ind]])\n",
    "        else:\n",
    "            single_key = True\n",
    "            \n",
    "    chars.append(ipa_graph2[word[ind+1]])\n",
    "    \n",
    "    return chars\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The method below generates the alternative writings (takes some time...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_writings():\n",
    "    all_writings = []\n",
    "    m = 0\n",
    "\n",
    "    for ind,ip in enumerate(ipa):\n",
    "        if ind % 5000 == 0:\n",
    "            print(\"Currently examining word \", ind)\n",
    "\n",
    "        word_lists = split_word(ip, ipa_graph2)\n",
    "        alt_write_raw = list(itertools.product(*word_lists))\n",
    "        alt_write = [''.join(a) for a in alt_write_raw]\n",
    "        try:\n",
    "            alt_write.remove(words[ind])\n",
    "        except ValueError:\n",
    "            _ = 1\n",
    "\n",
    "        all_writings.append(alt_write)\n",
    "\n",
    "        \"\"\"        \n",
    "        if len(alt_write) > m:\n",
    "            print(len(alt_write),ind)\n",
    "            m = len(alt_write)\"\"\"\n",
    "    print(\"DONE! Alternative writings generated. Resulting list has\", len(all_writings), 'entries.')    \n",
    "    return all_writings\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now convert the list of alternative writings into a format that can be saved to disk. Tried A LOT of things here. We could use int8 as datatype since the values are in the range of [0,num_dec_symbols], but the arrays have varying size which numpy cannot handle effciently. Low storage solution: Use np.save to save a list that contains for each word of the corpus a list with num_alt_writings lists, each containing a possible target sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(all_writings):\n",
    "\n",
    "    print(words_num.shape)\n",
    "    seq_len = words_num.shape[1]\n",
    "    max_alt_spellings = max(len(l) for l in all_writings)\n",
    "    num_alt_writings = []\n",
    "\n",
    "    m = 0\n",
    "    for wo_ind in range(len(all_writings)):\n",
    "\n",
    "        if wo_ind % 1000 == 0:\n",
    "            print(\"Currently converting word\", wo_ind)\n",
    "        tmp = []\n",
    "        \n",
    "        for tar_ind in range(len(all_writings[wo_ind])):\n",
    "            l = seq_len - len(all_writings[wo_ind][tar_ind])\n",
    "            num = [word_dict['<PAD>']]*l + [word_dict[k] for k in all_writings[wo_ind][tar_ind]]\n",
    "            tmp.append(num)\n",
    "\n",
    "        num_alt_writings.append(tmp)\n",
    "            \n",
    "    return num_alt_writings\n",
    "\n",
    "# Alternatives:\n",
    "\n",
    "#num_alt_writings = dict()\n",
    "#num_alt_writings = np.array([])\n",
    "#num_alt_writings = np.zeros((words_num.shape[0],seq_len),dtype=np.int32)\n",
    "#num_alt_writings = np.zeros((words_num.shape[0],seq_len,max_alt_spellings),dtype=np.int8)\n",
    "\n",
    "    #for wo_ind in range(num_alt_writings.shape[0]):\n",
    "\n",
    "        #tmp = np.zeros((seq_len, len(all_writings[wo_ind])),dtype=np.int8)\n",
    "        \n",
    "            #num = np.array([word_dict['<PAD>']]*l + [word_dict[k] for k in all_writings[wo_ind][tar_ind]])\n",
    "            #tmp[:,tar_ind] = num.astype(np.int8)\n",
    "            \n",
    "        #num_alt_writings = np.array([num_alt_writings,num])\n",
    "        #num_alt_writings[wo_ind,:,tar_ind] = num.astype(np.int8)\n",
    "        \n",
    "    #num_alt_writings = np.array([num_alt_writings,tmp])\n",
    "    #num_alt_writings[wo_ind] = tmp\n",
    "    #num_alt_writings = np.append(num_alt_writings,tmp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally execute the whole pipeline for CELEX dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A a:\n",
      "aalen a:l\n",
      "aalglatt a:l#glat\n",
      "Aasgeier a:z#gai@r\n",
      "abaenderlich ap#End@r#lIx\n",
      "abbauwuerdig ap#bau#vYrdIx\n",
      "Abbrucharbeit ap#brEx#arbait\n",
      "Excluded 87 words because they were too long (more than 15 phons)\n",
      "Size of dataset is 31877 samples\n",
      "Currently examining word  0\n",
      "Currently examining word  5000\n",
      "Currently examining word  10000\n",
      "Currently examining word  15000\n",
      "Currently examining word  20000\n",
      "Currently examining word  25000\n",
      "Currently examining word  30000\n",
      "DONE! Alternative writings generated. Resulting list has 31877 entries.\n"
     ]
    }
   ],
   "source": [
    "words,phons = extract_celex(path)\n",
    "((phons_num, words_num), (phon_dict, word_dict)) = str_to_num_dataset(phons,words)\n",
    "alt_writings_str = generate_writings()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31877, 28)\n",
      "Currently converting word 0\n",
      "Currently converting word 1000\n",
      "Currently converting word 2000\n",
      "Currently converting word 3000\n",
      "Currently converting word 4000\n",
      "Currently converting word 5000\n",
      "Currently converting word 6000\n",
      "Currently converting word 7000\n",
      "Currently converting word 8000\n",
      "Currently converting word 9000\n",
      "Currently converting word 10000\n",
      "Currently converting word 11000\n",
      "Currently converting word 12000\n",
      "Currently converting word 13000\n",
      "Currently converting word 14000\n",
      "Currently converting word 15000\n",
      "Currently converting word 16000\n",
      "Currently converting word 17000\n",
      "Currently converting word 18000\n",
      "Currently converting word 19000\n",
      "Currently converting word 20000\n",
      "Currently converting word 21000\n",
      "Currently converting word 22000\n",
      "Currently converting word 23000\n",
      "Currently converting word 24000\n",
      "Currently converting word 25000\n",
      "Currently converting word 26000\n",
      "Currently converting word 27000\n",
      "Currently converting word 28000\n",
      "Currently converting word 29000\n",
      "Currently converting word 30000\n",
      "Currently converting word 31000\n"
     ]
    }
   ],
   "source": [
    "alt_writings_num = convert(alt_writings_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-38cd31761204>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Save data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/Users/jannisborn/Desktop/LDS_Data/celex_alt_targets\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0malt_writings_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(file, arr, allow_pickle, fix_imports)\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m         format.write_array(fid, arr, allow_pickle=allow_pickle,\n\u001b[0;32m--> 511\u001b[0;31m                            pickle_kwargs=pickle_kwargs)\n\u001b[0m\u001b[1;32m    512\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mown_fid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36mwrite_array\u001b[0;34m(fp, array, version, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpickle_kwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0mpickle_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m         \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_contiguous\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_contiguous\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misfileobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 22] Invalid argument"
     ]
    }
   ],
   "source": [
    "# Save data\n",
    "np.save(\"/Users/jannisborn/Desktop/LDS_Data/celex_alt_targets\",alt_writings_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303749\n"
     ]
    }
   ],
   "source": [
    "print(max([len(l) for l in alt_writings_num]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract childlex data from downloaded CSV and clean it up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "path = 'data/childlex_6-8_lemmata.csv'\n",
    "forbidden = \"_,.!?*:=&%- /\\\\–•»«…›‹()[]{}’'\"\n",
    "\n",
    "\n",
    "def rem_um(word):\n",
    "    \"\"\"\n",
    "    Converts a word with German umlauts (ü,ä,ö) into a word without\n",
    "    \"\"\"\n",
    "        \n",
    "    umlautfree = str()\n",
    "    for char in word:\n",
    "        if char == 'ä':\n",
    "            umlautfree += 'ae'\n",
    "        elif char == 'ü':\n",
    "            umlautfree += 'ue'\n",
    "        elif char == 'ö':\n",
    "            umlautfree += 'oe'\n",
    "        elif char == 'ß':\n",
    "            umlautfree += 'ss'\n",
    "        else:\n",
    "            umlautfree += char\n",
    "    return umlautfree\n",
    "\n",
    "\n",
    "with open(path, 'r') as csvfile:\n",
    "    \n",
    "    raw_file = csv.reader(csvfile,delimiter=';')\n",
    "    raw_data = []\n",
    "    \n",
    "    \n",
    "    for line in raw_file:\n",
    "        umlautfree = rem_um(line[0].lower())\n",
    "        \n",
    "       # data cleanup\n",
    "        has_digit = any(char.isdigit() for char in umlautfree)\n",
    "        has_sonderz = any(char in forbidden for char in umlautfree)\n",
    "        \n",
    "        if not has_digit and not has_sonderz:\n",
    "        \n",
    "            raw_data.append(umlautfree)\n",
    "        \n",
    "        \n",
    "    \n",
    "childlex_words_all = raw_data\n",
    "print(\"Amount of words in childlex\", len(childlex_words_all))\n",
    "    \n",
    "chars = []\n",
    "for word in childlex_words_all:\n",
    "    for char in word:\n",
    "        if char not in chars:\n",
    "            chars.append(char)\n",
    "    \n",
    "print(\"Set of characters in childlex words is\", chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look up phonetic sequences in CELEX Database. \n",
    "#### Problem: We do not have phonetic sequences yet. Idea: Retrieve as many phonetic sequences as possible from the CELEX database, convert this database into numerical values and save it.\n",
    "#### Also retrieve the alternative writings for these words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "celex_alt_targs = np.load(\"/Users/jannisborn/Desktop/LDS_Data/celex_alt_targets.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "childlex_phons = []\n",
    "childlex_words = []\n",
    "childlex_alt_writings = []\n",
    "for word in childlex_words_all:\n",
    "    if word in words:\n",
    "        ind = words.index(word)\n",
    "        childlex_alt_writings.append(celex_alt_targs[ind])\n",
    "        childlex_phons.append(phons[ind])\n",
    "        childlex_words.append(words[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"From the\", len(childlex_words_all),\"words in CHILDLEX database\", len(childlex_words),\"were succesfully retrieved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save the childlex dataset\n",
    "((phons_num, words_num), (phon_dict, word_dict)) = str_to_num_dataset(childlex_phons,childlex_words)\n",
    "print(len(word_dict),word_dict)\n",
    "print(len(phon_dict),phon_dict)\n",
    "print(words_num.shape, phons_num.shape)\n",
    "print(words_num[321,:], phons_num[321,:])\n",
    "np.savez(\"data/childlex.npz\", words=words_num, phons=phons_num, word_dict=word_dict, phon_dict=phon_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"/Users/jannisborn/Desktop/LDS_Data/childlex_alt_targets\",childlex_alt_writings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract data from the FIBEL dataset\n",
    "##### Focus on the \"Mia and Mo\" Fibel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fibel_path = \"/Users/jannisborn/Dropbox/GitHub/LSTM/LdS_bLSTM/Code/data/Fibelwörter.txt\"\n",
    "\n",
    "\n",
    "with open(fibel_path, 'r') as txtfile:\n",
    "    fibel_words = []\n",
    "    for line in txtfile.read().split(','):\n",
    "        # Use lowercase letters only, remove leading whitespace and take care of line breaks.\n",
    "        if '\\n' in line:    \n",
    "            for item in line.split('\\n'):\n",
    "                if item.lstrip().lower() not in fibel_words:\n",
    "                    fibel_words.append(rem_um(item.lstrip().lower()))\n",
    "        elif line.lstrip().lower() not in fibel_words:\n",
    "            fibel_words.append(rem_um(line.lstrip().lower()))\n",
    "        \n",
    "lektions_inds = [9,14,20,28,36,46,58,77,99,121,154,174]\n",
    "\n",
    "fibel_phons = []\n",
    "fibel_alt_writings = []\n",
    "k=0\n",
    "for w in fibel_words:\n",
    "    if w in words:\n",
    "        ind = words.index(w)\n",
    "        fibel_phons.append(phons[ind])\n",
    "        fibel_alt_writings.append(celex_alt_targs[ind])\n",
    "        \n",
    "    else:\n",
    "        fibel_phons.append('NO')\n",
    "        fibel_alt_writings.append('NO')\n",
    "        k +=1\n",
    "print(\"The dataset has \"+str(len(fibel_words))+\" words, from which \"+str(len(fibel_words)-k)+\" could be retrieved from celex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate the SAMPA spellings for the missing words. First step: Copy the IPA values from wiktionary + convert IPA into SAMPA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipa_sampa = dict(zip(sampa_ipa.values(), sampa_ipa.keys()))\n",
    "\n",
    "fibel_phons[fibel_words.index('mia')] = ''.join([ipa_sampa[w] for w in 'miːa'])\n",
    "fibel_phons[fibel_words.index('mo')] = ''.join([ipa_sampa[w] for w in 'mo'])\n",
    "fibel_phons[fibel_words.index('mimi')] = ''.join([ipa_sampa[w] for w in 'mimi'])\n",
    "fibel_phons[fibel_words.index('im')] = ''.join([ipa_sampa[w] for w in 'ɪm'])\n",
    "fibel_phons[fibel_words.index('am')] = ''.join([ipa_sampa[w] for w in 'am'])\n",
    "fibel_phons[fibel_words.index('momo')] = ''.join([ipa_sampa[w] for w in 'momo'])\n",
    "fibel_phons[fibel_words.index('omi')] = ''.join([ipa_sampa[w] for w in 'oːmiː'])\n",
    "fibel_phons[fibel_words.index('radio')] = ''.join([ipa_sampa[w] for w in 'reɪdɪəʊ'])\n",
    "fibel_phons[fibel_words.index('sissi')] = ''.join([ipa_sampa[w] for w in 'zɪsɪː'])\n",
    "fibel_phons[fibel_words.index('susi')] = ''.join([ipa_sampa[w] for w in 'susi'])\n",
    "fibel_phons[fibel_words.index('oli')] = ''.join([ipa_sampa[w] for w in 'ɔliː'])\n",
    "fibel_phons[fibel_words.index('salami')] = ''.join([ipa_sampa[w] for w in 'zaˈlaːmi'])\n",
    "fibel_phons[fibel_words.index('ist')] = ''.join([ipa_sampa[w] for w in 'ɪst'])\n",
    "fibel_phons[fibel_words.index('tim')] = ''.join([ipa_sampa[w] for w in 'tɪm'])\n",
    "fibel_phons[fibel_words.index('tom')] = ''.join([ipa_sampa[w] for w in 'tɔm'])\n",
    "fibel_phons[fibel_words.index('mario')] = ''.join([ipa_sampa[w] for w in 'ˈmaːrio'])\n",
    "fibel_phons[fibel_words.index('isa')] = ''.join([ipa_sampa[w] for w in 'iːza'])\n",
    "fibel_phons[fibel_words.index('maria')] = ''.join([ipa_sampa[w] for w in 'maˈriːa'])\n",
    "fibel_phons[fibel_words.index('rosarot')] = ''.join([ipa_sampa[w] for w in 'ˈroːzaˈroːt'])\n",
    "fibel_phons[fibel_words.index('nimm')] = ''.join([ipa_sampa[w] for w in 'nɪm'])\n",
    "fibel_phons[fibel_words.index('nimmt')] = ''.join([ipa_sampa[w] for w in 'nɪmt'])\n",
    "fibel_phons[fibel_words.index('nina')] = ''.join([ipa_sampa[w] for w in 'niːna'])\n",
    "fibel_phons[fibel_words.index('rosinen')] = ''.join([ipa_sampa[w] for w in 'roˈziːnən'])\n",
    "fibel_phons[fibel_words.index('anna')] = ''.join([ipa_sampa[w] for w in 'ˈana'])\n",
    "fibel_phons[fibel_words.index('bananen')] = ''.join([ipa_sampa[w] for w in 'baˈnaːnən'])\n",
    "fibel_phons[fibel_words.index('birnen')] = ''.join([ipa_sampa[w] for w in 'bɪrnən'])\n",
    "fibel_phons[fibel_words.index('nuesse')] = ''.join([ipa_sampa[w] for w in 'nʏsə'])\n",
    "fibel_phons[fibel_words.index('orangen')] = ''.join([ipa_sampa[w] for w in 'oˈraŋʒn'])\n",
    "fibel_phons[fibel_words.index('weintrauben')] = ''.join([ipa_sampa[w] for w in 'ˈvaintraʊbn'])\n",
    "fibel_phons[fibel_words.index('maroni')] = ''.join([ipa_sampa[w] for w in 'maˈroːni'])\n",
    "fibel_phons[fibel_words.index('erna')] = ''.join([ipa_sampa[w] for w in 'ˈɛrna'])\n",
    "fibel_phons[fibel_words.index('die')] = ''.join([ipa_sampa[w] for w in 'diː'])\n",
    "fibel_phons[fibel_words.index('das')] = ''.join([ipa_sampa[w] for w in 'das'])\n",
    "fibel_phons[fibel_words.index('sind')] = ''.join([ipa_sampa[w] for w in 'zɪnt'])\n",
    "fibel_phons[fibel_words.index('domino')] = ''.join([ipa_sampa[w] for w in 'ˈdoːmino'])\n",
    "fibel_phons[fibel_words.index('indianer')] = ''.join([ipa_sampa[w] for w in 'ɪnˈdiaːnər'])\n",
    "fibel_phons[fibel_words.index('tonio')] = ''.join([ipa_sampa[w] for w in 'tɔnioː'])\n",
    "fibel_phons[fibel_words.index('sagt')] = ''.join([ipa_sampa[w] for w in 'zaːkt'])\n",
    "fibel_phons[fibel_words.index('italien')] = ''.join([ipa_sampa[w] for w in 'iˈtaːliən'])\n",
    "fibel_phons[fibel_words.index('ute')] = ''.join([ipa_sampa[w] for w in 'uːtə'])\n",
    "fibel_phons[fibel_words.index('umarmt')] = ''.join([ipa_sampa[w] for w in 'ʊmˈarmt'])\n",
    "fibel_phons[fibel_words.index('runter')] = ''.join([ipa_sampa[w] for w in 'rʊntər'])\n",
    "fibel_phons[fibel_words.index('turnen')] = ''.join([ipa_sampa[w] for w in 'tʊrnən'])\n",
    "fibel_phons[fibel_words.index('dem')] = ''.join([ipa_sampa[w] for w in 'deːm'])\n",
    "fibel_phons[fibel_words.index('rennt')] = ''.join([ipa_sampa[w] for w in 'rɛnt'])\n",
    "fibel_phons[fibel_words.index('ene')] = ''.join([ipa_sampa[w] for w in 'eːnə'])\n",
    "fibel_phons[fibel_words.index('mene')] = ''.join([ipa_sampa[w] for w in 'meːnə'])\n",
    "fibel_phons[fibel_words.index('mu')] = ''.join([ipa_sampa[w] for w in 'muː'])\n",
    "fibel_phons[fibel_words.index('turnt')] = ''.join([ipa_sampa[w] for w in 'ˈtʊrnt'])\n",
    "fibel_phons[fibel_words.index('miauen')] = ''.join([ipa_sampa[w] for w in 'miˈaʊən'])\n",
    "fibel_phons[fibel_words.index('raus')] = ''.join([ipa_sampa[w] for w in 'raʊs'])\n",
    "fibel_phons[fibel_words.index('mauert')] = ''.join([ipa_sampa[w] for w in 'tɔm'])\n",
    "fibel_phons[fibel_words.index('saust')] = ''.join([ipa_sampa[w] for w in 'zaʊst'])\n",
    "fibel_phons[fibel_words.index('rosaroten')] = ''.join([ipa_sampa[w] for w in 'roːzaˈroːtn'])\n",
    "fibel_phons[fibel_words.index('martin')] = ''.join([ipa_sampa[w] for w in 'ˈmartiːn'])\n",
    "fibel_phons[fibel_words.index('martins')] = ''.join([ipa_sampa[w] for w in 'ˈmartiːns'])\n",
    "fibel_phons[fibel_words.index('eine')] = ''.join([ipa_sampa[w] for w in 'ˈaɪnə'])\n",
    "fibel_phons[fibel_words.index('miaut')] = ''.join([ipa_sampa[w] for w in 'miˈaʊt'])\n",
    "fibel_phons[fibel_words.index('otto')] = ''.join([ipa_sampa[w] for w in 'ˈɔto'])\n",
    "fibel_phons[fibel_words.index('isst')] = ''.join([ipa_sampa[w] for w in 'ɪst'])\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    fibel_phons.index('NO')\n",
    "except ValueError:\n",
    "    print(\"YAY, all words have phonetic transcript\")\n",
    "print(fibel_phons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Fibel dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((phons_num, words_num), (phon_dict, word_dict)) = str_to_num_dataset(childlex_phons,childlex_words)\n",
    "np.savez(\"data/childlex.npz\", words=words_num, phons=phons_num, word_dict=word_dict, phon_dict=phon_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(words.index('haus'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.InteractiveSession()\n",
    "\n",
    "a = tf.zeros((3,),dtype=tf.int32)\n",
    "\n",
    "b = tf.constant([[[1,2,3],[0,0,0]],[[1,2,3],[0,0,0]]],dtype=tf.int64)\n",
    "print(b[1,1,:].shape)\n",
    "if tf.reduce_all(tf.equal(a,b[1,1,:])).eval():\n",
    "    print(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.zeros((10,1))\n",
    "b = a\n",
    "for k in range(10):\n",
    "    \n",
    "    b = np.array([b,np.ones((k,1))])\n",
    "    print(b.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insight 1: If you have k different classes, then k is the third \n",
    "#    dimension of the logits and then the target matrix must not \n",
    "#    contain values higher than k-1 (i.e. labels are 0, ..., k)\n",
    "\n",
    "# Insight 2: sequence_loss expects unnormalized logits (BEFORE softmax!)\n",
    "# -> This makes testing for me harder, since I cannot simply use values\n",
    "# with a sum over 1 (will get normalized)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
