{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: Come up with alternative spellings for all orthographic words in a database\n",
    "# Tool 1: IPA -> Graphem Konverter Wiki\n",
    "# Tool 2: SAMPA -> IPA Konverter Wiki\n",
    "\n",
    "# Idea: Take the phonetic sequence (SAMPA) of every word, convert it to an IPA sequence and\n",
    "        # then convert that to all grapheme sequences\n",
    "    \n",
    "# Needs: A dictionary for SAMPA -> IPA (ideally non-ambiguous). A dictionary for IPA -> Text (ambigu.)\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os, sys, pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/jannisborn/Desktop/LDS_Data/celex2/german/gpl/gpl.cd'\n",
    "\n",
    "def extract_celex(path):\n",
    "    \"\"\"\n",
    "    Reads in data from the CELEX corpus\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    PATH        {str} the path to the desired celex file, i.e. gpl.cd \n",
    "                    (contains orthography and phonology)\n",
    "\n",
    "    Returns:\n",
    "    -----------\n",
    "    2 Tuples, each with 2 variables. \n",
    "        First tuple:\n",
    "    W           {np.array} of words (length 51728) for gpl.cd\n",
    "    P           {np.array} of phoneme sequences (length 51728) for gpl.cd\n",
    "        Second tuple:\n",
    "    WORD_DICT   {dict} allowing to map the numerical array W back to strings\n",
    "    PHON_DICT   {dict} doing the same for the phonetical arrays P\n",
    "\n",
    "    \n",
    "    Call via:\n",
    "    path = \"/Users/jannisborn/Desktop/LDS_Data/celex2/german/gpl/gpl.cd\"\n",
    "    ((w,p) , (word_dict, phon_dict)) = extract_celex(path)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    with open(path, 'r') as file:\n",
    "\n",
    "        raw_data = file.read().splitlines()\n",
    "        words = []\n",
    "        phons = []\n",
    "        m = 0\n",
    "        t = 0\n",
    "        for ind,raw_line in enumerate(raw_data):\n",
    "            \n",
    "            line = raw_line.split(\"\\\\\")\n",
    "            \n",
    "            \n",
    "\n",
    "            if line[-2]: # Use only words that HAVE a SAMPA transcript (reduces from 51k to 37345)\n",
    "\n",
    "            # exclude foreign words that have the 'æ' tone (SAMPA '{' ) like in PoINte   - 18 words\n",
    "            # exclude foreign words that have the 'ɑ' tone (SAMPA 'A' ) like in NuANce   - 28 words\n",
    "            # exclude foreign words that have a nasal vowel (SAMPA '~' ) like in Jargon  - 22 words\n",
    "                if not 'A' in line[-2] and not '{' in line[-2] and not '~' in line[-2]: \n",
    "\n",
    "                    if not ('tS' in line[-2] and not 'tsch' in line[1]): # exclude 9 foreign words like 'Image', 'Match', 'Punch', 'Sketch'\n",
    "                        \n",
    "                        if not ('e' in line[-2] and not 'e:' in line[-2]) and not 'Z' in line[-2]: # exclude aerosol and Z laut (Garage, Jury)\n",
    "                            \n",
    "                            if not 'aero' in line[1].lower(): # exclude words with aero\n",
    "\n",
    "                                if len(line[-2]) < 15 : # exclude words with more than 10 phons\n",
    "\n",
    "                                    if len(line[-2]) > m:\n",
    "                                        m = len(line[-2])\n",
    "                                        print(line[1],line[-2])\n",
    "\n",
    "\n",
    "                                    words.append(line[1].lower()) # All words are lowercase only\n",
    "                                    phons.append(line[-2]) # Using SAMPA notation\n",
    "                            \n",
    "                        else:\n",
    "                            t+=1\n",
    "                            \n",
    "    print(\"Excluded\",t, \"words because they were too long (more than 15 phons)\" )\n",
    "    print(\"Size of dataset is\", len(words), \"samples\")\n",
    "\n",
    "    return words,phons\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def str_to_num_dataset(X,Y,pads=10):\n",
    "    \"\"\"\n",
    "    This method receives 2 lists of strings (input X and output Y) and converts it to padded, numerical arrays.\n",
    "    It returns the numerical dataset as well as the dictionaries to retrieve the strings.\n",
    "    \n",
    "    PADS    {int} specifiying how many additional fields should be padded (to allow long words to have longer alt. writings)\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Define dictionaries \n",
    "    # Dictionary assignining a unique integer to each input character\n",
    "    try:\n",
    "        u_characters = set(' '.join(X)) \n",
    "    except TypeError:\n",
    "        # Exception for TIMIT dataset (one phoneme is repr. by seq. of chars)\n",
    "        print(\"TypeError occurred.\")\n",
    "        u_characters = set([quant for seq in X for quant in seq])\n",
    "\n",
    "    char2numX = dict(zip(u_characters, range(1,len(u_characters)+1)))\n",
    "\n",
    "    # Dictionary assignining a unique integer to each phoneme\n",
    "    try:\n",
    "        v_characters = set(' '.join(Y)) \n",
    "    except TypeError:\n",
    "        print(\"TypeError occurred.\")\n",
    "        v_characters = set([quant for seq in Y for quant in seq])\n",
    "    char2numY = dict(zip(v_characters, range(1,len(v_characters)+1))) # Using 0 causes trouble for tf.edit_distance\n",
    "    \n",
    "    # 2. Padding\n",
    "    # Pad inputs\n",
    "    char2numX['<GO>'] = len(char2numX) + 1\n",
    "    char2numX['<PAD>'] = len(char2numX) + 1\n",
    "    mx_l_X = max([len(word) for word in X]) # longest input sequence\n",
    "    # Padd all X for the final form for the LSTM\n",
    "    x = [[char2numX['<GO>']] + [char2numX['<PAD>']]*(mx_l_X - len(word)) +[char2numX[char] for char in word] for word in X]\n",
    "    x = np.array(x) \n",
    "\n",
    "    # Pad targets\n",
    "    char2numY['<GO>'] = len(char2numY) + 1 # Define number denoting the response onset\n",
    "    char2numY['<PAD>'] = len(char2numY) + 1 \n",
    "    mx_l_Y = max([len(phon_seq) for phon_seq in Y]) # longest output sequence\n",
    "\n",
    "    y = [[char2numY['<GO>']] + pads*[char2numY['<PAD>']] + [char2numY['<PAD>']]*(mx_l_Y - len(ph_sq)) + [char2numY[phon] for phon in ph_sq] for ph_sq in Y]\n",
    "    y = np.array(y)\n",
    "\n",
    "    return ((x,y) , (char2numX,char2numY))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A a:\n",
      "aalen a:l\n",
      "aalglatt a:l#glat\n",
      "Aasgeier a:z#gai@r\n",
      "abaenderlich ap#End@r#lIx\n",
      "abbauwuerdig ap#bau#vYrdIx\n",
      "Abbrucharbeit ap#brEx#arbait\n",
      "Excluded 87 words because they were too long (more than 15 phons)\n",
      "Size of dataset is 31877 samples\n",
      "29 {'w': 1, 'h': 2, 's': 3, 'e': 4, 'y': 5, 'b': 6, 'u': 7, 'f': 8, 'z': 9, 'g': 10, ' ': 11, 'k': 12, 'o': 13, 'n': 14, 'i': 15, 'v': 16, 'x': 17, 'q': 18, 'p': 19, 'm': 20, 'l': 21, 'j': 22, 'c': 23, 'd': 24, 'r': 25, 'a': 26, 't': 27, '<GO>': 28, '<PAD>': 29}\n",
      "39 {'N': 1, 'h': 2, 's': 3, 'e': 4, 'y': 5, 'b': 6, 'Y': 7, 'u': 8, 'f': 9, 'z': 10, 'g': 11, ' ': 12, 'E': 13, 'k': 14, 'o': 15, 'n': 16, 'i': 17, 'v': 18, 'x': 19, ':': 20, 't': 21, 'I': 22, '#': 23, 'U': 24, 'p': 25, 'S': 26, 'm': 27, '|': 28, 'l': 29, 'j': 30, '+': 31, 'O': 32, 'd': 33, 'r': 34, 'a': 35, '/': 36, '@': 37, '<GO>': 38, '<PAD>': 39}\n",
      "(31877, 28) (31877, 15)\n",
      "[28 29 29 29 29 29 29 29 29 29 29 29 29 29 29 29 29 29 29 26  6 21  7 23\n",
      "  2  3  4 14] [38 39 39 39 39 39 39 39 35 25 23 29 24 14  3]\n"
     ]
    }
   ],
   "source": [
    "words,phons = extract_celex(path)\n",
    "((phons_num, words_num), (phon_dict, word_dict)) = str_to_num_dataset(phons,words)\n",
    "    \n",
    "print(len(word_dict),word_dict)\n",
    "print(len(phon_dict),phon_dict)\n",
    "print(words_num.shape, phons_num.shape)\n",
    "print(words_num[321,:], phons_num[321,:])\n",
    "np.savez(\"data/celex.npz\", words=words_num, phons=phons_num, word_dict=word_dict, phon_dict=phon_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31877\n"
     ]
    }
   ],
   "source": [
    "# Save small subset of corpus\n",
    "print(len(alt_writings_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A a:\n",
      "aalen a:l\n",
      "aalglatt a:l#glat\n",
      "Aasgeier a:z#gai@r\n",
      "abaenderlich ap#End@r#lIx\n",
      "abbauwuerdig ap#bau#vYrdIx\n",
      "Abbrucharbeit ap#brEx#arbait\n",
      "Excluded 87 words because they were too long (more than 15 phons)\n",
      "Size of dataset is 31877 samples\n"
     ]
    }
   ],
   "source": [
    "words,phons = extract_celex(path)\n",
    "(phons_num, words_num) = str_to_num_with_dict(phons,words,phon_dict, word_dict)\n",
    "\n",
    "np.savez(\"/Users/jannisborn/Desktop/LDS_Data/celex_small.npz\", words=words_num[:2000], phons=phons_num[:2000], word_dict=word_dict, phon_dict=phon_dict)\n",
    "np.save(\"/Users/jannisborn/Desktop/LDS_Data/celex_alt_targets_small.npy\", alt_writings_num[:2000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of phonetic dict is  39  and the keys are: \n",
      " ['N', 'h', 's', 'e', 'y', 'b', 'Y', 'u', 'f', 'z', 'g', ' ', 'E', 'k', 'o', 'n', 'i', 'v', 'x', ':', 't', 'I', '#', 'U', 'p', 'S', 'm', '|', 'l', 'j', '+', 'O', 'd', 'r', 'a', '/', '@', '<GO>', '<PAD>']\n"
     ]
    }
   ],
   "source": [
    "#data = np.load('data/celex_no_alt.npz')\n",
    "#sampa_dict = {key:data['phon_dict'].item().get(key) for key in data['phon_dict'].item()}\n",
    "sampa_dict = phon_dict\n",
    "sampa_keys = list(sampa_dict.keys()) # Has 43 keys originally, 40 after excluding {, ~, A\n",
    "print(\"Length of phonetic dict is \", len(sampa_dict), \" and the keys are: \\n\", sampa_keys) # Has 43 keys\n",
    "\n",
    "\n",
    "# Step 1: Make a sampa_ipa dict. How to: Go to SAMPA -> IPA tabelle, for every SAMPA char, check whether it \n",
    "# is in the CELEX korpus. If yes, look up example word from wiki in korpus and check whether it is the right sampa\n",
    "# sign. If yes, look up on wiktionary example word in IPA and check whether output sign is correct.\n",
    "\n",
    "sampa_ipa = dict()\n",
    "\n",
    "# Vowels\n",
    "sampa_ipa['i'] = 'i'\n",
    "sampa_ipa[':'] = 'ː'\n",
    "sampa_ipa['I'] = 'ɪ'\n",
    "sampa_ipa['e'] = 'e'\n",
    "sampa_ipa['E'] = 'ɛ'\n",
    "sampa_ipa['y'] = 'y'   # meaning a real ü like in kühl\n",
    "sampa_ipa['@'] = 'ə'\n",
    "sampa_ipa['a'] = 'a'\n",
    "sampa_ipa['u'] = 'u'\n",
    "sampa_ipa['U'] = 'ʊ'\n",
    "sampa_ipa['o'] = 'o'\n",
    "sampa_ipa['O'] = 'ɔ'\n",
    "\n",
    "\n",
    "# consonants\n",
    "sampa_ipa['p'] = 'p'\n",
    "sampa_ipa['b'] = 'b'\n",
    "sampa_ipa['t'] = 't'\n",
    "sampa_ipa['d'] = 'd'\n",
    "sampa_ipa['k'] = 'k'\n",
    "sampa_ipa['g'] = 'g'\n",
    "sampa_ipa['f'] = 'f'\n",
    "sampa_ipa['v'] = 'v'\n",
    "sampa_ipa['s'] = 's'\n",
    "sampa_ipa['z'] = 'z'\n",
    "sampa_ipa['S'] = 'ʃ'\n",
    "sampa_ipa['x'] = 'x'\n",
    "sampa_ipa['h'] = 'h'\n",
    "sampa_ipa['m'] = 'm'\n",
    "sampa_ipa['n'] = 'n'\n",
    "sampa_ipa['N'] = 'ŋ'\n",
    "sampa_ipa['l'] = 'l'\n",
    "sampa_ipa['r'] = 'r'\n",
    "sampa_ipa['j'] = 'j'\n",
    "sampa_ipa['Z'] = 'ʒ'\n",
    "sampa_ipa['+'] = ''  # meaning a bit unclear\n",
    "sampa_ipa['#'] = 'ˈ' # following syllabus carries primary intonation\n",
    "sampa_ipa['|'] = 'ø' # meaning a bit unclear\n",
    "sampa_ipa['/'] = 'œ' # usually SAMPA uses 9 instead of / for this \n",
    "sampa_ipa['Y'] = 'ʏ' # meaning more a 'oü' like in Müll\n",
    "\n",
    "# These are 37 keys only, so 6 are missing. Remaining ones are:\n",
    "\n",
    "# <GO>          not needed for alt. writing creation\n",
    "# <PAD>         not needed for alt. writing creation\n",
    "#    (SPACE)    not needed\n",
    "# {             excluded some foreign words\n",
    "# A             excluded some foreign words\n",
    "# ~             excluded some foreign words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now convert the SAMPA words into IPA words:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of IPA samples is 31877 . Some samples are: \n",
      "\n",
      " WORD         ===>        SAMPA        ===>        IPA\n",
      "glueckselig  =>  glYk#ze:lIg  =>  glʏkˈzeːlɪg\n",
      "gluecksfall  =>  glYk+s#fal  =>  glʏksˈfal\n",
      "glueckskind  =>  glYk+s#kInd  =>  glʏksˈkɪnd\n",
      "gluecksspiel  =>  glYk+s#Spi:l  =>  glʏksˈʃpiːl\n",
      "glueckstreffer  =>  glYk+s#trEf+@r  =>  glʏksˈtrɛfər\n",
      "glueckwunsch  =>  glYk#vUnS  =>  glʏkˈvʊnʃ\n",
      "gluehen  =>  gly:  =>  glyː\n",
      "gluehbirne  =>  gly:#bIrn@  =>  glyːˈbɪrnə\n",
      "gluehheiss  =>  gly:#hais  =>  glyːˈhais\n",
      "gluehlampe  =>  gly:#lamp@  =>  glyːˈlampə\n"
     ]
    }
   ],
   "source": [
    "ipa = []\n",
    "for samp in phons:\n",
    "    s = []\n",
    "    for char in samp:\n",
    "        s.append(sampa_ipa[char])\n",
    "    ipa.append(''.join(s))\n",
    "print(\"Amount of IPA samples is\", len(words), \". Some samples are: \\n\")\n",
    "print(\" WORD         ===>        SAMPA        ===>        IPA\")\n",
    "for k in range(10000, 10010):\n",
    "    print(words[k],\" => \", phons[k],\" => \", ipa[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now define the second dictionary, mapping IPA signs to graphemes (according to [here](https://de.wiktionary.org/wiki/Verzeichnis:Deutsch/Phoneme_und_Grapheme) ) \n",
    "\n",
    "Go to every phon and write down all graphemes (copying table, excluding very weird graphemes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_values(['i', 'ː', 'ɪ', 'e', 'ɛ', 'y', 'ə', 'a', 'u', 'ʊ', 'o', 'ɔ', 'p', 'b', 't', 'd', 'k', 'g', 'f', 'v', 's', 'z', 'ʃ', 'x', 'h', 'm', 'n', 'ŋ', 'l', 'r', 'j', 'ʒ', '', 'ˈ', 'ø', 'œ', 'ʏ'])\n"
     ]
    }
   ],
   "source": [
    "print(sampa_ipa.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37 46\n",
      "ː\n",
      "\n",
      "ˈ\n",
      "ø\n"
     ]
    }
   ],
   "source": [
    "ipa_graph = dict()\n",
    "ipa_graph['t'] = ['t', 'd', 'tt'] # excluded dt (mostly in Stadt) and th (Methode) -> hard but not do more than 3 opts.\n",
    "ipa_graph['ə'] = ['e']\n",
    "ipa_graph['n'] = ['n', 'nn']\n",
    "ipa_graph['s'] = ['s', 'ss'] # excluded t for Patience, ce for Farce, zz for Jazz, ß (no occ.), c for Sauce, z for Quiz\n",
    "ipa_graph['a'] = ['a', 'ah']\n",
    "ipa_graph['r'] = ['r', 'rr'] # excluded rrh for Zirrhose/Myrrhe, rh for Rhythmus\n",
    "ipa_graph['l'] = ['l', 'll']\n",
    "ipa_graph['ɛ'] = ['e', 'ae']\n",
    "ipa_graph['f'] = ['f', 'v', 'ff'] # excluded ph for Physik\n",
    "ipa_graph['g'] = ['g', 'gh'] # excluded gg for Bagger\n",
    "ipa_graph['ɪ'] = ['i']\n",
    "ipa_graph['k'] = ['k', 'ck', 'c'] # Excluded cch for Zucchini, gg for Flaggschiff, qu for Boutique, kk for Akkordeon\n",
    "                  #  qu chars are usually kv ipa (tracked below), g for Krieg, ch for Chor\n",
    "ipa_graph['m'] = ['m', 'mm']\n",
    "ipa_graph['b'] = ['b', 'bb']\n",
    "ipa_graph['ʃ'] = ['sch', 's'] # excluded sk for Ski, sh for Sheriff, Show and ch for Recherche \n",
    "ipa_graph['d'] = ['d','dd']\n",
    "ipa_graph['p'] = ['p', 'b', 'pp'] # excluded bb for abebben or schrubben\n",
    "ipa_graph['ŋ'] = ['ng','n']\n",
    "ipa_graph['ɔ'] = ['o'] # excluded ch for Chauffeur (very rare exception)\n",
    "ipa_graph['v'] = ['w', 'v']\n",
    "ipa_graph['ʊ'] = ['u']\n",
    "ipa_graph['z'] = ['s'] # excluded zz for Blizzard, Puzzle and z for zoomen, bulldozer (since only in foreign words)\n",
    "ipa_graph['h'] = ['h']\n",
    "ipa_graph['i'] = ['i'] # excluding y (Baby/Party/Hockey) only 10 words in corpus\n",
    "ipa_graph['ʏ'] = ['ue', 'u'] # The corpus is weird here and writes Druck as drʏk, i.e. \"Drück\" rather than drʊk\n",
    "ipa_graph['x'] = ['ch']\n",
    "ipa_graph['e'] = ['e'] # excluded ee for Kaffee since IPA would be eː\n",
    "ipa_graph['j'] = ['j', 'y']\n",
    "ipa_graph['u'] = ['u'] # excluded ou like in Boutique\n",
    "ipa_graph['o'] = ['o'] # not needed anyways since o always followed by ː\n",
    "ipa_graph['œ'] = ['oe']\n",
    "ipa_graph['y'] = ['y']\n",
    "ipa_graph['ʒ'] = ['g', 'j'] # no wiki entry, self generated. For Garage or Jury\n",
    "\n",
    "\n",
    "# 2 character keys:\n",
    "ipa_graph['ts'] = ['z', 'ts', 'tz'] # excluded zz for Pizza/Skizze, c for circa, Penicillin, tts for trittst\n",
    "            # t for Aktion, Negation, Infektion, Proportion, ...\n",
    "ipa_graph['aː'] = ['a', 'ah', 'aa']\n",
    "ipa_graph['ai'] = ['ei', 'ai'] # excluded ail for Detail, aill for Medaillon, aille for Medaille and y for Nylon\n",
    "ipa_graph['iː'] = ['ie', 'i', 'ih'] # excluded ieh for Entziehung\n",
    "ipa_graph['eː'] = ['e', 'ee', 'eh'] # excluded et like in Bidet\n",
    "ipa_graph['ɛː'] = ['ae', 'aeh']\n",
    "ipa_graph['uː'] = ['u', 'uh'] # excluded ou like in Ragout, Limousine and oo like on zoomen/Cartoon\n",
    "ipa_graph['oː'] = ['o', 'oh', 'oo'] # excluded au for aubergine/sauce and eau for plateau, Niveau\n",
    "ipa_graph['yː'] = ['ue', 'ueh', 'y'] # excluded uet like in Debüt and u like in deja-vu\n",
    "ipa_graph['ɔy'] = ['eu', 'aeu'] # instead of what wiki calls ɔɪ̯, excluded oi for Boiler and oy for Boykott\n",
    "ipa_graph['ks'] = ['chs', 'x', 'ks'] #excluded gs like in legst/bugsieren, ggs like in eggst (?), cks like in zwecks, gs (legst)\n",
    "ipa_graph['øː'] = ['oe', 'oeh'] # excluded eu like in Ingenieur and eue like in Queue (?)\n",
    "ipa_graph['kv'] = ['qu']\n",
    "\n",
    "\n",
    "print(len(sampa_ipa), len(ipa_graph))\n",
    "# We had 36 keys in sampa_ipa dict, now we have 46 already in ipa_graph due to 2-phoneme-groups\n",
    "# But still there are one-char-values in sampa_ipa which are not keys in ipa_graph. Let us print them:\n",
    "for key in sampa_ipa.values():\n",
    "    if key not in ipa_graph.keys():\n",
    "        print(key)\n",
    "        \n",
    "# Okay 4 are missing:\n",
    "ipa_graph['ː'] = [''] # not needed anyways since ː always occurs after vowel\n",
    "ipa_graph['ˈ'] = [''] # Just a pronounciation symbol, does not carry meaning for spelling\n",
    "# Then the empty string '' is not needed as key\n",
    "# Then ø only occurs followed by a ː\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DO again with Jäger source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37 48\n",
      "e\n",
      "o\n",
      "ʒ\n",
      "\n",
      "ø\n"
     ]
    }
   ],
   "source": [
    "ipa_graph2 = dict()\n",
    "ipa_graph2['t'] = ['t', 'd', 'tt','dt','th'] \n",
    "ipa_graph2['n'] = ['n', 'nn']\n",
    "ipa_graph2['s'] = ['s', 'ss'] \n",
    "ipa_graph2['a'] = ['a']\n",
    "ipa_graph2['r'] = ['r', 'rr','rh'] # our IPA uses r instead of ʁ\n",
    "ipa_graph2['l'] = ['l', 'll']\n",
    "ipa_graph2['ɛ'] = ['e', 'ae']\n",
    "ipa_graph2['f'] = ['f', 'v', 'ff','ph']\n",
    "ipa_graph2['g'] = ['g', 'gg'] \n",
    "ipa_graph2['ɪ'] = ['i','ie']\n",
    "ipa_graph2['k'] = ['k', 'ck', 'c','g','ch'] # Excluded cch for Zucchini, gg for Flaggschiff, qu for Boutique, kk for Akkordeon\n",
    "ipa_graph2['m'] = ['m', 'mm']\n",
    "ipa_graph2['b'] = ['b', 'bb']\n",
    "ipa_graph2['ʃ'] = ['sch', 's'] # excluded sk for Ski, sh for Sheriff, Show and ch for Recherche \n",
    "ipa_graph2['d'] = ['d','dd']\n",
    "ipa_graph2['p'] = ['p', 'b', 'pp'] # excluded bb for abebben or schrubben\n",
    "ipa_graph2['ŋ'] = ['ng','n']\n",
    "ipa_graph2['ɔ'] = ['o'] # excluded au for Chauffeur (very rare exception)\n",
    "ipa_graph2['v'] = ['w', 'v']\n",
    "ipa_graph2['ʊ'] = ['u']\n",
    "ipa_graph2['z'] = ['s'] \n",
    "ipa_graph2['h'] = ['h']\n",
    "ipa_graph2['ʏ'] = ['ue', 'u','y'] # The corpus is weird here and writes Druck as drʏk, i.e. \"Drück\" rather than drʊk\n",
    "ipa_graph2['x'] = ['ch']\n",
    "ipa_graph2['j'] = ['j']\n",
    "ipa_graph2['œ'] = ['oe']\n",
    "\n",
    "\n",
    "# 2 character keys:\n",
    "ipa_graph2['ts'] = ['z', 'tz'] # excluded zz for Pizza/Skizze, c for circa, Penicillin, tts for trittst\n",
    "            # t for Aktion, Negation, Infektion, Proportion, ...\n",
    "ipa_graph2['aː'] = ['a', 'ah', 'aa']\n",
    "\n",
    "ipa_graph2['aɪ'] = ['ei', 'ai'] # excluded ail for Detail, aill for Medaillon, aille for Medaille and y for Nylon\n",
    "ipa_graph2['iː'] = ['ie', 'i', 'ih','ieh'] # excluded ieh for Entziehung\n",
    "ipa_graph2['eː'] = ['e', 'ee', 'eh'] # excluded et like in Bidet\n",
    "ipa_graph2['ɛː'] = ['ae', 'aeh']\n",
    "ipa_graph2['uː'] = ['u', 'uh'] # excluded ou like in Ragout, Limousine and oo like on zoomen/Cartoon\n",
    "ipa_graph2['oː'] = ['o', 'oh', 'oo'] # excluded au for aubergine/sauce and eau for plateau, Niveau\n",
    "ipa_graph2['yː'] = ['ue', 'ueh'] # excluded uet like in Debüt and u like in deja-vu\n",
    "ipa_graph2['ɔy'] = ['eu', 'aeu'] # instead of what table calls 'ɔɪ'\n",
    "ipa_graph2['ks'] = ['chs', 'x', 'ks','cks','gs']\n",
    "ipa_graph2['øː'] = ['oe', 'oeh'] # excluded eu like in Ingenieur and eue like in Queue (?)\n",
    "ipa_graph2['aʊ'] = ['au']\n",
    "ipa_graph2['pf'] = ['pf']\n",
    "\n",
    "# The Jäger table itself is not sufficient. E.g. there are occurrences of y without yː and \n",
    "# since only the latter has a dict entry, we would get a key error. \n",
    "# Thus we add some dict entries manually\n",
    "ipa_graph2['y'] = ['y']\n",
    "ipa_graph2['ə'] = ['e']\n",
    "ipa_graph2['i'] = ['i'] \n",
    "ipa_graph2['u'] = ['u'] \n",
    "ipa_graph2['kv'] = ['qu']\n",
    "ipa_graph2['ˈ'] = [''] # Just a pronounciation symbol, does not carry meaning for spelling\n",
    "ipa_graph2['ː'] = ['']\n",
    "\n",
    "\n",
    "print(len(sampa_ipa), len(ipa_graph))\n",
    "# We had 36 keys in sampa_ipa dict, now we have 46 already in ipa_graph due to 2-phoneme-groups\n",
    "# But still there are one-char-values in sampa_ipa which are not keys in ipa_graph. Let us print them:\n",
    "for key in sampa_ipa.values():\n",
    "    if key not in ipa_graph2.keys():\n",
    "        print(key)\n",
    "        \n",
    "# Okay 4 are missing:\n",
    "#ipa_graph['ː'] = [''] # not needed anyways since ː always occurs after vowel\n",
    "# Then the empty string '' is not needed as key\n",
    "# Then ø only occurs followed by a ː\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'w': 1, 'h': 2, 's': 3, 'e': 4, 'y': 5, 'b': 6, 'u': 7, 'f': 8, 'z': 9, 'g': 10, ' ': 11, 'k': 12, 'o': 13, 'n': 14, 'i': 15, 'v': 16, 'x': 17, 'q': 18, 'p': 19, 'm': 20, 'l': 21, 'j': 22, 'c': 23, 'd': 24, 'r': 25, 'a': 26, 't': 27, '<GO>': 28, '<PAD>': 29}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pretty tough to draw the boundary between which phons rarely but \"regularly\" translate to grapheme strings ([k] to kk like in Akkordeon, Mokka is still regular?) whereas [ʃ] (sch) to ch like in Champagner, Recherche or Lunch is irregular?\n",
    "\n",
    "## Use the IPA->Graphem dict to create alternative writings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_word(word, ipa_graph2):\n",
    "    \"\"\"\n",
    "    Splits up an IPA word into a list of lists each with the possible replacement grapheme for each phoneme\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    WORD       {list} in IPA notation\n",
    "    IPA_GRAPH  {dict} mapping IPA symbols to possible grapheme sequences\n",
    "    \n",
    "    Returns:\n",
    "    ---------\n",
    "    CHARS      {list} containing lists with possible grapheme sequences\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    chars = []\n",
    "    single_key = True\n",
    "    for ind in range(len(word)-1):\n",
    "        \n",
    "        if single_key: \n",
    "            if word[ind:ind+2] in ipa_graph2:\n",
    "                chars.append(ipa_graph2[word[ind:ind+2]])\n",
    "                single_key = False\n",
    "            else:\n",
    "                chars.append(ipa_graph2[word[ind]])\n",
    "        else:\n",
    "            single_key = True\n",
    "            \n",
    "    chars.append(ipa_graph2[word[ind+1]])\n",
    "    \n",
    "    return chars\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The method below generates the alternative writings (takes some time...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_writings():\n",
    "    all_writings = []\n",
    "    m = 0\n",
    "\n",
    "    for ind,ip in enumerate(ipa):\n",
    "        if ind % 5000 == 0:\n",
    "            print(\"Currently examining word \", ind)\n",
    "\n",
    "        word_lists = split_word(ip, ipa_graph2)\n",
    "        alt_write_raw = list(itertools.product(*word_lists))\n",
    "        alt_write = [''.join(a) for a in alt_write_raw]\n",
    "        try:\n",
    "            alt_write.remove(words[ind])\n",
    "        except ValueError:\n",
    "            _ = 1\n",
    "\n",
    "        all_writings.append(alt_write)\n",
    "\n",
    "        \"\"\"        \n",
    "        if len(alt_write) > m:\n",
    "            print(len(alt_write),ind)\n",
    "            m = len(alt_write)\"\"\"\n",
    "    print(\"DONE! Alternative writings generated. Resulting list has\", len(all_writings), 'entries.')    \n",
    "    return all_writings\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now convert the list of alternative writings into a format that can be saved to disk. Tried A LOT of things here. We could use int8 as datatype since the values are in the range of [0,num_dec_symbols], but the arrays have varying size which numpy cannot handle effciently. Low storage solution: Use np.save to save a list that contains for each word of the corpus a list with num_alt_writings lists, each containing a possible target sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(all_writings):\n",
    "\n",
    "    print(words_num.shape)\n",
    "    seq_len = words_num.shape[1]\n",
    "    max_alt_spellings = max(len(l) for l in all_writings)\n",
    "    num_alt_writings = []\n",
    "\n",
    "    m = 0\n",
    "    for wo_ind in range(len(all_writings)):\n",
    "\n",
    "        if wo_ind % 1000 == 0:\n",
    "            print(\"Currently converting word\", wo_ind)\n",
    "        tmp = []\n",
    "        \n",
    "        for tar_ind in range(len(all_writings[wo_ind])):\n",
    "            l = seq_len - len(all_writings[wo_ind][tar_ind])\n",
    "            num = [word_dict['<PAD>']]*l + [word_dict[k] for k in all_writings[wo_ind][tar_ind]]\n",
    "            tmp.append(num)\n",
    "\n",
    "        num_alt_writings.append(tmp)\n",
    "            \n",
    "    return num_alt_writings\n",
    "\n",
    "# Alternatives:\n",
    "\n",
    "#num_alt_writings = dict()\n",
    "#num_alt_writings = np.array([])\n",
    "#num_alt_writings = np.zeros((words_num.shape[0],seq_len),dtype=np.int32)\n",
    "#num_alt_writings = np.zeros((words_num.shape[0],seq_len,max_alt_spellings),dtype=np.int8)\n",
    "\n",
    "    #for wo_ind in range(num_alt_writings.shape[0]):\n",
    "\n",
    "        #tmp = np.zeros((seq_len, len(all_writings[wo_ind])),dtype=np.int8)\n",
    "        \n",
    "            #num = np.array([word_dict['<PAD>']]*l + [word_dict[k] for k in all_writings[wo_ind][tar_ind]])\n",
    "            #tmp[:,tar_ind] = num.astype(np.int8)\n",
    "            \n",
    "        #num_alt_writings = np.array([num_alt_writings,num])\n",
    "        #num_alt_writings[wo_ind,:,tar_ind] = num.astype(np.int8)\n",
    "        \n",
    "    #num_alt_writings = np.array([num_alt_writings,tmp])\n",
    "    #num_alt_writings[wo_ind] = tmp\n",
    "    #num_alt_writings = np.append(num_alt_writings,tmp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally execute the whole pipeline for CELEX dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A a:\n",
      "aalen a:l\n",
      "aalglatt a:l#glat\n",
      "Aasgeier a:z#gai@r\n",
      "abaenderlich ap#End@r#lIx\n",
      "abbauwuerdig ap#bau#vYrdIx\n",
      "Abbrucharbeit ap#brEx#arbait\n",
      "Excluded 87 words because they were too long (more than 15 phons)\n",
      "Size of dataset is 31877 samples\n",
      "Currently examining word  0\n",
      "Currently examining word  5000\n",
      "Currently examining word  10000\n",
      "Currently examining word  15000\n",
      "Currently examining word  20000\n",
      "Currently examining word  25000\n",
      "Currently examining word  30000\n",
      "DONE! Alternative writings generated. Resulting list has 31877 entries.\n"
     ]
    }
   ],
   "source": [
    "words,phons = extract_celex(path)\n",
    "((phons_num, words_num), (phon_dict, word_dict)) = str_to_num_dataset(phons,words)\n",
    "alt_writings_str = generate_writings()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31877, 28)\n",
      "Currently converting word 0\n",
      "Currently converting word 1000\n",
      "Currently converting word 2000\n",
      "Currently converting word 3000\n",
      "Currently converting word 4000\n",
      "Currently converting word 5000\n",
      "Currently converting word 6000\n",
      "Currently converting word 7000\n",
      "Currently converting word 8000\n",
      "Currently converting word 9000\n",
      "Currently converting word 10000\n",
      "Currently converting word 11000\n",
      "Currently converting word 12000\n",
      "Currently converting word 13000\n",
      "Currently converting word 14000\n",
      "Currently converting word 15000\n",
      "Currently converting word 16000\n",
      "Currently converting word 17000\n",
      "Currently converting word 18000\n",
      "Currently converting word 19000\n",
      "Currently converting word 20000\n",
      "Currently converting word 21000\n",
      "Currently converting word 22000\n",
      "Currently converting word 23000\n",
      "Currently converting word 24000\n",
      "Currently converting word 25000\n",
      "Currently converting word 26000\n",
      "Currently converting word 27000\n",
      "Currently converting word 28000\n",
      "Currently converting word 29000\n",
      "Currently converting word 30000\n",
      "Currently converting word 31000\n"
     ]
    }
   ],
   "source": [
    "alt_writings_num = convert(alt_writings_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data\n",
    "import pickle, sys\n",
    "path = \"/Users/jannisborn/Desktop/LDS_Data/celex_alt_targets.npy\"\n",
    "#pickle.dump(alt_writings_num, open(path, \"wb\"))\n",
    "\n",
    "max_bytes = 2**31 - 1\n",
    "bytes_out = pickle.dumps(alt_writings_num)\n",
    "n_bytes = sys.getsizeof(bytes_out)\n",
    "with open(path, 'wb') as f_out:\n",
    "    for idx in range(0, n_bytes, max_bytes):\n",
    "        f_out.write(bytes_out[idx:idx+max_bytes])\n",
    "#np.save(path,alt_writings_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_to_load_as_pickled_object_or_None(filepath):\n",
    "    \"\"\"\n",
    "    This is a defensive way to write pickle.load, allowing for very large files on all platforms\n",
    "    \"\"\"\n",
    "    max_bytes = 2**31 - 1\n",
    "    try:\n",
    "        input_size = os.path.getsize(filepath)\n",
    "        bytes_in = bytearray(0)\n",
    "        with open(filepath, 'rb') as f_in:\n",
    "            for _ in range(0, input_size, max_bytes):\n",
    "                bytes_in += f_in.read(max_bytes)\n",
    "        obj = pickle.loads(bytes_in)\n",
    "    except:\n",
    "        return None\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "27.2 s ± 354 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "a = try_to_load_as_pickled_object_or_None(path)\n",
    "print(type(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "25.1 s ± 229 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "a = np.load(\"/Users/jannisborn/Desktop/LDS_Data/celex_alt_targets.npy\")\n",
    "print(type(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract childlex data from downloaded CSV and clean it up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of words in childlex 9769\n",
      "Set of characters in childlex words is ['a', 'e', 'c', 'h', 'z', 'n', 'd', 'f', 'm', 'l', 'i', 'r', 'g', 's', 't', 'u', 'o', 'b', 'k', 'p', 'q', 'w', 'v', 'x', 'j', 'y']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "path = 'data/childlex_6-8_lemmata.csv'\n",
    "forbidden = \"_,.!?*:=&%- /\\\\–•»«…›‹()[]{}’'\"\n",
    "\n",
    "\n",
    "def rem_um(word):\n",
    "    \"\"\"\n",
    "    Converts a word with German umlauts (ü,ä,ö) into a word without\n",
    "    \"\"\"\n",
    "        \n",
    "    umlautfree = str()\n",
    "    for char in word:\n",
    "        if char == 'ä':\n",
    "            umlautfree += 'ae'\n",
    "        elif char == 'ü':\n",
    "            umlautfree += 'ue'\n",
    "        elif char == 'ö':\n",
    "            umlautfree += 'oe'\n",
    "        elif char == 'ß':\n",
    "            umlautfree += 'ss'\n",
    "        else:\n",
    "            umlautfree += char\n",
    "    return umlautfree\n",
    "\n",
    "\n",
    "with open(path, 'r') as csvfile:\n",
    "    \n",
    "    raw_file = csv.reader(csvfile,delimiter=';')\n",
    "    raw_data = []\n",
    "    \n",
    "    \n",
    "    for line in raw_file:\n",
    "        umlautfree = rem_um(line[0].lower())\n",
    "        \n",
    "       # data cleanup\n",
    "        has_digit = any(char.isdigit() for char in umlautfree)\n",
    "        has_sonderz = any(char in forbidden for char in umlautfree)\n",
    "        \n",
    "        if not has_digit and not has_sonderz:\n",
    "        \n",
    "            raw_data.append(umlautfree)\n",
    "        \n",
    "        \n",
    "    \n",
    "childlex_words_all = raw_data\n",
    "print(\"Amount of words in childlex\", len(childlex_words_all))\n",
    "    \n",
    "chars = []\n",
    "for word in childlex_words_all:\n",
    "    for char in word:\n",
    "        if char not in chars:\n",
    "            chars.append(char)\n",
    "    \n",
    "print(\"Set of characters in childlex words is\", chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look up phonetic sequences in CELEX Database. \n",
    "#### Problem: We do not have phonetic sequences yet. Idea: Retrieve as many phonetic sequences as possible from the CELEX database, convert this database into numerical values and save it.\n",
    "#### Also retrieve the alternative writings for these words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31877\n"
     ]
    }
   ],
   "source": [
    "#celex_alt_targs = np.load(\"/Users/jannisborn/Desktop/LDS_Data/celex_alt_targets.npy\")\n",
    "print(len(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "childlex_phons = []\n",
    "childlex_words = []\n",
    "childlex_alt_writings = []\n",
    "for word in childlex_words_all:\n",
    "    if word in words:\n",
    "        ind = words.index(word)\n",
    "        childlex_alt_writings.append(a[ind])\n",
    "        childlex_phons.append(phons[ind])\n",
    "        childlex_words.append(words[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From the 9769 words in CHILDLEX database 5891 were succesfully retrieved\n"
     ]
    }
   ],
   "source": [
    "print(\"From the\", len(childlex_words_all),\"words in CHILDLEX database\", len(childlex_words),\"were succesfully retrieved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_num_with_dict(X,Y,dic_X,dic_Y,pads=10):\n",
    "    \"\"\"\n",
    "    This method receives 2 lists of strings (input X and output Y) and converts it to padded, numerical arrays - \n",
    "    based on a dictionary given as third argument.\n",
    "    It returns the numerical dataset .\n",
    "    PADS    {int} specifiying how many additional fields should be padded (to allow long words to have longer alt. writings)\n",
    "    \"\"\"\n",
    "\n",
    "    mx_l_X = max([len(word) for word in X]) # longest input sequence\n",
    "    # Padd all X for the final form for the LSTM\n",
    "    x = [[dic_X['<GO>']] + [dic_X['<PAD>']]*(mx_l_X - len(word)) +[dic_X[char] for char in word] for word in X]\n",
    "    x = np.array(x) \n",
    "\n",
    "    # Pad targets\n",
    "\n",
    "    mx_l_Y = max([len(phon_seq) for phon_seq in Y]) # longest output sequence\n",
    "\n",
    "    y = [[dic_Y['<GO>']] + pads*[dic_Y['<PAD>']] + [dic_Y['<PAD>']]*(mx_l_Y - len(ph_sq)) + [dic_Y[phon] for phon in ph_sq] for ph_sq in Y]\n",
    "    y = np.array(y)\n",
    "\n",
    "    return (x,y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Important: Use the same dictionary like for CELEX (some writings are retrived from there)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29 {'w': 1, 'h': 2, 's': 3, 'e': 4, 'y': 5, 'b': 6, 'u': 7, 'f': 8, 'z': 9, 'g': 10, ' ': 11, 'k': 12, 'o': 13, 'n': 14, 'i': 15, 'v': 16, 'x': 17, 'q': 18, 'p': 19, 'm': 20, 'l': 21, 'j': 22, 'c': 23, 'd': 24, 'r': 25, 'a': 26, 't': 27, '<GO>': 28, '<PAD>': 29}\n",
      "39 {'N': 1, 'h': 2, 's': 3, 'e': 4, 'y': 5, 'b': 6, 'Y': 7, 'u': 8, 'f': 9, 'z': 10, 'g': 11, ' ': 12, 'E': 13, 'k': 14, 'o': 15, 'n': 16, 'i': 17, 'v': 18, 'x': 19, ':': 20, 't': 21, 'I': 22, '#': 23, 'U': 24, 'p': 25, 'S': 26, 'm': 27, '|': 28, 'l': 29, 'j': 30, '+': 31, 'O': 32, 'd': 33, 'r': 34, 'a': 35, '/': 36, '@': 37, '<GO>': 38, '<PAD>': 39}\n",
      "(5891, 27) (5891, 15)\n"
     ]
    }
   ],
   "source": [
    "(phons_num, words_num) = str_to_num_with_dict(childlex_phons, childlex_words, phon_dict, word_dict)\n",
    "print(len(word_dict),word_dict)\n",
    "print(len(phon_dict),phon_dict)\n",
    "print(words_num.shape, phons_num.shape)\n",
    "np.savez(\"data/childlex.npz\", words=words_num, phons=phons_num, word_dict=word_dict, phon_dict=phon_dict)\n",
    "np.savez(\"/Users/jannisborn/Desktop/LDS_Data/childlex.npz\", words=words_num, phons=phons_num, word_dict=word_dict, phon_dict=phon_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the alternative writings\n",
    "np.save(\"/Users/jannisborn/Desktop/LDS_Data/childlex_alt_targets2.npy\", childlex_alt_writings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract data from the FIBEL dataset\n",
    "##### Focus on the \"Mia and Mo\" Fibel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has 174 words, from which 115 could be retrieved from celex\n"
     ]
    }
   ],
   "source": [
    "fibel_path = \"/Users/jannisborn/Dropbox/GitHub/LSTM/LdS_bLSTM/Code/data/Fibelwörter.txt\"\n",
    "celex_alt_targs = a\n",
    "\n",
    "with open(fibel_path, 'r') as txtfile:\n",
    "    fibel_words = []\n",
    "    for line in txtfile.read().split(','):\n",
    "        # Use lowercase letters only, remove leading whitespace and take care of line breaks.\n",
    "        if '\\n' in line:    \n",
    "            for item in line.split('\\n'):\n",
    "                if item.lstrip().lower() not in fibel_words:\n",
    "                    fibel_words.append(rem_um(item.lstrip().lower()))\n",
    "        elif line.lstrip().lower() not in fibel_words:\n",
    "            fibel_words.append(rem_um(line.lstrip().lower()))\n",
    "        \n",
    "lektions_inds = [9,14,20,28,36,46,58,77,98,120,153,173]\n",
    "\n",
    "# Retrieve some phonetic transcripts and alternative writings from CELEX\n",
    "fibel_phons = []\n",
    "fibel_alt_writings = []\n",
    "k=0\n",
    "for w in fibel_words:\n",
    "    if w in words:\n",
    "        ind = words.index(w)\n",
    "        fibel_phons.append(phons[ind])\n",
    "        fibel_alt_writings.append(celex_alt_targs[ind])\n",
    "        \n",
    "    else:\n",
    "        fibel_phons.append('NO')\n",
    "        fibel_alt_writings.append('NO')\n",
    "        k +=1\n",
    "print(\"The dataset has \"+str(len(fibel_words))+\" words, from which \"+str(len(fibel_words)-k)+\" could be retrieved from celex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate the SAMPA spellings for the missing words. First step: Copy the IPA values from wiktionary + convert IPA into SAMPA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YAY, all words have phonetic transcript\n"
     ]
    }
   ],
   "source": [
    "ipa_sampa = dict(zip(sampa_ipa.values(), sampa_ipa.keys()))\n",
    "\n",
    "fibel_phons[fibel_words.index('mia')] = ''.join([ipa_sampa[w] for w in 'miːa'])\n",
    "fibel_phons[fibel_words.index('mo')] = ''.join([ipa_sampa[w] for w in 'moː'])\n",
    "fibel_phons[fibel_words.index('mimi')] = ''.join([ipa_sampa[w] for w in 'mimi'])\n",
    "fibel_phons[fibel_words.index('im')] = ''.join([ipa_sampa[w] for w in 'ɪm'])\n",
    "fibel_phons[fibel_words.index('am')] = ''.join([ipa_sampa[w] for w in 'am'])\n",
    "fibel_phons[fibel_words.index('momo')] = ''.join([ipa_sampa[w] for w in 'moːmoː'])\n",
    "fibel_phons[fibel_words.index('omi')] = ''.join([ipa_sampa[w] for w in 'oːmiː'])\n",
    "fibel_phons[fibel_words.index('radio')] = ''.join([ipa_sampa[w] for w in 'raːdioː'])\n",
    "fibel_phons[fibel_words.index('sissi')] = ''.join([ipa_sampa[w] for w in 'zɪsɪː'])\n",
    "fibel_phons[fibel_words.index('susi')] = ''.join([ipa_sampa[w] for w in 'susi'])\n",
    "fibel_phons[fibel_words.index('oli')] = ''.join([ipa_sampa[w] for w in 'ɔliː'])\n",
    "fibel_phons[fibel_words.index('salami')] = ''.join([ipa_sampa[w] for w in 'zaˈlaːmi'])\n",
    "fibel_phons[fibel_words.index('ist')] = ''.join([ipa_sampa[w] for w in 'ɪst'])\n",
    "fibel_phons[fibel_words.index('tim')] = ''.join([ipa_sampa[w] for w in 'tɪm'])\n",
    "fibel_phons[fibel_words.index('tom')] = ''.join([ipa_sampa[w] for w in 'tɔm'])\n",
    "fibel_phons[fibel_words.index('mario')] = ''.join([ipa_sampa[w] for w in 'ˈmaːrioː'])\n",
    "fibel_phons[fibel_words.index('isa')] = ''.join([ipa_sampa[w] for w in 'iːza'])\n",
    "fibel_phons[fibel_words.index('maria')] = ''.join([ipa_sampa[w] for w in 'maˈriːa'])\n",
    "fibel_phons[fibel_words.index('rosarot')] = ''.join([ipa_sampa[w] for w in 'ˈroːzaˈroːt'])\n",
    "fibel_phons[fibel_words.index('nimm')] = ''.join([ipa_sampa[w] for w in 'nɪm'])\n",
    "fibel_phons[fibel_words.index('nimmt')] = ''.join([ipa_sampa[w] for w in 'nɪmt'])\n",
    "fibel_phons[fibel_words.index('nina')] = ''.join([ipa_sampa[w] for w in 'niːna'])\n",
    "fibel_phons[fibel_words.index('rosinen')] = ''.join([ipa_sampa[w] for w in 'roːziːnən'])\n",
    "fibel_phons[fibel_words.index('anna')] = ''.join([ipa_sampa[w] for w in 'ˈana'])\n",
    "fibel_phons[fibel_words.index('bananen')] = ''.join([ipa_sampa[w] for w in 'baˈnaːnən'])\n",
    "fibel_phons[fibel_words.index('birnen')] = ''.join([ipa_sampa[w] for w in 'bɪrnən'])\n",
    "fibel_phons[fibel_words.index('nuesse')] = ''.join([ipa_sampa[w] for w in 'nʏsə'])\n",
    "fibel_phons[fibel_words.index('weintrauben')] = ''.join([ipa_sampa[w] for w in 'ˈvaintraʊbn'])\n",
    "fibel_phons[fibel_words.index('maroni')] = ''.join([ipa_sampa[w] for w in 'maˈroːni'])\n",
    "fibel_phons[fibel_words.index('erna')] = ''.join([ipa_sampa[w] for w in 'ˈɛrna'])\n",
    "fibel_phons[fibel_words.index('die')] = ''.join([ipa_sampa[w] for w in 'diː'])\n",
    "fibel_phons[fibel_words.index('das')] = ''.join([ipa_sampa[w] for w in 'das'])\n",
    "fibel_phons[fibel_words.index('sind')] = ''.join([ipa_sampa[w] for w in 'zɪnt'])\n",
    "fibel_phons[fibel_words.index('domino')] = ''.join([ipa_sampa[w] for w in 'ˈdoːminoː'])\n",
    "fibel_phons[fibel_words.index('indianer')] = ''.join([ipa_sampa[w] for w in 'ɪnˈdiaːnər'])\n",
    "fibel_phons[fibel_words.index('tonio')] = ''.join([ipa_sampa[w] for w in 'tɔnioː'])\n",
    "fibel_phons[fibel_words.index('sagt')] = ''.join([ipa_sampa[w] for w in 'zaːkt'])\n",
    "fibel_phons[fibel_words.index('italien')] = ''.join([ipa_sampa[w] for w in 'iˈtaːliən'])\n",
    "fibel_phons[fibel_words.index('ute')] = ''.join([ipa_sampa[w] for w in 'uːtə'])\n",
    "fibel_phons[fibel_words.index('umarmt')] = ''.join([ipa_sampa[w] for w in 'ʊmˈarmt'])\n",
    "fibel_phons[fibel_words.index('runter')] = ''.join([ipa_sampa[w] for w in 'rʊntər'])\n",
    "fibel_phons[fibel_words.index('turnen')] = ''.join([ipa_sampa[w] for w in 'tʊrnən'])\n",
    "fibel_phons[fibel_words.index('dem')] = ''.join([ipa_sampa[w] for w in 'deːm'])\n",
    "fibel_phons[fibel_words.index('rennt')] = ''.join([ipa_sampa[w] for w in 'rɛnt'])\n",
    "fibel_phons[fibel_words.index('ene')] = ''.join([ipa_sampa[w] for w in 'eːnə'])\n",
    "fibel_phons[fibel_words.index('mene')] = ''.join([ipa_sampa[w] for w in 'meːnə'])\n",
    "fibel_phons[fibel_words.index('mu')] = ''.join([ipa_sampa[w] for w in 'muː'])\n",
    "fibel_phons[fibel_words.index('turnt')] = ''.join([ipa_sampa[w] for w in 'ˈtʊrnt'])\n",
    "fibel_phons[fibel_words.index('miauen')] = ''.join([ipa_sampa[w] for w in 'miˈaʊən'])\n",
    "fibel_phons[fibel_words.index('raus')] = ''.join([ipa_sampa[w] for w in 'raʊs'])\n",
    "fibel_phons[fibel_words.index('mauert')] = ''.join([ipa_sampa[w] for w in 'tɔm'])\n",
    "fibel_phons[fibel_words.index('saust')] = ''.join([ipa_sampa[w] for w in 'zaʊst'])\n",
    "fibel_phons[fibel_words.index('rosaroten')] = ''.join([ipa_sampa[w] for w in 'roːzaˈroːtn'])\n",
    "fibel_phons[fibel_words.index('martin')] = ''.join([ipa_sampa[w] for w in 'ˈmartiːn'])\n",
    "fibel_phons[fibel_words.index('martins')] = ''.join([ipa_sampa[w] for w in 'ˈmartiːns'])\n",
    "fibel_phons[fibel_words.index('eine')] = ''.join([ipa_sampa[w] for w in 'ˈaɪnə'])\n",
    "fibel_phons[fibel_words.index('miaut')] = ''.join([ipa_sampa[w] for w in 'miˈaʊt'])\n",
    "fibel_phons[fibel_words.index('otto')] = ''.join([ipa_sampa[w] for w in 'ˈɔtoː'])\n",
    "fibel_phons[fibel_words.index('isst')] = ''.join([ipa_sampa[w] for w in 'ɪst'])\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    fibel_phons.index('NO')\n",
    "except ValueError:\n",
    "    print(\"YAY, all words have phonetic transcript\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Fibel dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "(phons_num, words_num) = str_to_num_with_dict(fibel_phons,fibel_words, phon_dict, word_dict)\n",
    "np.savez(\"data/fibel.npz\", words=words_num, phons=phons_num, word_dict=word_dict, phon_dict=phon_dict)\n",
    "np.savez(\"/Users/jannisborn/Desktop/LDS_Data/fibel.npz\", words=words_num, phons=phons_num, word_dict=word_dict, phon_dict=phon_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "fibel_alt_writings = []\n",
    "for ind,word in enumerate(fibel_phons):\n",
    "    \n",
    "    # If word is in CELEX, just retrieve alternative writings from there\n",
    "    if word in words:\n",
    "        \n",
    "        ind = words.index(word)\n",
    "        fibel_alt_writings.append(a[ind])\n",
    "        \n",
    "    # Otherwise generate alternative writings\n",
    "    else:        \n",
    "        # Convert SAMPA to IPA\n",
    "        ipa = ''.join([sampa_ipa[w] for w in word])\n",
    "        # generate string writings\n",
    "        word_lists = split_word(ipa, ipa_graph2)\n",
    "        alt_write_raw = list(itertools.product(*word_lists))\n",
    "        alt_write = [''.join(a) for a in alt_write_raw]\n",
    "        try:\n",
    "            alt_write.remove(words[ind])\n",
    "        except ValueError:\n",
    "            _ = 1\n",
    "       \n",
    "        # Convert to numerical\n",
    "        tmp = []\n",
    "        for tar_ind in range(len(alt_write)):\n",
    "            l = 28 - len(alt_write[tar_ind])\n",
    "            num = [word_dict['<PAD>']]*l + [word_dict[k] for k in alt_write[tar_ind]]\n",
    "            tmp.append(num)\n",
    "\n",
    "        fibel_alt_writings.append(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the alternative writings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"/Users/jannisborn/Desktop/LDS_Data/fibel_alt_targets.npy\", fibel_alt_writings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.load('/Users/jannisborn/Desktop/LDS_Data/data/celex_alt_targets_small.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 26, 4, 2]]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-147-4ac5fd2a0136>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "print(a[1])\n",
    "print(a[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar = [np.array(d,dtype=np.int8) for d in a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module tensorflow.python.framework.ops in tensorflow.python.framework:\n",
      "\n",
      "tensorflow.python.framework.ops = <module 'tensorflow.python.framework.ops' from '...ite-packages/tensorflow/python/framework/ops.py'>\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.framework import ops\n",
    "\n",
    "help(ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Const_3:0' shape=(1,) dtype=float32>"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array_ops.constant([2], ops.dtypes.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Const_1:0' shape=(1,) dtype=float32>"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on DType in module tensorflow.python.framework.dtypes object:\n",
      "\n",
      "class DType(builtins.object)\n",
      " |  Represents the type of the elements in a `Tensor`.\n",
      " |  \n",
      " |  The following `DType` objects are defined:\n",
      " |  \n",
      " |  * `tf.float16`: 16-bit half-precision floating-point.\n",
      " |  * `tf.float32`: 32-bit single-precision floating-point.\n",
      " |  * `tf.float64`: 64-bit double-precision floating-point.\n",
      " |  * `tf.bfloat16`: 16-bit truncated floating-point.\n",
      " |  * `tf.complex64`: 64-bit single-precision complex.\n",
      " |  * `tf.complex128`: 128-bit double-precision complex.\n",
      " |  * `tf.int8`: 8-bit signed integer.\n",
      " |  * `tf.uint8`: 8-bit unsigned integer.\n",
      " |  * `tf.uint16`: 16-bit unsigned integer.\n",
      " |  * `tf.uint32`: 32-bit unsigned integer.\n",
      " |  * `tf.uint64`: 64-bit unsigned integer.\n",
      " |  * `tf.int16`: 16-bit signed integer.\n",
      " |  * `tf.int32`: 32-bit signed integer.\n",
      " |  * `tf.int64`: 64-bit signed integer.\n",
      " |  * `tf.bool`: Boolean.\n",
      " |  * `tf.string`: String.\n",
      " |  * `tf.qint8`: Quantized 8-bit signed integer.\n",
      " |  * `tf.quint8`: Quantized 8-bit unsigned integer.\n",
      " |  * `tf.qint16`: Quantized 16-bit signed integer.\n",
      " |  * `tf.quint16`: Quantized 16-bit unsigned integer.\n",
      " |  * `tf.qint32`: Quantized 32-bit signed integer.\n",
      " |  * `tf.resource`: Handle to a mutable resource.\n",
      " |  * `tf.variant`: Values of arbitrary types.\n",
      " |  \n",
      " |  In addition, variants of these types with the `_ref` suffix are\n",
      " |  defined for reference-typed tensors.\n",
      " |  \n",
      " |  The `tf.as_dtype()` function converts numpy types and string type\n",
      " |  names to a `DType` object.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __eq__(self, other)\n",
      " |      Returns True iff this DType refers to the same type as `other`.\n",
      " |  \n",
      " |  __hash__(self)\n",
      " |      Return hash(self).\n",
      " |  \n",
      " |  __init__(self, type_enum)\n",
      " |      Creates a new `DataType`.\n",
      " |      \n",
      " |      NOTE(mrry): In normal circumstances, you should not need to\n",
      " |      construct a `DataType` object directly. Instead, use the\n",
      " |      `tf.as_dtype()` function.\n",
      " |      \n",
      " |      Args:\n",
      " |        type_enum: A `types_pb2.DataType` enum value.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If `type_enum` is not a value `types_pb2.DataType`.\n",
      " |  \n",
      " |  __int__(self)\n",
      " |  \n",
      " |  __ne__(self, other)\n",
      " |      Returns True iff self != other.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  is_compatible_with(self, other)\n",
      " |      Returns True if the `other` DType will be converted to this DType.\n",
      " |      \n",
      " |      The conversion rules are as follows:\n",
      " |      \n",
      " |      ```python\n",
      " |      DType(T)       .is_compatible_with(DType(T))        == True\n",
      " |      DType(T)       .is_compatible_with(DType(T).as_ref) == True\n",
      " |      DType(T).as_ref.is_compatible_with(DType(T))        == False\n",
      " |      DType(T).as_ref.is_compatible_with(DType(T).as_ref) == True\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        other: A `DType` (or object that may be converted to a `DType`).\n",
      " |      \n",
      " |      Returns:\n",
      " |        True if a Tensor of the `other` `DType` will be implicitly converted to\n",
      " |        this `DType`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  as_datatype_enum\n",
      " |      Returns a `types_pb2.DataType` enum value based on this `DType`.\n",
      " |  \n",
      " |  as_numpy_dtype\n",
      " |      Returns a `numpy.dtype` based on this `DType`.\n",
      " |  \n",
      " |  base_dtype\n",
      " |      Returns a non-reference `DType` based on this `DType`.\n",
      " |  \n",
      " |  is_bool\n",
      " |      Returns whether this is a boolean data type\n",
      " |  \n",
      " |  is_complex\n",
      " |      Returns whether this is a complex floating point type.\n",
      " |  \n",
      " |  is_floating\n",
      " |      Returns whether this is a (non-quantized, real) floating point type.\n",
      " |  \n",
      " |  is_integer\n",
      " |      Returns whether this is a (non-quantized) integer type.\n",
      " |  \n",
      " |  is_numpy_compatible\n",
      " |  \n",
      " |  is_quantized\n",
      " |      Returns whether this is a quantized data type.\n",
      " |  \n",
      " |  is_unsigned\n",
      " |      Returns whether this type is unsigned.\n",
      " |      \n",
      " |      Non-numeric, unordered, and quantized types are not considered unsigned, and\n",
      " |      this function returns `False`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Whether a `DType` is unsigned.\n",
      " |  \n",
      " |  limits\n",
      " |      Return intensity limits, i.e. (min, max) tuple, of the dtype.\n",
      " |      Args:\n",
      " |        clip_negative : bool, optional\n",
      " |            If True, clip the negative range (i.e. return 0 for min intensity)\n",
      " |            even if the image dtype allows negative values.\n",
      " |      Returns\n",
      " |        min, max : tuple\n",
      " |          Lower and upper intensity limits.\n",
      " |  \n",
      " |  max\n",
      " |      Returns the maximum representable value in this data type.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: if this is a non-numeric, unordered, or quantized type.\n",
      " |  \n",
      " |  min\n",
      " |      Returns the minimum representable value in this data type.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: if this is a non-numeric, unordered, or quantized type.\n",
      " |  \n",
      " |  name\n",
      " |      Returns the string name for this `DType`.\n",
      " |  \n",
      " |  real_dtype\n",
      " |      Returns the dtype correspond to this dtype's real part.\n",
      " |  \n",
      " |  size\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.InteractiveSession()\n",
    "\n",
    "a = tf.zeros((3,),dtype=tf.int32)\n",
    "\n",
    "b = tf.constant([[[1,2,3],[0,0,0]],[[1,2,3],[0,0,0]]],dtype=tf.int64)\n",
    "print(b[1,1,:].shape)\n",
    "if tf.reduce_all(tf.equal(a,b[1,1,:])).eval():\n",
    "    print(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.zeros((10,1))\n",
    "b = a\n",
    "for k in range(10):\n",
    "    \n",
    "    b = np.array([b,np.ones((k,1))])\n",
    "    print(b.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insight 1: If you have k different classes, then k is the third \n",
    "#    dimension of the logits and then the target matrix must not \n",
    "#    contain values higher than k-1 (i.e. labels are 0, ..., k)\n",
    "\n",
    "# Insight 2: sequence_loss expects unnormalized logits (BEFORE softmax!)\n",
    "# -> This makes testing for me harder, since I cannot simply use values\n",
    "# with a sum over 1 (will get normalized)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
