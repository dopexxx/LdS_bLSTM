{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jannisborn/anaconda3/envs/tf/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# Task: Come up with alternative spellings for all orthographic words in a database\n",
    "# Tool 1: IPA -> Graphem Konverter Wiki\n",
    "# Tool 2: SAMPA -> IPA Konverter Wiki\n",
    "\n",
    "# Idea: Take the phonetic sequence (SAMPA) of every word, convert it to an IPA sequence and\n",
    "        # then convert that to all grapheme sequences\n",
    "    \n",
    "# Needs: A dictionary for SAMPA -> IPA (ideally non-ambiguous). A dictionary for IPA -> Text (ambigu.)\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43 ['y', 'f', 'O', '/', 'N', 'e', 'k', '~', 'j', 'Z', 'd', 'i', 'b', 'r', 'S', 'v', 'm', 'o', 'l', 'E', 'Y', 's', '@', 'z', 'x', '{', 'A', ':', 'h', 'g', '+', 'U', 'I', 'n', 'p', ' ', 'u', 'a', '#', '|', 't', '<GO>', '<PAD>']\n"
     ]
    }
   ],
   "source": [
    "data = np.load('data/celex.npz')\n",
    "sampa_dict = {key:data['phon_dict'].item().get(key) for key in data['phon_dict'].item()}\n",
    "sampa_keys = list(sampa_dict.keys())\n",
    "print(len(sampa_dict), sampa_keys)\n",
    "\n",
    "\n",
    "# Step 1: Make a sampa_ipa dict. How to: Go to SAMPA -> IPA tabelle, for every SAMPA char, check whether it \n",
    "# is in the CELEX korpus. If yes, look up example word from wiki in korpus and check whether it is the right sampa\n",
    "# sign. If yes, look up on wiktionary example word in IPA and check whether output sign is correct.\n",
    "\n",
    "sampa_ipa = dict()\n",
    "sampa_ipa['i'] = 'i'\n",
    "sampa_ipa[':'] = 'ː'\n",
    "sampa_ipa['I'] = 'ɪ'\n",
    "sampa_ipa['e'] = 'e'\n",
    "sampa_ipa['E'] = 'ɛ'\n",
    "sampa_ipa['{'] = 'ɛ'\n",
    "sampa_ipa['I'] = 'ɪ'\n",
    "sampa_ipa['I'] = 'ɪ'\n",
    "sampa_ipa['I'] = 'ɪ'\n",
    "sampa_ipa['I'] = 'ɪ'\n",
    "sampa_ipa['I'] = 'ɪ'\n",
    "sampa_ipa['I'] = 'ɪ'\n",
    "sampa_ipa['I'] = 'ɪ'\n",
    "sampa_ipa['I'] = 'ɪ'\n",
    "sampa_ipa['I'] = 'ɪ'\n",
    "sampa_ipa['I'] = 'ɪ'\n",
    "sampa_ipa['I'] = 'ɪ'\n",
    "sampa_ipa['I'] = 'ɪ'\n",
    "sampa_ipa['I'] = 'ɪ'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print('{' in sampa_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jannisborn/anaconda3/envs/tf/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "def BAS_P2G_retrieve():\n",
    "    \"\"\"\n",
    "    Shortcut method for quickly retrieving numerical dataset of BAS-Sprecher corpus\n",
    "    In case whole dataset is not copied on remote machine\n",
    "    \"\"\"\n",
    "    data = np.load('data/BAS_P2G.npz')\n",
    "    input_dict = np_dict_to_dict(data['inp_dict'])\n",
    "    target_dict = np_dict_to_dict(data['tar_dict'])\n",
    "\n",
    "    return ( (data['inputs'], data['targets']) , (input_dict, target_dict) )\n",
    "\n",
    "def batch_data(x, y, BATCH_SIZE):\n",
    "    \"\"\"\n",
    "    Receives a batch_size and the entire training data [i.e inputs (x) and labels (y)]\n",
    "    Returns a data iterator\n",
    "    \"\"\"\n",
    "    shuffle = np.random.permutation(len(x))\n",
    "    start = 0\n",
    "    x = x[shuffle]\n",
    "    y = y[shuffle]\n",
    "    while start + BATCH_SIZE <= len(x):\n",
    "        yield x[start:start+BATCH_SIZE], y[start:start+BATCH_SIZE]\n",
    "        start += BATCH_SIZE\n",
    "\n",
    "def np_dict_to_dict(np_dict):\n",
    "    \"\"\"\n",
    "    Converts a dictionary saved via np.save (as structured np array) into an object of type dict\n",
    "\n",
    "    Parameters:\n",
    "    --------------\n",
    "    NP_DICT        : {np.array} structured np.array with dict keys and items\n",
    "\n",
    "    Returns:\n",
    "    --------------\n",
    "    DICT            : {dict} converted NP_DICT\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    return {key:np_dict.item().get(key) for key in np_dict.item()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def extract_celex(path):\n",
    "    \"\"\"\n",
    "    Reads in data from the CELEX corpus\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    PATH        {str} the path to the desired celex file, i.e. gpl.cd \n",
    "                    (contains orthography and phonology)\n",
    "    \n",
    "    Returns:\n",
    "    -----------\n",
    "    WORDS       {list} of words (length 51728) for gpl.cd\n",
    "    PHONS       {list} of phoneme sequences (length 51728) for gpl.cd\n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    with open(path, 'r') as file:\n",
    "\n",
    "        raw_data = file.read().splitlines()\n",
    "        words = []\n",
    "        phons = []\n",
    "        \n",
    "        for ind,raw_line in enumerate(raw_data):\n",
    "            \n",
    "            \n",
    "            line = raw_line.split(\"\\\\\")\n",
    "            words.append(line[1])\n",
    "            phons.append(line[-2]) # Using SAMPA notation\n",
    "                \n",
    "    return words, phons\n",
    "                               \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abbeizen ap#baits@\n",
      "(51728, 31) (51728, 36) 55 43\n"
     ]
    }
   ],
   "source": [
    "path = \"/Users/jannisborn/Desktop/LDS_Data/celex2/german/gpl/gpl.cd\"\n",
    "words, phons = extract_celex(path)\n",
    "\n",
    "print(words[30], phons[30])\n",
    "\n",
    "((w,p) , (word_dict, phon_dict)) = str_to_num_dataset(words,phons)\n",
    "\n",
    "print(w.shape, p.shape, len(word_dict), len(phon_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51728, 36)\n",
      "(51728, 31)\n",
      "{'U': 1, 'm': 2, 'd': 3, 'Y': 4, 'k': 5, ':': 6, 'N': 7, 'g': 8, 'i': 9, 'y': 10, 'E': 11, 'Z': 12, 'n': 13, 'S': 14, 'a': 15, '#': 16, '+': 17, '/': 18, 'o': 19, 'z': 20, 'j': 21, 'A': 22, 'f': 23, 'b': 24, '~': 25, '{': 26, 'e': 27, 's': 28, 'I': 29, 'v': 30, 'l': 31, 'O': 32, 't': 33, 'p': 34, '|': 35, 'u': 36, ' ': 37, '@': 38, 'r': 39, 'h': 40, 'x': 41, '<GO>': 41, '<PAD>': 42}\n",
      "{'G': 0, 'U': 1, 'm': 2, 'd': 3, 'Y': 4, 'k': 5, 'C': 6, 'N': 7, 'H': 8, 'B': 9, 'g': 10, 'x': 11, 'i': 12, 'y': 13, 'Q': 14, 'E': 15, 'M': 16, 'X': 17, 'Z': 18, 'n': 19, 'S': 20, 'a': 21, 'T': 22, 'D': 23, 'q': 24, 'K': 25, 'o': 26, 'R': 27, 'z': 28, 'V': 29, 'w': 30, 'j': 31, 'A': 32, 'W': 33, 'f': 34, 'b': 35, 'e': 36, 's': 37, 'F': 38, 'I': 39, 'v': 40, 'l': 41, 'L': 42, 'O': 43, 'J': 44, 't': 45, 'p': 46, 'u': 47, ' ': 48, 'r': 49, 'h': 50, 'P': 51, 'c': 52, '<PAD>': 53, '<GO>': 54}\n"
     ]
    }
   ],
   "source": [
    "np.savez('celex.npz', words=w, phons=p, word_dict=word_dict, phon_dict=phon_dict)\n",
    "data = np.load('data/celex.npz')\n",
    "print(data['phons'].shape)\n",
    "print(data['words'].shape)\n",
    "print(data['phon_dict'])\n",
    "print(data['word_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def BAS_json(path):\n",
    "    \"\"\"\n",
    "    This method receives a path for the BAS-SprecherInnen corpus and iterates through all JSON files in all subfolders.\n",
    "    It creates and returns a list of words and a list of pronounciations\n",
    "    \"\"\"\n",
    "    \n",
    "    import json, os\n",
    "\n",
    "    words = []\n",
    "    prons = []\n",
    "    ind = 0\n",
    "    # Read in filenames\n",
    "    for dirpath, dirnames, filenames in os.walk(path):\n",
    "        for filename in [f for f in filenames if f.endswith(\".json\")]:\n",
    "\n",
    "            if filename == 'SprecherInnen_DBconfig.json':\n",
    "                continue\n",
    "\n",
    "            # Open the json\n",
    "            with open(os.path.join(dirpath,filename)) as json_file:\n",
    "                data = json.load(json_file)\n",
    "\n",
    "                for item in data['levels'][1]['items']:\n",
    "                    words.append(item['labels'][0]['value'])\n",
    "                    prons.append(item['labels'][1]['value'])\n",
    "\n",
    "    return words,prons\n",
    "\n",
    "\n",
    "def clean_corpus_BAS_Sprecherinnen(words,prons):\n",
    "    \"\"\"\n",
    "    This method receives a list of words and a list of pronunciations of the BAS-Sprech. corpus and returns a cleaned dataset.\n",
    "    Clearning means:    1) Removing multiple occurrences of words       2) Remove misspellings and ambiguities\n",
    "                        3) Remove capitalization at begin of sentence   \n",
    "    Homophon words (Meer, mehr) are kept!\n",
    "\n",
    "    This method required manual inspection (once for each corpus).\n",
    "    Returns a condensed list of words and pronounciations (strings) that can be converted in numerical values next.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # First, we remove multiple occurrences.\n",
    "    # We cannot use set(words), set(prons) since some words are homophon 8(results in diff. lengths)\n",
    "    all_tups = []\n",
    "    for (w,p) in zip(words,prons):\n",
    "        all_tups.append(tuple((w,p)))\n",
    "    set_tup = set(all_tups)\n",
    "    print('Amount of non-unique words in corpus is ', len(all_tups))\n",
    "    unique_tups = dict(set_tup)\n",
    "    print('Amount of unique words in corpus is ', len(set_tup))\n",
    "\n",
    "    # Now we have removed multiple occurrences and we have a dict of tuples (word, pron)\n",
    "\n",
    "    def find_poss_mistakes(unique_tups):\n",
    "        \"\"\"\n",
    "        Receives a list of hopefully unique tupels (word,pron) and collect the tupels\n",
    "        which may have incorrect spelling/pronounciations.\n",
    "        \"\"\"\n",
    "        possible_mistakes = []\n",
    "        for key, val in unique_tups.items():\n",
    "            for keyy,vall in unique_tups.items():\n",
    "                if key != keyy and val == vall:\n",
    "                    # Detect multiple spellings of same pronounciation\n",
    "                    possible_mistakes.append((key,val, keyy, vall))\n",
    "                if key == keyy and val != vall:\n",
    "                    # Detect multiple pronounciations of same spelling\n",
    "                    possible_mistakes.append((key,val, keyy, vall))\n",
    "                    \n",
    "        return possible_mistakes\n",
    "        \n",
    "    poss_mist = find_poss_mistakes(unique_tups)\n",
    "    \"\"\"\n",
    "    print(\"+++ Possible mistakes are +++\")\n",
    "    for k in range(len(poss_mist)):\n",
    "        print(poss_mist[k][0],' -> ',poss_mist[k][1], \n",
    "              poss_mist[k][2],' -> ',poss_mist[k][2])\n",
    "    \"\"\"\n",
    "        \n",
    "    # Remove mistakes (after manual inspection)\n",
    "    unique_tups.pop('BäckerInnen') # removing as a duplicate of Bäckerinnen\n",
    "    unique_tups.pop('nu') # Duplicate of Nu\n",
    "    unique_tups.pop('Abonentinnen') # Misspelled\n",
    "    unique_tups.pop('Mit') # Duplicate of mit\n",
    "    unique_tups.pop('A') # Duplicate of ah\n",
    "    unique_tups.pop('Bei') # Duplicate of bei\n",
    "    unique_tups.pop('backwaren') # Duplicate of Backwaren\n",
    "    unique_tups.pop('-vertreterinnen') # Duplicate of Vertreterinnen\n",
    "    unique_tups.pop('leu') # Duplicate of Leu\n",
    "    unique_tups.pop('teil') # Duplicate of Teilt\n",
    "    unique_tups.pop('Un') # Duplicate of un\n",
    "    unique_tups.pop('Ver') # Duplicate of ver\n",
    "    unique_tups.pop('AutorInnen') # Duplicate of Autorinnen\n",
    "    unique_tups.pop('FreundInnen') # Duplicate of Freundinnen\n",
    "    unique_tups.pop('-pflegerin') # Duplicate of Pflegerin\n",
    "    unique_tups.pop('Neu') # Duplicate of neu\n",
    "    unique_tups.pop('re') # Duplicate of Re\n",
    "    unique_tups.pop('-kolleginnen') # Duplicate of Koleginnen\n",
    "    unique_tups.pop('-trinkerinnen') # Duplicate of Trinkerinnen\n",
    "    unique_tups.pop('Twitter-NutzerInnen') # Duplicate of Twitter-Nutzerinnen\n",
    "    unique_tups.pop('kommissionen') # Duplicate of Koleginnen\n",
    "    # Remaining: (Ihnen, ihnen), (dass,das), (Meer, mehr), (Ihres, ihres), (mal, Mal)\n",
    "\n",
    "    wordss = list(unique_tups.keys())\n",
    "    pronss = list(unique_tups.values())\n",
    "    print('After clearning ', len(wordss), ' different words remain')\n",
    "\n",
    "    return wordss, pronss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_num_dataset(X,Y):\n",
    "    \"\"\"\n",
    "    This method receives 2 lists of strings (input X and output Y) and converts it to padded, numerical arrays.\n",
    "    It returns the numerical dataset as well as the dictionaries to retrieve the strings.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Define dictionaries \n",
    "    # Dictionary assignining a unique integer to each input character\n",
    "    try:\n",
    "        u_characters = set(' '.join(X)) \n",
    "    except TypeError:\n",
    "        # Exception for TIMIT dataset (one phoneme is repr. by seq. of chars)\n",
    "        print(\"TypeError occurred.\")\n",
    "        u_characters = set([quant for seq in X for quant in seq])\n",
    "\n",
    "    char2numX = dict(zip(u_characters, range(len(u_characters))))\n",
    "\n",
    "    # Dictionary assignining a unique integer to each phoneme\n",
    "    try:\n",
    "        v_characters = set(' '.join(Y)) \n",
    "    except TypeError:\n",
    "        print(\"TypeError occurred.\")\n",
    "        v_characters = set([quant for seq in Y for quant in seq])\n",
    "    char2numY = dict(zip(v_characters, range(1,len(v_characters)+1))) # Using 0 causes trouble for tf.edit_distance\n",
    "    \n",
    "    # 2. Padding\n",
    "    # Pad inputs\n",
    "    char2numX['<PAD>'] = len(char2numX) \n",
    "    char2numX['<GO>']  = len(char2numX) \n",
    "    mx_l_X = max([len(word) for word in X]) # longest input sequence\n",
    "    # Padd all X for the final form for the LSTM\n",
    "    x = [[char2numX['<PAD>']]*(mx_l_X - len(word)) +[char2numX[char] for char in word] for word in X]\n",
    "    x = np.array(x) \n",
    "\n",
    "    # Pad targets\n",
    "    char2numY['<GO>'] = len(char2numY) # Define number denoting the response onset\n",
    "    char2numY['<PAD>'] = len(char2numY)  \n",
    "    mx_l_Y = max([len(phon_seq) for phon_seq in Y]) # longest output sequence\n",
    "\n",
    "    y = [[char2numY['<GO>']] + [char2numY['<PAD>']]*(mx_l_Y - len(ph_sq)) + [char2numY[phon] for phon in ph_sq] for ph_sq in Y]\n",
    "    y = np.array(y)\n",
    "\n",
    "    return ((x,y) , (char2numX,char2numY))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of non-unique words in corpus is  15698\n",
      "Amount of unique words in corpus is  854\n",
      "After clearning  833  different words remain\n",
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'dict'> <class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "path = '/Users/jannisborn/Desktop/LDS_Data/BAS_SprecherInnen'\n",
    "words, prons = BAS_json(path)\n",
    "words, prons = clean_corpus_BAS_Sprecherinnen(words,prons)\n",
    "\n",
    "((x,y) , (char2numX, char2numY)) = str_to_num_dataset(words,prons)\n",
    "np.savez('BAS_G2P.npz', inputs=x,targets=y, inp_dict=char2numX,\n",
    "        tar_dict=char2numY)\n",
    "\n",
    "((x,y) , (char2numX, char2numY)) = str_to_num_dataset(prons,words)\n",
    "np.savez('BAS_P2G.npz', inputs=x,targets=y, inp_dict=char2numX,\n",
    "        tar_dict=char2numY)\n",
    "\n",
    "print(type(x), type(y), type(char2numX),type(char2numY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "x = 'x'\n",
    "b = tf.equal(x,'x')\n",
    "print(type(b))\n",
    "y = tf.cond(tf.equal(x, 'x'), lambda: 1, lambda: 0)\n",
    "o = tf.cond(tf.equal(x, 'x'), lambda: print(2), lambda: print(43))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.ops import array_ops, gen_math_ops\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = targs\n",
    "batch_size = array_ops.shape(targets)[0]\n",
    "\n",
    "print(batch_size.eval(), targets.get_shape().as_list(), targets.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from time import time\n",
    "batch_size = 2\n",
    "seq_len = 3\n",
    "num_class = 4 # 0=A, 4=D -> Targets are> ABD and CAD\n",
    "nan = num_class + 1\n",
    "max_alt_spellings = 6\n",
    "tf.InteractiveSession()\n",
    "log = tf.reshape(np.array([[[4.73, 0.3, 2.1, 0.8], [1.0, 2.8, 0.1, 0.2], [0.1, 0.3, 0.1, 8.1]],\n",
    "                           [[0.14, 0.9, 0.1, 0.2], [0.3, 4.5, 0.9, 0.2], [8.1, 1.0, 1.1, 1.9]]]),\n",
    "                 [batch_size, seq_len, num_class])\n",
    "log = tf.cast(log, tf.float32)\n",
    "targs = tf.reshape(np.array([[0,1,3],[0,1,0]]),[batch_size,seq_len])\n",
    "alt_targets = tf.reshape(np.array([[[0, 1,nan,nan,nan,nan], [1, 1,nan,nan,nan,nan],\n",
    "                                        [3,3,nan,nan,nan,nan]],  \n",
    "                                   [[1, 0, 1, 0,1,0], [2,2, 3,3, 1,1], [0,0,0,0,0,0]]]), \n",
    "                                  [batch_size, seq_len, max_alt_spellings])\n",
    "alt_targets = tf.cast(alt_targets, tf.int64)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Targets \", targs.shape, targs.eval())\n",
    "print(\"Alternative targets\", alt_targets.shape, alt_targets.eval())\n",
    "\n",
    "tim = time()\n",
    "writings = tf.argmax(log,axis=-1)\n",
    "print(\"Writings shape is \", writings.get_shape().as_list())\n",
    "print(\"Writings are \",writings.eval())\n",
    "\n",
    "# For every output word of the batch, check whether it matches to any alternative writing\n",
    "# If so, replace the target\n",
    "fin_targets = []\n",
    "for wo_ind in range(alt_targets.shape[0]):\n",
    "    wrote_alternative = False\n",
    "    for tar_ind in range(alt_targets.shape[2]):\n",
    "        \n",
    "        if tf.reduce_all(tf.equal(writings[wo_ind,:],alt_targets[wo_ind,:,tar_ind])).eval():\n",
    "            fin_targets.append(alt_targets[wo_ind,:,tar_ind])\n",
    "            wrote_alternative = True\n",
    "            continue\n",
    "                     \n",
    "    if not wrote_alternative:\n",
    "        fin_targets.append(targs[wo_ind,:])\n",
    "        \n",
    "fin_targets = tf.stack(fin_targets)\n",
    "\n",
    "print()\n",
    "print()\n",
    "\n",
    "a = tf.contrib.seq2seq.sequence_loss(log, targs, tf.ones([batch_size, seq_len]))\n",
    "print(\"Loss according to regular learning\", a.eval())\n",
    "\n",
    "a = tf.contrib.seq2seq.sequence_loss(log, fin_targets, tf.ones([batch_size, seq_len]))\n",
    "print(\"Loss according to LdS learning with seq.loss\", a.eval())\n",
    "\n",
    "a = tf.contrib.seq2seq.sequence_loss_lds(log, targs, alt_targets, tf.ones([batch_size, seq_len]),\n",
    "                                        True)\n",
    "print(\"Loss according to LdS learning with NEW LOSS\", a.eval())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2g = np.load('/Users/jannisborn/Dropbox/GitHub/LSTM/Code/data/BAS_P2G.npz')\n",
    "bd = np.load('/Users/jannisborn/Dropbox/GitHub/LSTM/Code/data/BAS_G2P.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'9': 0, ':': 1, 'Z': 2, 'y': 3, ' ': 4, 'Y': 5, 'p': 6, '2': 7, '>': 8, 'r': 9, '@': 10, 'C': 11, 'd': 12, '6': 13, 'e': 14, 'l': 15, 'S': 16, 'z': 17, 'j': 18, 'k': 19, 'N': 20, 'U': 21, 'E': 22, 'u': 23, 'h': 24, 'o': 25, 'O': 26, 'a': 27, 'g': 28, 'm': 29, 't': 30, 'b': 31, '<': 32, 'I': 33, 'n': 34, 'x': 35, 'f': 36, 's': 37, 'i': 38, '?': 39, 'v': 40, '<PAD>': 41}\n",
      "42\n",
      "43 {'9': 0, ':': 1, 'Z': 2, 'y': 3, ' ': 4, 'Y': 5, 'p': 6, '2': 7, '>': 8, 'r': 9, '@': 10, 'C': 11, 'd': 12, '6': 13, 'e': 14, 'l': 15, 'S': 16, 'z': 17, 'j': 18, 'k': 19, 'N': 20, 'U': 21, 'E': 22, 'u': 23, 'h': 24, 'o': 25, 'O': 26, 'a': 27, 'g': 28, 'm': 29, 't': 30, 'b': 31, '<': 32, 'I': 33, 'n': 34, 'x': 35, 'f': 36, 's': 37, 'i': 38, '?': 39, 'v': 40, '<PAD>': 41, '<GO>': 42}\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-ca325883fdff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'inputs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'inputs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'targets'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'targets'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'inp_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'inp_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "print(p2g['inp_dict'])\n",
    "\n",
    "\n",
    "inp_dict = np_dict_to_dict(p2g['inp_dict'])\n",
    "print(len(inp_dict))\n",
    "inp_dict['<GO>'] = len(inp_dict)\n",
    "print(len(inp_dict), inp_dict)\n",
    "\n",
    "np.save\n",
    "\n",
    "print(np.array_equal(a['inputs'],ad['inputs']))\n",
    "print(np.array_equal(a['targets'],ad['targets']))\n",
    "print(np.array_equal(a['inp_dict'],ad['inp_dict']))\n",
    "print(np.array_equal(a['tar_dict'],ad['tar_dict']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "writings = tf.argmax(log,axis=-1)\n",
    "#print(\"Writings shape is \", writings.get_shape().as_list())\n",
    "#print(\"Writings are \",writings.eval())\n",
    "\n",
    "# For every output word of the batch, check whether it matches to any alternative writing\n",
    "# If so, replace the target\n",
    "fin_targets = []\n",
    "for wo_ind in range(alt_targets.shape[0]):\n",
    "    wrote_alternative = False\n",
    "    for tar_ind in range(alt_targets.shape[2]):\n",
    "        \n",
    "        if tf.reduce_all(tf.equal(writings[wo_ind,:],alt_targets[wo_ind,:,tar_ind])).eval():\n",
    "            fin_targets.append(alt_targets[wo_ind,:,tar_ind])\n",
    "            wrote_alternative = True\n",
    "            continue\n",
    "                     \n",
    "    if not wrote_alternative:\n",
    "        fin_targets.append(targs[wo_ind,:])\n",
    "        \n",
    "fin_targets = tf.stack(fin_targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insight 1: If you have k different classes, then k is the third \n",
    "#    dimension of the logits and then the target matrix must not \n",
    "#    contain values higher than k-1 (i.e. labels are 0, ..., k)\n",
    "\n",
    "# Insight 2: sequence_loss expects unnormalized logits (BEFORE softmax!)\n",
    "# -> This makes testing for me harder, since I cannot simply use values\n",
    "# with a sum over 1 (will get normalized)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
