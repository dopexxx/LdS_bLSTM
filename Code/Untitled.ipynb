{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys,time,os\n",
    "from bLSTM import bLSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understand how to initialize 2 Tensorflow models,\n",
    "# train them in one session, and save them independently\n",
    "\n",
    "\n",
    "class model(object):\n",
    "    \n",
    "    def __init__(self,size):\n",
    "        \n",
    "        self.inp = tf.placeholder(tf.float32,(size,2),'inp')\n",
    "        self.mat = tf.placeholder(tf.float32,(2,5),'mat')\n",
    "        \n",
    "        \n",
    "        \n",
    "    def foo(self):\n",
    "        self.out = tf.matmul(self.inp,self.mat)\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No variables to save",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-184-6e3d89be62ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_to_keep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, var_list, reshape, sharded, max_to_keep, keep_checkpoint_every_n_hours, name, restore_sequentially, saver_def, builder, defer_build, allow_empty, write_version, pad_step_number, save_relative_paths, filename)\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1292\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdefer_build\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_graph_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1293\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1294\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1295\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_saver_def\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1300\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_eager_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1301\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Use save/restore instead of build in eager mode.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_save\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_restore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_build_eager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_save\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_restore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36m_build\u001b[0;34m(self, checkpoint_path, build_save, build_restore)\u001b[0m\n\u001b[1;32m   1325\u001b[0m           \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No variables to save\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_empty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No variables to save"
     ]
    }
   ],
   "source": [
    "small = model(3)\n",
    "small.foo()\n",
    "large = model(10)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    saver = tf.train.Saver(max_to_keep=4)\n",
    "\n",
    "    \n",
    "    b = sess.run(small.out, feed_dict={small.inp:np.random.random((3,2)),\n",
    "                                      small.mat:np.random.random((2,5))})\n",
    "    print(b)\n",
    "    \n",
    "    saver.save(sess, save_path + '/Model', global_step=epoch, write_meta_graph=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_dict_to_dict(np_dict):\n",
    "    \"\"\"\n",
    "    Converts a dictionary saved via np.save (as structured np array) into an object of type dict\n",
    "\n",
    "    Parameters:\n",
    "    --------------\n",
    "    NP_DICT        : {np.array} structured np.array with dict keys and items\n",
    "\n",
    "    Returns:\n",
    "    --------------\n",
    "    DICT            : {dict} converted NP_DICT\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    return {key:np_dict.item().get(key) for key in np_dict.item()}\n",
    "\n",
    "\n",
    "\n",
    "def extract_celex(path):\n",
    "    \"\"\"\n",
    "    Reads in data from the CELEX corpus\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    PATH        {str} the path to the desired celex file, i.e. gpl.cd \n",
    "                    (contains orthography and phonology)\n",
    "\n",
    "    Returns:\n",
    "    -----------\n",
    "    2 Tuples, each with 2 variables. \n",
    "        First tuple:\n",
    "    W           {np.array} of words (length 51728) for gpl.cd\n",
    "    P           {np.array} of phoneme sequences (length 51728) for gpl.cd\n",
    "        Second tuple:\n",
    "    WORD_DICT   {dict} allowing to map the numerical array W back to strings\n",
    "    PHON_DICT   {dict} doing the same for the phonetical arrays P\n",
    "\n",
    "    \n",
    "    Call via:\n",
    "    path = \"/Users/jannisborn/Desktop/LDS_Data/celex2/german/gpl/gpl.cd\"\n",
    "    ((w,p) , (word_dict, phon_dict)) = extract_celex(path)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    with open(path, 'r') as file:\n",
    "\n",
    "        raw_data = file.read().splitlines()\n",
    "        words = []\n",
    "        phons = []\n",
    "        m = 0\n",
    "        for ind,raw_line in enumerate(raw_data):\n",
    "            \n",
    "            line = raw_line.split(\"\\\\\")\n",
    "\n",
    "            if line[-2]: # Use only words that HAVE a SAMPA transcript (reduces from 51k to 37345)\n",
    "\n",
    "            # exclude foreign words that have the 'æ' tone (SAMPA '{' ) like in PoINte   - 18 words\n",
    "            # exclude foreign words that have the 'ɑ' tone (SAMPA 'A' ) like in NuANce   - 28 words\n",
    "            # exclude foreign words that have a nasal vowel (SAMPA '~' ) like in Jargon  - 22 words\n",
    "                if not 'A' in line[-2] and not '{' in line[-2] and not '~' in line[-2]: \n",
    "\n",
    "                    if not ('tS' in line[-2] and not 'tsch' in line[1]): # exclude 9 foreign words like 'Image', 'Match', 'Punch', 'Sketch'\n",
    "\n",
    "                        if len(line[1]) < 15 and len(line[-2]) < 15 : # exclude extra long words (reduces to 34376)\n",
    "                            \n",
    "                            if len(line[-2]) > m:\n",
    "                                m = len(line[-2])\n",
    "                                print(line[1],line[-2])\n",
    "                                \n",
    "\n",
    "                            words.append(line[1].lower()) # All words are lowercase only\n",
    "                            phons.append(line[-2]) # Using SAMPA notation\n",
    "\n",
    "    return words,phons\n",
    "\n",
    "\n",
    "\n",
    "def str_to_num_dataset(X,Y):\n",
    "    \"\"\"\n",
    "    This method receives 2 lists of strings (input X and output Y) and converts it to padded, numerical arrays.\n",
    "    It returns the numerical dataset as well as the dictionaries to retrieve the strings.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Define dictionaries \n",
    "    # Dictionary assignining a unique integer to each input character\n",
    "    try:\n",
    "        u_characters = set(' '.join(X)) \n",
    "    except TypeError:\n",
    "        # Exception for TIMIT dataset (one phoneme is repr. by seq. of chars)\n",
    "        print(\"TypeError occurred.\")\n",
    "        u_characters = set([quant for seq in X for quant in seq])\n",
    "\n",
    "    char2numX = dict(zip(u_characters, range(1,len(u_characters)+1)))\n",
    "\n",
    "    # Dictionary assignining a unique integer to each phoneme\n",
    "    try:\n",
    "        v_characters = set(' '.join(Y)) \n",
    "    except TypeError:\n",
    "        print(\"TypeError occurred.\")\n",
    "        v_characters = set([quant for seq in Y for quant in seq])\n",
    "    char2numY = dict(zip(v_characters, range(1,len(v_characters)+1))) # Using 0 causes trouble for tf.edit_distance\n",
    "    \n",
    "    # 2. Padding\n",
    "    # Pad inputs\n",
    "    char2numX['<GO>'] = len(char2numX) + 1\n",
    "    char2numX['<PAD>'] = len(char2numX) + 1\n",
    "    mx_l_X = max([len(word) for word in X]) # longest input sequence\n",
    "    # Padd all X for the final form for the LSTM\n",
    "    x = [[char2numX['<GO>']] + [char2numX['<PAD>']]*(mx_l_X - len(word)) +[char2numX[char] for char in word] for word in X]\n",
    "    x = np.array(x) \n",
    "\n",
    "    # Pad targets\n",
    "    char2numY['<GO>'] = len(char2numY) + 1 # Define number denoting the response onset\n",
    "    char2numY['<PAD>'] = len(char2numY) + 1 \n",
    "    mx_l_Y = max([len(phon_seq) for phon_seq in Y]) # longest output sequence\n",
    "\n",
    "    y = [[char2numY['<GO>']] + [char2numY['<PAD>']]*(mx_l_Y - len(ph_sq)) + [char2numY[phon] for phon in ph_sq] for ph_sq in Y]\n",
    "    y = np.array(y)\n",
    "\n",
    "    return ((x,y) , (char2numX,char2numY))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('data/celex.npz')\n",
    "phon_dict = np_dict_to_dict(data['phon_dict'])\n",
    "word_dict = np_dict_to_dict(data['word_dict'])\n",
    "\n",
    "\n",
    "datao = np.load('data/celex_old.npz')\n",
    "phon_dicto = np_dict_to_dict(datao['phon_dict'])\n",
    "word_dicto = np_dict_to_dict(datao['word_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 1, '@': 2, 'n': 3, 'l': 4, 't': 5, 'U': 6, 'e': 7, '/': 8, 'j': 9, 'v': 10, 'Z': 11, 'x': 12, 'N': 13, 'h': 14, 'u': 15, 'i': 16, '|': 17, 'E': 18, 's': 19, '#': 20, 'r': 21, 'k': 22, 'S': 23, ':': 24, ' ': 25, 'g': 26, 'Y': 27, 'p': 28, 'm': 29, 'I': 30, 'z': 31, '+': 32, 'b': 33, 'y': 34, 'o': 35, 'd': 36, 'O': 37, 'f': 38, '<GO>': 39, '<PAD>': 40}\n",
      "{'S': 0, ':': 1, 'U': 2, 'Z': 3, '|': 4, '+': 5, 'a': 6, 'o': 7, '#': 8, 'Y': 9, '/': 10, 'r': 11, 'v': 12, 'N': 13, 'O': 14, 'j': 15, 'I': 16, 'e': 17, 'g': 18, 't': 19, 'k': 20, 'f': 21, 'p': 22, 'n': 23, 'E': 24, 'u': 25, 'x': 26, 'b': 27, 'd': 28, 'i': 29, ' ': 30, 'h': 31, 'm': 32, 'z': 33, 's': 34, '@': 35, 'y': 36, 'l': 37, '<GO>': 38, '<PAD>': 39}\n",
      "[[39 40 40 40 40 40 40 40 40 40 40 40 40  1 24 33  2  3 36 20  4 30 12]\n",
      " [39 40 40 40 40 40 40 40 40 40 40 40  1 24 33  2  3 36 20 29  1 24  4]\n",
      " [39 40 40 40 40 40 40 40 40 40 40 40  1 24 33  2  3 36 20 21 35 24  5]\n",
      " [39 40 40 40 40 40 40 40 40 40  1 24 33  2  3 36 20 21 35 24  5 32  2]\n",
      " [39 40 40 40 40 40 40 40 40 40 40 40 40  1 24 33  2  3  5 37 34  2 21]]\n",
      "[[39  6  1 27 35 23 28  8 37  6 23 28  5 16  0]\n",
      " [39 39 39 39 39  6  1 27 35 23 28  8 37 16 26]\n",
      " [39 39 39 39  6  1 27 35 23 28  8 32  6  1 37]\n",
      " [39 39 39 39  6  1 27 35 23 28  8 11  7  1 19]\n",
      " [39 39  6  1 27 35 23 28  8 11  7  1 19  5 35]]\n"
     ]
    }
   ],
   "source": [
    "print(phon_dict)\n",
    "print(phon_dicto)\n",
    "\n",
    "print(data['phons'][100:105])\n",
    "print(datao['phons'][100:105])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 1, 'n': 2, 'w': 3, 'l': 4, 't': 5, 'e': 6, 'j': 7, 'v': 8, 'x': 9, 'h': 10, 'u': 11, 'i': 12, 'q': 13, 's': 14, 'r': 15, 'k': 16, ' ': 17, 'c': 18, 'g': 19, 'p': 20, 'm': 21, 'z': 22, 'b': 23, 'y': 24, 'o': 25, 'd': 26, 'f': 27, '<GO>': 28, '<PAD>': 29}\n",
      "{'q': 1, 'a': 2, 'o': 3, 'r': 4, 'v': 5, 'j': 6, 'w': 7, 'e': 8, 'g': 9, 'k': 10, 't': 11, 'f': 12, 'p': 13, 'n': 14, 'c': 15, 'u': 16, 'x': 17, 'b': 18, 'd': 19, 'i': 20, ' ': 21, 'h': 22, 'm': 23, 'z': 24, 's': 25, 'y': 26, 'l': 27, '<GO>': 27, '<PAD>': 28}\n",
      "[[29 29 29 29 29  1 23  6  2 26  4 12 18 10]\n",
      " [29 29 29 29 29  1 23  6  2 26 21  1 10  4]\n",
      " [29 29 29 29 29 29  1 23  6  2 26 15 25  5]\n",
      " [29 29 29 29  1 23  6  2 26 15 25  6  5  6]\n",
      " [29 29 29 29  1 23  6  2  5  6 11  6 15  2]]\n",
      "[[27 28 28 28 28  2 18  8 14 19 27  2  8 14 19 20 25 15 22]\n",
      " [27 28 28 28 28 28 28 28 28 28  2 18  8 14 19 27 20 15 22]\n",
      " [27 28 28 28 28 28 28 28 28 28  2 18  8 14 19 23  2 22 27]\n",
      " [27 28 28 28 28 28 28 28 28 28 28  2 18  8 14 19  4  3 11]\n",
      " [27 28 28 28 28 28 28 28 28  2 18  8 14 19  4  3  8 11  8]]\n"
     ]
    }
   ],
   "source": [
    "print(word_dict)\n",
    "print(word_dicto)\n",
    "\n",
    "print(data['words'][100:105])\n",
    "print(datao['words'][100:105])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A a:\n",
      "aalen a:l\n",
      "aalglatt a:l#glat\n",
      "Aasgeier a:z#gai@r\n",
      "abaenderlich ap#End@r#lIx\n",
      "abbauwuerdig ap#bau#vYrdIx\n",
      "Abbrucharbeit ap#brEx#arbait\n"
     ]
    }
   ],
   "source": [
    "path = \"/Users/jannisborn/Desktop/LDS_Data/celex2/german/gpl/gpl.cd\"\n",
    "(w,p) = extract_celex(path)\n",
    "((words,phons) , (word_dict, phon_dict)) = str_to_num_dataset(w,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[28 29 29 29 29 29  1 23  6  2 26 21  1 10  4]\n",
      " [28 29 29 29 29 29 29  1 23  6  2 26 15 25  5]\n",
      " [28 29 29 29 29  1 23  6  2 26 15 25  6  5  6]\n",
      " [28 29 29 29 29  1 23  6  2  5  6 11  6 15  2]\n",
      " [28 29 29 29 29 29  1 23  6  2  5  6 11  6 15]]\n",
      "[[39 40 40 40  1 24 33  2  3 36 20 29  1 24  4]\n",
      " [39 40 40 40  1 24 33  2  3 36 20 21 35 24  5]\n",
      " [39 40  1 24 33  2  3 36 20 21 35 24  5 32  2]\n",
      " [39 40 40 40 40  1 24 33  2  3  5 37 34  2 21]\n",
      " [39 40 40 40 40  1 24 33  2  3  5 37 34  2 21]]\n",
      "[28 29 29 29 29 29  1 23  6  2 26 21  1 10]\n",
      "{'a': 1, 'n': 2, 'w': 3, 'l': 4, 't': 5, 'e': 6, 'j': 7, 'v': 8, 'x': 9, 'h': 10, 'u': 11, 'i': 12, 'q': 13, 's': 14, 'r': 15, 'k': 16, ' ': 17, 'c': 18, 'g': 19, 'p': 20, 'm': 21, 'z': 22, 'b': 23, 'y': 24, 'o': 25, 'd': 26, 'f': 27, '<GO>': 28, '<PAD>': 29} {'a': 1, '@': 2, 'n': 3, 'l': 4, 't': 5, 'U': 6, 'e': 7, '/': 8, 'j': 9, 'v': 10, 'Z': 11, 'x': 12, 'N': 13, 'h': 14, 'u': 15, 'i': 16, '|': 17, 'E': 18, 's': 19, '#': 20, 'r': 21, 'k': 22, 'S': 23, ':': 24, ' ': 25, 'g': 26, 'Y': 27, 'p': 28, 'm': 29, 'I': 30, 'z': 31, '+': 32, 'b': 33, 'y': 34, 'o': 35, 'd': 36, 'O': 37, 'f': 38, '<GO>': 39, '<PAD>': 40}\n",
      "29 40\n",
      "(31675, 15) (31675, 15)\n"
     ]
    }
   ],
   "source": [
    "print(words[100:105])\n",
    "print(phons[100:105])\n",
    "print(words[100,:-1])\n",
    "\n",
    "print(word_dict, phon_dict)\n",
    "print(len(word_dict),len(phon_dict))\n",
    "print(words.shape, phons.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('celex_new.npz', phons=phons, words=words, phon_dict=phon_dict, word_dict=word_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
