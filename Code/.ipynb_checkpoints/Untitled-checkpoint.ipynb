{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "def BAS_P2G_retrieve():\n",
    "    \"\"\"\n",
    "    Shortcut method for quickly retrieving numerical dataset of BAS-Sprecher corpus\n",
    "    In case whole dataset is not copied on remote machine\n",
    "    \"\"\"\n",
    "    data = np.load(os.getcwd()+'data/BAS_P2G.npz')\n",
    "    input_dict = np_dict_to_dict(data['inp_dict'])\n",
    "    target_dict = np_dict_to_dict(data['tar_dict'])\n",
    "\n",
    "    return ( (data['inputs'], data['targets']) , (input_dict, target_dict) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/jannisborn/anaconda3/envs/tf/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-210-c561368c16b7>\", line 1, in <module>\n",
      "    print(os.getcwd())\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jannisborn/anaconda3/envs/tf/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 1828, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'FileNotFoundError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jannisborn/anaconda3/envs/tf/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1090, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/Users/jannisborn/anaconda3/envs/tf/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 311, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/jannisborn/anaconda3/envs/tf/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 345, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/Users/jannisborn/anaconda3/envs/tf/lib/python3.6/inspect.py\", line 1483, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/Users/jannisborn/anaconda3/envs/tf/lib/python3.6/inspect.py\", line 1441, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/Users/jannisborn/anaconda3/envs/tf/lib/python3.6/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/Users/jannisborn/anaconda3/envs/tf/lib/python3.6/inspect.py\", line 725, in getmodule\n",
      "    file = getabsfile(object, _filename)\n",
      "  File \"/Users/jannisborn/anaconda3/envs/tf/lib/python3.6/inspect.py\", line 709, in getabsfile\n",
      "    return os.path.normcase(os.path.abspath(_filename))\n",
      "  File \"/Users/jannisborn/anaconda3/envs/tf/lib/python3.6/posixpath.py\", line 374, in abspath\n",
      "    cwd = os.getcwd()\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "#((inputs, targets) , (dict_char2num_x, dict_char2num_y)) = BAS_P2G_retrieve()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/jannisborn/anaconda3/envs/tf/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-211-efacdae2ab0b>\", line 1, in <module>\n",
      "    print(os.getcwd())\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jannisborn/anaconda3/envs/tf/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 1828, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'FileNotFoundError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jannisborn/anaconda3/envs/tf/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1090, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/Users/jannisborn/anaconda3/envs/tf/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 311, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/jannisborn/anaconda3/envs/tf/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 345, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/Users/jannisborn/anaconda3/envs/tf/lib/python3.6/inspect.py\", line 1483, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/Users/jannisborn/anaconda3/envs/tf/lib/python3.6/inspect.py\", line 1441, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/Users/jannisborn/anaconda3/envs/tf/lib/python3.6/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/Users/jannisborn/anaconda3/envs/tf/lib/python3.6/inspect.py\", line 725, in getmodule\n",
      "    file = getabsfile(object, _filename)\n",
      "  File \"/Users/jannisborn/anaconda3/envs/tf/lib/python3.6/inspect.py\", line 709, in getabsfile\n",
      "    return os.path.normcase(os.path.abspath(_filename))\n",
      "  File \"/Users/jannisborn/anaconda3/envs/tf/lib/python3.6/posixpath.py\", line 374, in abspath\n",
      "    cwd = os.getcwd()\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P2G\n",
    "import nltk\n",
    "timitdict = nltk.corpus.timit.transcription_dict()\n",
    "\n",
    "# 1. Split data into inputs and targets\n",
    "X = [timitdict[word] for word in timitdict] # inputs are phonemes\n",
    "Y = [word for word in timitdict] # outputs are words\n",
    "((a,b),(c,d)) = str_to_num_dataset(X,Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_num_dataset(X,Y):\n",
    "    \"\"\"\n",
    "    This method receives 2 lists of strings (input X and output Y) and converts it to padded, numerical arrays.\n",
    "    It returns the numerical dataset as well as the dictionaries to retrieve the strings.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Define dictionaries \n",
    "    # Dictionary assignining a unique integer to each input character\n",
    "    try:\n",
    "        u_characters = set(' '.join(X)) \n",
    "    except TypeError:\n",
    "        # Exception for TIMIT dataset (one phoneme is repr. by seq. of chars)\n",
    "        print(\"TypeError occurred.\")\n",
    "        u_characters = set([quant for seq in X for quant in seq])\n",
    "\n",
    "    char2numX = dict(zip(u_characters, range(len(u_characters))))\n",
    "\n",
    "    # Dictionary assignining a unique integer to each phoneme\n",
    "    try:\n",
    "        v_characters = set(' '.join(Y)) \n",
    "    except TypeError:\n",
    "        print(\"TypeError occurred.\")\n",
    "        v_characters = set([quant for seq in Y for quant in seq])\n",
    "    char2numY = dict(zip(v_characters, range(1,len(v_characters)+1))) # Using 0 causes trouble for tf.edit_distance\n",
    "    \n",
    "    # 2. Padding\n",
    "    # Pad inputs\n",
    "    char2numX['<PAD>'] = len(char2numX) \n",
    "    mx_l_X = max([len(word) for word in X]) # longest input sequence\n",
    "    # Padd all X for the final form for the LSTM\n",
    "    x = [[char2numX['<PAD>']]*(mx_l_X - len(word)) +[char2numX[char] for char in word] for word in X]\n",
    "    x = np.array(x) \n",
    "\n",
    "    # Pad targets\n",
    "    char2numY['<GO>'] = len(char2numY) # Define number denoting the response onset\n",
    "    char2numY['<PAD>'] = len(char2numY)  \n",
    "    mx_l_Y = max([len(phon_seq) for phon_seq in Y]) # longest output sequence\n",
    "\n",
    "    y = [[char2numY['<GO>']] + [char2numY['<PAD>']]*(mx_l_Y - len(ph_sq)) + [char2numY[phon] for phon in ph_sq] for ph_sq in Y]\n",
    "    y = np.array(y)\n",
    "\n",
    "    return ((x,y) , (char2numX,char2numY))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_corpus_BAS_Sprecherinnen(words,prons):\n",
    "    \"\"\"\n",
    "    This method receives a list of words and a list of pronunciations of the BAS-Sprech. corpus and returns a cleaned dataset.\n",
    "    Clearning means:    1) Removing multiple occurrences of words       2) Remove misspellings and ambiguities\n",
    "                        3) Remove capitalization at begin of sentence   \n",
    "    Homophon words (Meer, mehr) are kept!\n",
    "\n",
    "    This method required manual inspection (once for each corpus).\n",
    "    Returns a condensed list of words and pronounciations (strings) that can be converted in numerical values next.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # First, we remove multiple occurrences.\n",
    "    # We cannot use set(words), set(prons) since some words are homophon 8(results in diff. lengths)\n",
    "    all_tups = []\n",
    "    for (w,p) in zip(words,prons):\n",
    "        all_tups.append(tuple((w,p)))\n",
    "    set_tup = set(all_tups)\n",
    "    print('Amount of non-unique words in corpus is ', len(all_tups))\n",
    "    unique_tups = dict(set_tup)\n",
    "    print('Amount of unique words in corpus is ', len(set_tup))\n",
    "\n",
    "    # Now we have removed multiple occurrences and we have a dict of tuples (word, pron)\n",
    "\n",
    "    def find_poss_mistakes(unique_tups):\n",
    "        \"\"\"\n",
    "        Receives a list of hopefully unique tupels (word,pron) and collect the tupels\n",
    "        which may have incorrect spelling/pronounciations.\n",
    "        \"\"\"\n",
    "        possible_mistakes = []\n",
    "        for key, val in unique_tups.items():\n",
    "            for keyy,vall in unique_tups.items():\n",
    "                if key != keyy and val == vall:\n",
    "                    # Detect multiple spellings of same pronounciation\n",
    "                    possible_mistakes.append((key,val, keyy, vall))\n",
    "                if key == keyy and val != vall:\n",
    "                    # Detect multiple pronounciations of same spelling\n",
    "                    possible_mistakes.append((key,val, keyy, vall))\n",
    "                    \n",
    "        return possible_mistakes\n",
    "        \n",
    "    poss_mist = find_poss_mistakes(unique_tups)\n",
    "    \"\"\"\n",
    "    print(\"+++ Possible mistakes are +++\")\n",
    "    for k in range(len(poss_mist)):\n",
    "        print(poss_mist[k][0],' -> ',poss_mist[k][1], \n",
    "              poss_mist[k][2],' -> ',poss_mist[k][2])\n",
    "    \"\"\"\n",
    "        \n",
    "    # Remove mistakes (after manual inspection)\n",
    "    unique_tups.pop('BäckerInnen') # removing as a duplicate of Bäckerinnen\n",
    "    unique_tups.pop('nu') # Duplicate of Nu\n",
    "    unique_tups.pop('Abonentinnen') # Misspelled\n",
    "    unique_tups.pop('Mit') # Duplicate of mit\n",
    "    unique_tups.pop('A') # Duplicate of ah\n",
    "    unique_tups.pop('Bei') # Duplicate of bei\n",
    "    unique_tups.pop('backwaren') # Duplicate of Backwaren\n",
    "    unique_tups.pop('-vertreterinnen') # Duplicate of Vertreterinnen\n",
    "    unique_tups.pop('leu') # Duplicate of Leu\n",
    "    unique_tups.pop('teil') # Duplicate of Teilt\n",
    "    unique_tups.pop('Un') # Duplicate of un\n",
    "    unique_tups.pop('Ver') # Duplicate of ver\n",
    "    unique_tups.pop('AutorInnen') # Duplicate of Autorinnen\n",
    "    unique_tups.pop('FreundInnen') # Duplicate of Freundinnen\n",
    "    unique_tups.pop('-pflegerin') # Duplicate of Pflegerin\n",
    "    unique_tups.pop('Neu') # Duplicate of neu\n",
    "    unique_tups.pop('re') # Duplicate of Re\n",
    "    unique_tups.pop('-kolleginnen') # Duplicate of Koleginnen\n",
    "    unique_tups.pop('-trinkerinnen') # Duplicate of Trinkerinnen\n",
    "    unique_tups.pop('Twitter-NutzerInnen') # Duplicate of Twitter-Nutzerinnen\n",
    "    unique_tups.pop('kommissionen') # Duplicate of Koleginnen\n",
    "    # Remaining: (Ihnen, ihnen), (dass,das), (Meer, mehr), (Ihres, ihres), (mal, Mal)\n",
    "\n",
    "    wordss = list(unique_tups.keys())\n",
    "    pronss = list(unique_tups.values())\n",
    "    print('After clearning ', len(wordss), ' different words remain')\n",
    "\n",
    "    return wordss, pronss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/jannisborn/Desktop/LDS_Data/BAS_SprecherInnen'\n",
    "words, prons = BAS_json(path)\n",
    "words, prons = clean_corpus_BAS_Sprecherinnen(words,prons)\n",
    "\n",
    "((x,y) , (char2numX, char2numY)) = str_to_num_dataset(words,prons)\n",
    "np.savez('BAS_G2P.npz', inputs=x,targets=y, inp_dict=char2numX,\n",
    "        tar_dict=char2numY)\n",
    "\n",
    "((x,y) , (char2numX, char2numY)) = str_to_num_dataset(prons,words)\n",
    "np.savez('BAS_P2G.npz', inputs=x,targets=y, inp_dict=char2numX,\n",
    "        tar_dict=char2numY)\n",
    "\n",
    "print(type(x), type(y), type(char2numX),type(char2numY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "for dirpath, dirnames, filenames in os.walk(path):\n",
    "        #print(dirpath, dirnames, filenames)\n",
    "        for filename in [f for f in filenames if f.endswith(\".json\")]:\n",
    "            print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "x = 'x'\n",
    "b = tf.equal(x,'x')\n",
    "print(type(b))\n",
    "y = tf.cond(tf.equal(x, 'x'), lambda: 1, lambda: 0)\n",
    "o = tf.cond(tf.equal(x, 'x'), lambda: print(2), lambda: print(43))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.ops import array_ops, gen_math_ops\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = targs\n",
    "batch_size = array_ops.shape(targets)[0]\n",
    "\n",
    "print(batch_size.eval(), targets.get_shape().as_list(), targets.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from time import time\n",
    "batch_size = 2\n",
    "seq_len = 3\n",
    "num_class = 4 # 0=A, 4=D -> Targets are> ABD and CAD\n",
    "nan = num_class + 1\n",
    "max_alt_spellings = 6\n",
    "tf.InteractiveSession()\n",
    "log = tf.reshape(np.array([[[4.73, 0.3, 2.1, 0.8], [1.0, 2.8, 0.1, 0.2], [0.1, 0.3, 0.1, 8.1]],\n",
    "                           [[0.14, 0.9, 0.1, 0.2], [0.3, 4.5, 0.9, 0.2], [8.1, 1.0, 1.1, 1.9]]]),\n",
    "                 [batch_size, seq_len, num_class])\n",
    "log = tf.cast(log, tf.float32)\n",
    "targs = tf.reshape(np.array([[0,1,3],[0,1,0]]),[batch_size,seq_len])\n",
    "alt_targets = tf.reshape(np.array([[[0, 1,nan,nan,nan,nan], [1, 1,nan,nan,nan,nan],\n",
    "                                        [3,3,nan,nan,nan,nan]],  \n",
    "                                   [[1, 0, 1, 0,1,0], [2,2, 3,3, 1,1], [0,0,0,0,0,0]]]), \n",
    "                                  [batch_size, seq_len, max_alt_spellings])\n",
    "alt_targets = tf.cast(alt_targets, tf.int64)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Targets \", targs.shape, targs.eval())\n",
    "print(\"Alternative targets\", alt_targets.shape, alt_targets.eval())\n",
    "\n",
    "tim = time()\n",
    "writings = tf.argmax(log,axis=-1)\n",
    "print(\"Writings shape is \", writings.get_shape().as_list())\n",
    "print(\"Writings are \",writings.eval())\n",
    "\n",
    "# For every output word of the batch, check whether it matches to any alternative writing\n",
    "# If so, replace the target\n",
    "fin_targets = []\n",
    "for wo_ind in range(alt_targets.shape[0]):\n",
    "    wrote_alternative = False\n",
    "    for tar_ind in range(alt_targets.shape[2]):\n",
    "        \n",
    "        if tf.reduce_all(tf.equal(writings[wo_ind,:],alt_targets[wo_ind,:,tar_ind])).eval():\n",
    "            fin_targets.append(alt_targets[wo_ind,:,tar_ind])\n",
    "            wrote_alternative = True\n",
    "            continue\n",
    "                     \n",
    "    if not wrote_alternative:\n",
    "        fin_targets.append(targs[wo_ind,:])\n",
    "        \n",
    "fin_targets = tf.stack(fin_targets)\n",
    "\n",
    "print()\n",
    "print()\n",
    "\n",
    "a = tf.contrib.seq2seq.sequence_loss(log, targs, tf.ones([batch_size, seq_len]))\n",
    "print(\"Loss according to regular learning\", a.eval())\n",
    "\n",
    "a = tf.contrib.seq2seq.sequence_loss(log, fin_targets, tf.ones([batch_size, seq_len]))\n",
    "print(\"Loss according to LdS learning with seq.loss\", a.eval())\n",
    "\n",
    "a = tf.contrib.seq2seq.sequence_loss_lds(log, targs, alt_targets, tf.ones([batch_size, seq_len]),\n",
    "                                        True)\n",
    "print(\"Loss according to LdS learning with NEW LOSS\", a.eval())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.load('/Users/jannisborn/Dropbox/GitHub/LSTM/Code/BAS_P2G.npz')\n",
    "b = np.load('/Users/jannisborn/Dropbox/GitHub/LSTM/Code/BAS_G2P.npz')\n",
    "\n",
    "ad = np.load('/Users/jannisborn/Dropbox/GitHub/LSTM/Code/data/BAS_P2G.npz')\n",
    "bd = np.load('/Users/jannisborn/Dropbox/GitHub/LSTM/Code/data/BAS_G2P.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.array_equal(a['inputs'],ad['inputs']))\n",
    "print(np.array_equal(a['targets'],ad['targets']))\n",
    "print(np.array_equal(a['inp_dict'],ad['inp_dict']))\n",
    "print(np.array_equal(a['tar_dict'],ad['tar_dict']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "writings = tf.argmax(log,axis=-1)\n",
    "#print(\"Writings shape is \", writings.get_shape().as_list())\n",
    "#print(\"Writings are \",writings.eval())\n",
    "\n",
    "# For every output word of the batch, check whether it matches to any alternative writing\n",
    "# If so, replace the target\n",
    "fin_targets = []\n",
    "for wo_ind in range(alt_targets.shape[0]):\n",
    "    wrote_alternative = False\n",
    "    for tar_ind in range(alt_targets.shape[2]):\n",
    "        \n",
    "        if tf.reduce_all(tf.equal(writings[wo_ind,:],alt_targets[wo_ind,:,tar_ind])).eval():\n",
    "            fin_targets.append(alt_targets[wo_ind,:,tar_ind])\n",
    "            wrote_alternative = True\n",
    "            continue\n",
    "                     \n",
    "    if not wrote_alternative:\n",
    "        fin_targets.append(targs[wo_ind,:])\n",
    "        \n",
    "fin_targets = tf.stack(fin_targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insight 1: If you have k different classes, then k is the third \n",
    "#    dimension of the logits and then the target matrix must not \n",
    "#    contain values higher than k-1 (i.e. labels are 0, ..., k)\n",
    "\n",
    "# Insight 2: sequence_loss expects unnormalized logits (BEFORE softmax!)\n",
    "# -> This makes testing for me harder, since I cannot simply use values\n",
    "# with a sum over 1 (will get normalized)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
