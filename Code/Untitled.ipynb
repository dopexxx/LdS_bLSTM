{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys,time,os\n",
    "from bLSTM import bLSTM\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fibel_retrieve(learn_type):\n",
    "\n",
    "    data = np.load('data/fibel.npz')\n",
    "    phon_dict = np_dict_to_dict(data['phon_dict'])\n",
    "    word_dict = np_dict_to_dict(data['word_dict'])\n",
    "\n",
    "    if learn_type == 'lds':\n",
    "\n",
    "        path = 'data/fibel_alt_targets.npy'\n",
    "        print(\"Loading alternative targets ...\")\n",
    "        alt_targs_raw = np.load(path)\n",
    "\n",
    "\n",
    "        alt_targs = np.array([np.array(d,dtype=np.int8) for d in alt_targs_raw])\n",
    "        print(\"Alternative targets successfully loaded.\")\n",
    "\n",
    "        return ( (data['phons'], data['words']) , (phon_dict, word_dict), alt_targs )\n",
    "    else:\n",
    "        return ( (data['phons'], data['words']) , (phon_dict, word_dict))\n",
    "\n",
    "def np_dict_to_dict(np_dict):\n",
    "    \"\"\"\n",
    "    Converts a dictionary saved via np.save (as structured np array) into an object of type dict\n",
    "\n",
    "    Parameters:\n",
    "    --------------\n",
    "    NP_DICT        : {np.array} structured np.array with dict keys and items\n",
    "\n",
    "    Returns:\n",
    "    --------------\n",
    "    DICT            : {dict} converted NP_DICT\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    return {key:np_dict.item().get(key) for key in np_dict.item()}\n",
    "\n",
    "def num_to_str(inputs,logits,labels,alt_targs,dict_in,dict_out,mode='normal'):\n",
    "    \"\"\"\n",
    "    Method receives the numerical arrays and prints the strings\n",
    "\n",
    "    If mode = normal, then alt_targs should be set to [], if it is 'lds' the alternative targets are also printed\n",
    "    \"\"\"\n",
    "\n",
    "    fullPred = logits.argmax(-1) # Prediction string with padding\n",
    "\n",
    "    \n",
    "    #Padded target string\n",
    "    fullTarg = np.copy(labels) \n",
    "    # Set pads to 0 - as preparation for edit_distance\n",
    "\n",
    "    out_str = []\n",
    "    inp_str = []\n",
    "    label_str = []\n",
    "    #print(inputs.shape,fullPred.shape,labels.shape,alt_targs.shape)\n",
    "    #print(dict_in)\n",
    "    #print(dict_out)\n",
    "    #print(inputs[0:5],fullPred[:5],labels[:5])\n",
    "    for k in range(len(fullPred)):\n",
    "        out_str.append(''.join([dict_out[l] if dict_out[l] != '<PAD>' and  dict_out[l] != '<GO>' else '' for l in fullPred[k]]))\n",
    "        inp_str.append(''.join([dict_in[l] if dict_in[l] != '<PAD>' and  dict_in[l] != '<GO>' else '' for l in inputs[k]]))\n",
    "        label_str.append(''.join([dict_out[l] if dict_out[l] != '<PAD>' and  dict_out[l] != '<GO>' else '' for l in labels[k]]))\n",
    "\n",
    "        print(\"The input \" + inp_str[-1].upper() + \" was written as \" + out_str[-1].upper() + \" with the target as \" + label_str[-1].upper())\n",
    "\n",
    "        if mode == 'lds':\n",
    "            alt_targ_str = []\n",
    "\n",
    "            z = np.argwhere(alt_targs[k]==0) # indices of zeros (alt_targs where padded to have equally sized array)\n",
    "            # position 0,1 is the first row that contains zeros (i.e. not an alternative writing anymore)\n",
    "            for l in range(z[0,1]):\n",
    "                alt_targ_str.append(''.join([dict_out[m] if dict_out[m] != '<PAD>' and  dict_out[m] != '<GO>' else '' for m in alt_targs[k,:,l] ]))\n",
    "            print(\"The alternatives were \", alt_targ_str)\n",
    "\n",
    "            \n",
    "def set_model_params(inputs, targets, dict_char2num_x, dict_char2num_y):\n",
    "    \"\"\"\n",
    "    This method can receive data from any dataset (inputs, targets) and the corresponding dictionaries.\n",
    "    It returns the hyperparameters for the model, i.e. input and output sequence length as well as input and output dictionary size.\n",
    "    \"\"\"\n",
    "\n",
    "    # Error handling. If the dicts are not objects of type dict but np.arrays (dicts saved via np.save), convert them back.\n",
    "    if isinstance(dict_char2num_x, np.ndarray):\n",
    "        dict_char2num_x = np_dict_to_dict(dict_char2num_x)\n",
    "    if isinstance(dict_char2num_y, np.ndarray):\n",
    "        dict_char2num_y = np_dict_to_dict(dict_char2num_y)\n",
    "\n",
    "\n",
    "    dict_num2char_x = dict(zip(dict_char2num_x.values(), dict_char2num_x.keys()))\n",
    "    dict_num2char_y = dict(zip(dict_char2num_y.values(), dict_char2num_y.keys()))\n",
    "    x_dict_size = len(dict_char2num_x)\n",
    "    num_classes = len(dict_char2num_y) # (y_dict_size) Cardinality of output dictionary\n",
    "    x_seq_length = len(inputs[0]) - 1 \n",
    "    y_seq_length = len(targets[0]) - 1 # Because of the <GO> as response onset\n",
    "\n",
    "    return x_dict_size, num_classes, x_seq_length, y_seq_length, dict_num2char_x, dict_num2char_y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "positional argument follows keyword argument (<ipython-input-45-1b398f4babb2>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-45-1b398f4babb2>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    X_train, X_test,Y_train, Y_test = train_test_split(inputs, targets, test_size=0.1, 42)\u001b[0m\n\u001b[0m                                                                                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m positional argument follows keyword argument\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test,Y_train, Y_test = train_test_split(inputs, targets, test_size=0.1, 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading alternative targets ...\n",
      "Alternative targets successfully loaded.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-1503de3a10d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mwrite_ind\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_alt_train_l\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_ind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mY_alt_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_ind\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwrite_ind\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_alt_train_l\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_ind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwrite_ind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_to_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlogp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_alt_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdict_num2char_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdict_num2char_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'lds'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-50-13bce2477f04>\u001b[0m in \u001b[0;36mnum_to_str\u001b[0;34m(inputs, logits, labels, alt_targs, dict_in, dict_out, mode)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;31m#print(inputs[0:5],fullPred[:5],labels[:5])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfullPred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mout_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdict_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'<PAD>'\u001b[0m \u001b[0;32mand\u001b[0m  \u001b[0mdict_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'<GO>'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m''\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfullPred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0minp_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict_in\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdict_in\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'<PAD>'\u001b[0m \u001b[0;32mand\u001b[0m  \u001b[0mdict_in\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'<GO>'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m''\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mlabel_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdict_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'<PAD>'\u001b[0m \u001b[0;32mand\u001b[0m  \u001b[0mdict_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'<GO>'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m''\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-50-13bce2477f04>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;31m#print(inputs[0:5],fullPred[:5],labels[:5])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfullPred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mout_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdict_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'<PAD>'\u001b[0m \u001b[0;32mand\u001b[0m  \u001b[0mdict_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'<GO>'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m''\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfullPred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0minp_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict_in\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdict_in\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'<PAD>'\u001b[0m \u001b[0;32mand\u001b[0m  \u001b[0mdict_in\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'<GO>'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m''\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mlabel_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdict_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'<PAD>'\u001b[0m \u001b[0;32mand\u001b[0m  \u001b[0mdict_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'<GO>'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m''\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "((inputs, targets) , (dict_char2num_x, dict_char2num_y), alt_targets) = fibel_retrieve('lds')\n",
    "x_dict_size, num_classes, x_seq_length, y_seq_length, dict_num2char_x, dict_num2char_y = set_model_params(inputs, targets, dict_char2num_x, dict_char2num_y)\n",
    "\n",
    "indices = range(len(inputs))\n",
    "X_train, X_test,Y_train, Y_test, Y_alt_train_l, Y_alt_test_l, ind_train, ind_test = train_test_split(inputs, targets, alt_targets,indices,test_size=0.1, random_state=42)\n",
    "\n",
    "\n",
    "logp = np.reshape(np.arange(174*22*30),[17])\n",
    "max_len = max([len(l) for l in Y_alt_train_l])\n",
    "inp_seq_len = len(Y_alt_train_l[1][0])\n",
    "Y_alt_train = np.zeros([len(Y_alt_train_l), inp_seq_len, max_len], dtype=np.int8)\n",
    "for word_ind in range(len(Y_alt_train_l)):\n",
    "    for write_ind in range(len(Y_alt_train_l[word_ind])):\n",
    "        Y_alt_train[word_ind,:,write_ind] = np.array(Y_alt_train_l[word_ind][write_ind],dtype=np.int8)\n",
    "a=num_to_str(X_train,logp,Y_train,Y_alt_train,dict_num2char_x,dict_num2char_y,'lds')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understand how to initialize 2 Tensorflow models,\n",
    "# train them in one session, and save them independently\n",
    "\n",
    "\n",
    "class model(object):\n",
    "    \n",
    "    def __init__(self,size):\n",
    "        \n",
    "        self.inp = tf.placeholder(tf.float32,(size,2),'inp')\n",
    "        self.mat = tf.placeholder(tf.float32,(2,5),'mat')\n",
    "        \n",
    "        \n",
    "        \n",
    "    def foo(self):\n",
    "        self.out = tf.matmul(self.inp,self.mat)\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No variables to save",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-184-6e3d89be62ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_to_keep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, var_list, reshape, sharded, max_to_keep, keep_checkpoint_every_n_hours, name, restore_sequentially, saver_def, builder, defer_build, allow_empty, write_version, pad_step_number, save_relative_paths, filename)\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1292\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdefer_build\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_graph_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1293\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1294\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1295\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_saver_def\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1300\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_eager_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1301\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Use save/restore instead of build in eager mode.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_save\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_restore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_build_eager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_save\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_restore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36m_build\u001b[0;34m(self, checkpoint_path, build_save, build_restore)\u001b[0m\n\u001b[1;32m   1325\u001b[0m           \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No variables to save\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_empty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No variables to save"
     ]
    }
   ],
   "source": [
    "small = model(3)\n",
    "small.foo()\n",
    "large = model(10)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    saver = tf.train.Saver(max_to_keep=4)\n",
    "\n",
    "    \n",
    "    b = sess.run(small.out, feed_dict={small.inp:np.random.random((3,2)),\n",
    "                                      small.mat:np.random.random((2,5))})\n",
    "    print(b)\n",
    "    \n",
    "    saver.save(sess, save_path + '/Model', global_step=epoch, write_meta_graph=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_dict_to_dict(np_dict):\n",
    "    \"\"\"\n",
    "    Converts a dictionary saved via np.save (as structured np array) into an object of type dict\n",
    "\n",
    "    Parameters:\n",
    "    --------------\n",
    "    NP_DICT        : {np.array} structured np.array with dict keys and items\n",
    "\n",
    "    Returns:\n",
    "    --------------\n",
    "    DICT            : {dict} converted NP_DICT\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    return {key:np_dict.item().get(key) for key in np_dict.item()}\n",
    "\n",
    "\n",
    "\n",
    "def extract_celex(path):\n",
    "    \"\"\"\n",
    "    Reads in data from the CELEX corpus\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    PATH        {str} the path to the desired celex file, i.e. gpl.cd \n",
    "                    (contains orthography and phonology)\n",
    "\n",
    "    Returns:\n",
    "    -----------\n",
    "    2 Tuples, each with 2 variables. \n",
    "        First tuple:\n",
    "    W           {np.array} of words (length 51728) for gpl.cd\n",
    "    P           {np.array} of phoneme sequences (length 51728) for gpl.cd\n",
    "        Second tuple:\n",
    "    WORD_DICT   {dict} allowing to map the numerical array W back to strings\n",
    "    PHON_DICT   {dict} doing the same for the phonetical arrays P\n",
    "\n",
    "    \n",
    "    Call via:\n",
    "    path = \"/Users/jannisborn/Desktop/LDS_Data/celex2/german/gpl/gpl.cd\"\n",
    "    ((w,p) , (word_dict, phon_dict)) = extract_celex(path)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    with open(path, 'r') as file:\n",
    "\n",
    "        raw_data = file.read().splitlines()\n",
    "        words = []\n",
    "        phons = []\n",
    "        m = 0\n",
    "        for ind,raw_line in enumerate(raw_data):\n",
    "            \n",
    "            line = raw_line.split(\"\\\\\")\n",
    "\n",
    "            if line[-2]: # Use only words that HAVE a SAMPA transcript (reduces from 51k to 37345)\n",
    "\n",
    "            # exclude foreign words that have the 'æ' tone (SAMPA '{' ) like in PoINte   - 18 words\n",
    "            # exclude foreign words that have the 'ɑ' tone (SAMPA 'A' ) like in NuANce   - 28 words\n",
    "            # exclude foreign words that have a nasal vowel (SAMPA '~' ) like in Jargon  - 22 words\n",
    "                if not 'A' in line[-2] and not '{' in line[-2] and not '~' in line[-2]: \n",
    "\n",
    "                    if not ('tS' in line[-2] and not 'tsch' in line[1]): # exclude 9 foreign words like 'Image', 'Match', 'Punch', 'Sketch'\n",
    "\n",
    "                        if len(line[1]) < 15 and len(line[-2]) < 15 : # exclude extra long words (reduces to 34376)\n",
    "                            \n",
    "                            if len(line[-2]) > m:\n",
    "                                m = len(line[-2])\n",
    "                                print(line[1],line[-2])\n",
    "                                \n",
    "\n",
    "                            words.append(line[1].lower()) # All words are lowercase only\n",
    "                            phons.append(line[-2]) # Using SAMPA notation\n",
    "\n",
    "    return words,phons\n",
    "\n",
    "\n",
    "\n",
    "def str_to_num_dataset(X,Y):\n",
    "    \"\"\"\n",
    "    This method receives 2 lists of strings (input X and output Y) and converts it to padded, numerical arrays.\n",
    "    It returns the numerical dataset as well as the dictionaries to retrieve the strings.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Define dictionaries \n",
    "    # Dictionary assignining a unique integer to each input character\n",
    "    try:\n",
    "        u_characters = set(' '.join(X)) \n",
    "    except TypeError:\n",
    "        # Exception for TIMIT dataset (one phoneme is repr. by seq. of chars)\n",
    "        print(\"TypeError occurred.\")\n",
    "        u_characters = set([quant for seq in X for quant in seq])\n",
    "\n",
    "    char2numX = dict(zip(u_characters, range(1,len(u_characters)+1)))\n",
    "\n",
    "    # Dictionary assignining a unique integer to each phoneme\n",
    "    try:\n",
    "        v_characters = set(' '.join(Y)) \n",
    "    except TypeError:\n",
    "        print(\"TypeError occurred.\")\n",
    "        v_characters = set([quant for seq in Y for quant in seq])\n",
    "    char2numY = dict(zip(v_characters, range(1,len(v_characters)+1))) # Using 0 causes trouble for tf.edit_distance\n",
    "    \n",
    "    # 2. Padding\n",
    "    # Pad inputs\n",
    "    char2numX['<GO>'] = len(char2numX) + 1\n",
    "    char2numX['<PAD>'] = len(char2numX) + 1\n",
    "    mx_l_X = max([len(word) for word in X]) # longest input sequence\n",
    "    # Padd all X for the final form for the LSTM\n",
    "    x = [[char2numX['<GO>']] + [char2numX['<PAD>']]*(mx_l_X - len(word)) +[char2numX[char] for char in word] for word in X]\n",
    "    x = np.array(x) \n",
    "\n",
    "    # Pad targets\n",
    "    char2numY['<GO>'] = len(char2numY) + 1 # Define number denoting the response onset\n",
    "    char2numY['<PAD>'] = len(char2numY) + 1 \n",
    "    mx_l_Y = max([len(phon_seq) for phon_seq in Y]) # longest output sequence\n",
    "\n",
    "    y = [[char2numY['<GO>']] + [char2numY['<PAD>']]*(mx_l_Y - len(ph_sq)) + [char2numY[phon] for phon in ph_sq] for ph_sq in Y]\n",
    "    y = np.array(y)\n",
    "\n",
    "    return ((x,y) , (char2numX,char2numY))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('data/celex.npz')\n",
    "phon_dict = np_dict_to_dict(data['phon_dict'])\n",
    "word_dict = np_dict_to_dict(data['word_dict'])\n",
    "\n",
    "\n",
    "datao = np.load('data/celex_old.npz')\n",
    "phon_dicto = np_dict_to_dict(datao['phon_dict'])\n",
    "word_dicto = np_dict_to_dict(datao['word_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 1, '@': 2, 'n': 3, 'l': 4, 't': 5, 'U': 6, 'e': 7, '/': 8, 'j': 9, 'v': 10, 'Z': 11, 'x': 12, 'N': 13, 'h': 14, 'u': 15, 'i': 16, '|': 17, 'E': 18, 's': 19, '#': 20, 'r': 21, 'k': 22, 'S': 23, ':': 24, ' ': 25, 'g': 26, 'Y': 27, 'p': 28, 'm': 29, 'I': 30, 'z': 31, '+': 32, 'b': 33, 'y': 34, 'o': 35, 'd': 36, 'O': 37, 'f': 38, '<GO>': 39, '<PAD>': 40}\n",
      "{'S': 0, ':': 1, 'U': 2, 'Z': 3, '|': 4, '+': 5, 'a': 6, 'o': 7, '#': 8, 'Y': 9, '/': 10, 'r': 11, 'v': 12, 'N': 13, 'O': 14, 'j': 15, 'I': 16, 'e': 17, 'g': 18, 't': 19, 'k': 20, 'f': 21, 'p': 22, 'n': 23, 'E': 24, 'u': 25, 'x': 26, 'b': 27, 'd': 28, 'i': 29, ' ': 30, 'h': 31, 'm': 32, 'z': 33, 's': 34, '@': 35, 'y': 36, 'l': 37, '<GO>': 38, '<PAD>': 39}\n",
      "[[39 40 40 40 40 40 40 40 40 40 40 40 40  1 24 33  2  3 36 20  4 30 12]\n",
      " [39 40 40 40 40 40 40 40 40 40 40 40  1 24 33  2  3 36 20 29  1 24  4]\n",
      " [39 40 40 40 40 40 40 40 40 40 40 40  1 24 33  2  3 36 20 21 35 24  5]\n",
      " [39 40 40 40 40 40 40 40 40 40  1 24 33  2  3 36 20 21 35 24  5 32  2]\n",
      " [39 40 40 40 40 40 40 40 40 40 40 40 40  1 24 33  2  3  5 37 34  2 21]]\n",
      "[[39  6  1 27 35 23 28  8 37  6 23 28  5 16  0]\n",
      " [39 39 39 39 39  6  1 27 35 23 28  8 37 16 26]\n",
      " [39 39 39 39  6  1 27 35 23 28  8 32  6  1 37]\n",
      " [39 39 39 39  6  1 27 35 23 28  8 11  7  1 19]\n",
      " [39 39  6  1 27 35 23 28  8 11  7  1 19  5 35]]\n"
     ]
    }
   ],
   "source": [
    "print(phon_dict)\n",
    "print(phon_dicto)\n",
    "\n",
    "print(data['phons'][100:105])\n",
    "print(datao['phons'][100:105])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 1, 'n': 2, 'w': 3, 'l': 4, 't': 5, 'e': 6, 'j': 7, 'v': 8, 'x': 9, 'h': 10, 'u': 11, 'i': 12, 'q': 13, 's': 14, 'r': 15, 'k': 16, ' ': 17, 'c': 18, 'g': 19, 'p': 20, 'm': 21, 'z': 22, 'b': 23, 'y': 24, 'o': 25, 'd': 26, 'f': 27, '<GO>': 28, '<PAD>': 29}\n",
      "{'q': 1, 'a': 2, 'o': 3, 'r': 4, 'v': 5, 'j': 6, 'w': 7, 'e': 8, 'g': 9, 'k': 10, 't': 11, 'f': 12, 'p': 13, 'n': 14, 'c': 15, 'u': 16, 'x': 17, 'b': 18, 'd': 19, 'i': 20, ' ': 21, 'h': 22, 'm': 23, 'z': 24, 's': 25, 'y': 26, 'l': 27, '<GO>': 27, '<PAD>': 28}\n",
      "[[29 29 29 29 29  1 23  6  2 26  4 12 18 10]\n",
      " [29 29 29 29 29  1 23  6  2 26 21  1 10  4]\n",
      " [29 29 29 29 29 29  1 23  6  2 26 15 25  5]\n",
      " [29 29 29 29  1 23  6  2 26 15 25  6  5  6]\n",
      " [29 29 29 29  1 23  6  2  5  6 11  6 15  2]]\n",
      "[[27 28 28 28 28  2 18  8 14 19 27  2  8 14 19 20 25 15 22]\n",
      " [27 28 28 28 28 28 28 28 28 28  2 18  8 14 19 27 20 15 22]\n",
      " [27 28 28 28 28 28 28 28 28 28  2 18  8 14 19 23  2 22 27]\n",
      " [27 28 28 28 28 28 28 28 28 28 28  2 18  8 14 19  4  3 11]\n",
      " [27 28 28 28 28 28 28 28 28  2 18  8 14 19  4  3  8 11  8]]\n"
     ]
    }
   ],
   "source": [
    "print(word_dict)\n",
    "print(word_dicto)\n",
    "\n",
    "print(data['words'][100:105])\n",
    "print(datao['words'][100:105])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A a:\n",
      "aalen a:l\n",
      "aalglatt a:l#glat\n",
      "Aasgeier a:z#gai@r\n",
      "abaenderlich ap#End@r#lIx\n",
      "abbauwuerdig ap#bau#vYrdIx\n",
      "Abbrucharbeit ap#brEx#arbait\n"
     ]
    }
   ],
   "source": [
    "path = \"/Users/jannisborn/Desktop/LDS_Data/celex2/german/gpl/gpl.cd\"\n",
    "(w,p) = extract_celex(path)\n",
    "((words,phons) , (word_dict, phon_dict)) = str_to_num_dataset(w,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[28 29 29 29 29 29  1 23  6  2 26 21  1 10  4]\n",
      " [28 29 29 29 29 29 29  1 23  6  2 26 15 25  5]\n",
      " [28 29 29 29 29  1 23  6  2 26 15 25  6  5  6]\n",
      " [28 29 29 29 29  1 23  6  2  5  6 11  6 15  2]\n",
      " [28 29 29 29 29 29  1 23  6  2  5  6 11  6 15]]\n",
      "[[39 40 40 40  1 24 33  2  3 36 20 29  1 24  4]\n",
      " [39 40 40 40  1 24 33  2  3 36 20 21 35 24  5]\n",
      " [39 40  1 24 33  2  3 36 20 21 35 24  5 32  2]\n",
      " [39 40 40 40 40  1 24 33  2  3  5 37 34  2 21]\n",
      " [39 40 40 40 40  1 24 33  2  3  5 37 34  2 21]]\n",
      "[28 29 29 29 29 29  1 23  6  2 26 21  1 10]\n",
      "{'a': 1, 'n': 2, 'w': 3, 'l': 4, 't': 5, 'e': 6, 'j': 7, 'v': 8, 'x': 9, 'h': 10, 'u': 11, 'i': 12, 'q': 13, 's': 14, 'r': 15, 'k': 16, ' ': 17, 'c': 18, 'g': 19, 'p': 20, 'm': 21, 'z': 22, 'b': 23, 'y': 24, 'o': 25, 'd': 26, 'f': 27, '<GO>': 28, '<PAD>': 29} {'a': 1, '@': 2, 'n': 3, 'l': 4, 't': 5, 'U': 6, 'e': 7, '/': 8, 'j': 9, 'v': 10, 'Z': 11, 'x': 12, 'N': 13, 'h': 14, 'u': 15, 'i': 16, '|': 17, 'E': 18, 's': 19, '#': 20, 'r': 21, 'k': 22, 'S': 23, ':': 24, ' ': 25, 'g': 26, 'Y': 27, 'p': 28, 'm': 29, 'I': 30, 'z': 31, '+': 32, 'b': 33, 'y': 34, 'o': 35, 'd': 36, 'O': 37, 'f': 38, '<GO>': 39, '<PAD>': 40}\n",
      "29 40\n",
      "(31675, 15) (31675, 15)\n"
     ]
    }
   ],
   "source": [
    "print(words[100:105])\n",
    "print(phons[100:105])\n",
    "print(words[100,:-1])\n",
    "\n",
    "print(word_dict, phon_dict)\n",
    "print(len(word_dict),len(phon_dict))\n",
    "print(words.shape, phons.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('celex_new.npz', phons=phons, words=words, phon_dict=phon_dict, word_dict=word_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "a = np.zeros((2,3))\n",
    "b = np.ones((2,3))\n",
    "b -=1 \n",
    "print(np.array_equal(a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
