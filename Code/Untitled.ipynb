{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys,time,os\n",
    "from bLSTM import bLSTM\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fibel_retrieve(learn_type):\n",
    "\n",
    "    data = np.load('data/fibel.npz')\n",
    "    phon_dict = np_dict_to_dict(data['phon_dict'])\n",
    "    word_dict = np_dict_to_dict(data['word_dict'])\n",
    "\n",
    "    if learn_type == 'lds':\n",
    "\n",
    "        path = 'data/fibel_alt_targets.npy'\n",
    "        print(\"Loading alternative targets ...\")\n",
    "        alt_targs_raw = np.load(path)\n",
    "\n",
    "\n",
    "        alt_targs = np.array([np.array(d,dtype=np.int8) for d in alt_targs_raw])\n",
    "        print(\"Alternative targets successfully loaded.\")\n",
    "\n",
    "        return ( (data['phons'], data['words']) , (phon_dict, word_dict), alt_targs )\n",
    "    else:\n",
    "        return ( (data['phons'], data['words']) , (phon_dict, word_dict))\n",
    "\n",
    "def np_dict_to_dict(np_dict):\n",
    "    \"\"\"\n",
    "    Converts a dictionary saved via np.save (as structured np array) into an object of type dict\n",
    "\n",
    "    Parameters:\n",
    "    --------------\n",
    "    NP_DICT        : {np.array} structured np.array with dict keys and items\n",
    "\n",
    "    Returns:\n",
    "    --------------\n",
    "    DICT            : {dict} converted NP_DICT\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    return {key:np_dict.item().get(key) for key in np_dict.item()}\n",
    "\n",
    "def num_to_str(inputs,logits,labels,alt_targs,dict_in,dict_out,mode='normal'):\n",
    "    \"\"\"\n",
    "    Method receives the numerical arrays and prints the strings\n",
    "\n",
    "    If mode = normal, then alt_targs should be set to [], if it is 'lds' the alternative targets are also printed\n",
    "    \"\"\"\n",
    "\n",
    "    fullPred = logits.argmax(-1) # Prediction string with padding\n",
    "\n",
    "    \n",
    "    #Padded target string\n",
    "    fullTarg = np.copy(labels) \n",
    "    # Set pads to 0 - as preparation for edit_distance\n",
    "\n",
    "    out_str = []\n",
    "    inp_str = []\n",
    "    label_str = []\n",
    "    #print(inputs.shape,fullPred.shape,labels.shape,alt_targs.shape)\n",
    "    #print(dict_in)\n",
    "    #print(dict_out)\n",
    "    #print(inputs[0:5],fullPred[:5],labels[:5])\n",
    "    for k in range(len(fullPred)):\n",
    "        out_str.append(''.join([dict_out[l] if dict_out[l] != '<PAD>' and  dict_out[l] != '<GO>' else '' for l in fullPred[k]]))\n",
    "        inp_str.append(''.join([dict_in[l] if dict_in[l] != '<PAD>' and  dict_in[l] != '<GO>' else '' for l in inputs[k]]))\n",
    "        label_str.append(''.join([dict_out[l] if dict_out[l] != '<PAD>' and  dict_out[l] != '<GO>' else '' for l in labels[k]]))\n",
    "\n",
    "        print(\"The input \" + inp_str[-1].upper() + \" was written as \" + out_str[-1].upper() + \" with the target as \" + label_str[-1].upper())\n",
    "\n",
    "        if mode == 'lds':\n",
    "            alt_targ_str = []\n",
    "\n",
    "            z = np.argwhere(alt_targs[k]==0) # indices of zeros (alt_targs where padded to have equally sized array)\n",
    "            # position 0,1 is the first row that contains zeros (i.e. not an alternative writing anymore)\n",
    "            for l in range(z[0,1]):\n",
    "                alt_targ_str.append(''.join([dict_out[m] if dict_out[m] != '<PAD>' and  dict_out[m] != '<GO>' else '' for m in alt_targs[k,:,l] ]))\n",
    "            print(\"The alternatives were \", alt_targ_str)\n",
    "\n",
    "            \n",
    "def set_model_params(inputs, targets, dict_char2num_x, dict_char2num_y):\n",
    "    \"\"\"\n",
    "    This method can receive data from any dataset (inputs, targets) and the corresponding dictionaries.\n",
    "    It returns the hyperparameters for the model, i.e. input and output sequence length as well as input and output dictionary size.\n",
    "    \"\"\"\n",
    "\n",
    "    # Error handling. If the dicts are not objects of type dict but np.arrays (dicts saved via np.save), convert them back.\n",
    "    if isinstance(dict_char2num_x, np.ndarray):\n",
    "        dict_char2num_x = np_dict_to_dict(dict_char2num_x)\n",
    "    if isinstance(dict_char2num_y, np.ndarray):\n",
    "        dict_char2num_y = np_dict_to_dict(dict_char2num_y)\n",
    "\n",
    "\n",
    "    dict_num2char_x = dict(zip(dict_char2num_x.values(), dict_char2num_x.keys()))\n",
    "    dict_num2char_y = dict(zip(dict_char2num_y.values(), dict_char2num_y.keys()))\n",
    "    x_dict_size = len(dict_char2num_x)\n",
    "    num_classes = len(dict_char2num_y) # (y_dict_size) Cardinality of output dictionary\n",
    "    x_seq_length = len(inputs[0]) - 1 \n",
    "    y_seq_length = len(targets[0]) - 1 # Because of the <GO> as response onset\n",
    "\n",
    "    return x_dict_size, num_classes, x_seq_length, y_seq_length, dict_num2char_x, dict_num2char_y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.InteractiveSession()\n",
    "\n",
    "max_alt_wr = 2\n",
    "bs = 3\n",
    "seq_len = 4\n",
    "\n",
    "targ = tf.constant([[3,7,12,4],[18,18,2,2],[18,2,2,2]])\n",
    "\n",
    "wr = tf.constant([[3,7,12,1],[18,18,2,9],[18,7,2,2]],dtype=tf.int8)\n",
    "print('BEF',wr.get_shape())\n",
    "wr = tf.expand_dims(tf.ones([max_alt_wr,1],dtype=tf.int8),1) * wr\n",
    "print(wr.get_shape())\n",
    "wr = tf.transpose(wr,[1,2,0])\n",
    "\n",
    "\n",
    "alt_wr = tf.constant([[[4,7,12,1],[3,7,12,1]],[[1,2,3,4],[1,2,3,3]],\n",
    "                      [[18,7,2,2],[18,9,2,2]]],dtype=tf.int8)\n",
    "alt_wr = tf.transpose(alt_wr,[0,2,1])\n",
    "\n",
    "\n",
    "equal_raw = tf.equal(wr,alt_wr)\n",
    "equal = tf.reduce_all(equal_raw,1)\n",
    "equal_ind = tf.where(equal)\n",
    "\n",
    "\n",
    "a = equal_ind[:,0]\n",
    "print(type(targ),type(alt_wr),type(equal_ind))\n",
    "print(targ.eval(),\"That was current state\")\n",
    "\n",
    "# This is just the creation of the node new_targts.\n",
    "new_targets = tf.py_func(update_tensor,[targ, alt_wr, equal_ind], tf.int8)\n",
    "# This actually executes the new function\n",
    "print(new_targets.eval())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([2,3,4])\n",
    "\n",
    "print(np.where(a==2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "\n",
    "a = tf.constant([2, 5, -4, 0])\n",
    "aa = tf.Variable(a)\n",
    "print(type(aa))\n",
    "sess.run(tf.initialize_all_variables())\n",
    "b = tf.scatter_update(aa, [1], [9])\n",
    "print(b.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((inputs, targets) , (dict_char2num_x, dict_char2num_y), alt_targets) = fibel_retrieve('lds')\n",
    "x_dict_size, num_classes, x_seq_length, y_seq_length, dict_num2char_x, dict_num2char_y = set_model_params(inputs, targets, dict_char2num_x, dict_char2num_y)\n",
    "\n",
    "\n",
    "\n",
    "indices = range(len(inputs))\n",
    "X_train, X_test,Y_train, Y_test, Y_alt_train_l, Y_alt_test_l, ind_train, ind_test = train_test_split(inputs, targets, alt_targets,indices,test_size=0.1, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#logp = np.reshape(np.arange(174*22*30),[174,22,30])\n",
    "max_len = max([len(l) for l in Y_alt_train_l])\n",
    "inp_seq_len = len(Y_alt_train_l[1][0])\n",
    "Y_alt_train = np.zeros([len(Y_alt_train_l), inp_seq_len, max_len], dtype=np.int8)\n",
    "for word_ind in range(len(Y_alt_train_l)):\n",
    "    for write_ind in range(len(Y_alt_train_l[word_ind])):\n",
    "        Y_alt_train[word_ind,:,write_ind] = np.array(Y_alt_train_l[word_ind][write_ind],dtype=np.int8)\n",
    "#a=num_to_str(X_train,logp,Y_train,Y_alt_train,dict_num2char_x,dict_num2char_y,'lds')\n",
    "print(X_train.shape, Y_train.shape, Y_alt_train.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_num2char_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understand how to initialize 2 Tensorflow models,\n",
    "# train them in one session, and save them independently\n",
    "\n",
    "\n",
    "class model(object):\n",
    "    \n",
    "    def __init__(self,size):\n",
    "        \n",
    "        self.inp = tf.placeholder(tf.float32,(size,2),'inp')\n",
    "        self.mat = tf.placeholder(tf.float32,(2,5),'mat')\n",
    "        \n",
    "        \n",
    "        \n",
    "    def foo(self):\n",
    "        self.out = tf.matmul(self.inp,self.mat)\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small = model(3)\n",
    "small.foo()\n",
    "large = model(10)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    saver = tf.train.Saver(max_to_keep=4)\n",
    "\n",
    "    \n",
    "    b = sess.run(small.out, feed_dict={small.inp:np.random.random((3,2)),\n",
    "                                      small.mat:np.random.random((2,5))})\n",
    "    print(b)\n",
    "    \n",
    "    saver.save(sess, save_path + '/Model', global_step=epoch, write_meta_graph=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_dict_to_dict(np_dict):\n",
    "    \"\"\"\n",
    "    Converts a dictionary saved via np.save (as structured np array) into an object of type dict\n",
    "\n",
    "    Parameters:\n",
    "    --------------\n",
    "    NP_DICT        : {np.array} structured np.array with dict keys and items\n",
    "\n",
    "    Returns:\n",
    "    --------------\n",
    "    DICT            : {dict} converted NP_DICT\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    return {key:np_dict.item().get(key) for key in np_dict.item()}\n",
    "\n",
    "\n",
    "\n",
    "def extract_celex(path):\n",
    "    \"\"\"\n",
    "    Reads in data from the CELEX corpus\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    PATH        {str} the path to the desired celex file, i.e. gpl.cd \n",
    "                    (contains orthography and phonology)\n",
    "\n",
    "    Returns:\n",
    "    -----------\n",
    "    2 Tuples, each with 2 variables. \n",
    "        First tuple:\n",
    "    W           {np.array} of words (length 51728) for gpl.cd\n",
    "    P           {np.array} of phoneme sequences (length 51728) for gpl.cd\n",
    "        Second tuple:\n",
    "    WORD_DICT   {dict} allowing to map the numerical array W back to strings\n",
    "    PHON_DICT   {dict} doing the same for the phonetical arrays P\n",
    "\n",
    "    \n",
    "    Call via:\n",
    "    path = \"/Users/jannisborn/Desktop/LDS_Data/celex2/german/gpl/gpl.cd\"\n",
    "    ((w,p) , (word_dict, phon_dict)) = extract_celex(path)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    with open(path, 'r') as file:\n",
    "\n",
    "        raw_data = file.read().splitlines()\n",
    "        words = []\n",
    "        phons = []\n",
    "        m = 0\n",
    "        for ind,raw_line in enumerate(raw_data):\n",
    "            \n",
    "            line = raw_line.split(\"\\\\\")\n",
    "\n",
    "            if line[-2]: # Use only words that HAVE a SAMPA transcript (reduces from 51k to 37345)\n",
    "\n",
    "            # exclude foreign words that have the 'æ' tone (SAMPA '{' ) like in PoINte   - 18 words\n",
    "            # exclude foreign words that have the 'ɑ' tone (SAMPA 'A' ) like in NuANce   - 28 words\n",
    "            # exclude foreign words that have a nasal vowel (SAMPA '~' ) like in Jargon  - 22 words\n",
    "                if not 'A' in line[-2] and not '{' in line[-2] and not '~' in line[-2]: \n",
    "\n",
    "                    if not ('tS' in line[-2] and not 'tsch' in line[1]): # exclude 9 foreign words like 'Image', 'Match', 'Punch', 'Sketch'\n",
    "\n",
    "                        if len(line[1]) < 15 and len(line[-2]) < 15 : # exclude extra long words (reduces to 34376)\n",
    "                            \n",
    "                            if len(line[-2]) > m:\n",
    "                                m = len(line[-2])\n",
    "                                print(line[1],line[-2])\n",
    "                                \n",
    "\n",
    "                            words.append(line[1].lower()) # All words are lowercase only\n",
    "                            phons.append(line[-2]) # Using SAMPA notation\n",
    "\n",
    "    return words,phons\n",
    "\n",
    "\n",
    "\n",
    "def str_to_num_dataset(X,Y):\n",
    "    \"\"\"\n",
    "    This method receives 2 lists of strings (input X and output Y) and converts it to padded, numerical arrays.\n",
    "    It returns the numerical dataset as well as the dictionaries to retrieve the strings.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Define dictionaries \n",
    "    # Dictionary assignining a unique integer to each input character\n",
    "    try:\n",
    "        u_characters = set(' '.join(X)) \n",
    "    except TypeError:\n",
    "        # Exception for TIMIT dataset (one phoneme is repr. by seq. of chars)\n",
    "        print(\"TypeError occurred.\")\n",
    "        u_characters = set([quant for seq in X for quant in seq])\n",
    "\n",
    "    char2numX = dict(zip(u_characters, range(1,len(u_characters)+1)))\n",
    "\n",
    "    # Dictionary assignining a unique integer to each phoneme\n",
    "    try:\n",
    "        v_characters = set(' '.join(Y)) \n",
    "    except TypeError:\n",
    "        print(\"TypeError occurred.\")\n",
    "        v_characters = set([quant for seq in Y for quant in seq])\n",
    "    char2numY = dict(zip(v_characters, range(1,len(v_characters)+1))) # Using 0 causes trouble for tf.edit_distance\n",
    "    \n",
    "    # 2. Padding\n",
    "    # Pad inputs\n",
    "    char2numX['<GO>'] = len(char2numX) + 1\n",
    "    char2numX['<PAD>'] = len(char2numX) + 1\n",
    "    mx_l_X = max([len(word) for word in X]) # longest input sequence\n",
    "    # Padd all X for the final form for the LSTM\n",
    "    x = [[char2numX['<GO>']] + [char2numX['<PAD>']]*(mx_l_X - len(word)) +[char2numX[char] for char in word] for word in X]\n",
    "    x = np.array(x) \n",
    "\n",
    "    # Pad targets\n",
    "    char2numY['<GO>'] = len(char2numY) + 1 # Define number denoting the response onset\n",
    "    char2numY['<PAD>'] = len(char2numY) + 1 \n",
    "    mx_l_Y = max([len(phon_seq) for phon_seq in Y]) # longest output sequence\n",
    "\n",
    "    y = [[char2numY['<GO>']] + [char2numY['<PAD>']]*(mx_l_Y - len(ph_sq)) + [char2numY[phon] for phon in ph_sq] for ph_sq in Y]\n",
    "    y = np.array(y)\n",
    "\n",
    "    return ((x,y) , (char2numX,char2numY))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('data/celex.npz')\n",
    "phon_dict = np_dict_to_dict(data['phon_dict'])\n",
    "word_dict = np_dict_to_dict(data['word_dict'])\n",
    "\n",
    "\n",
    "datao = np.load('data/celex_old.npz')\n",
    "phon_dicto = np_dict_to_dict(datao['phon_dict'])\n",
    "word_dicto = np_dict_to_dict(datao['word_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "################## DATA ANALYSIS #####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
