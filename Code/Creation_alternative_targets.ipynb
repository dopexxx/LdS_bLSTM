{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* CELEX: Nimm die SAMPA ground truth, konvertiere nach IPA mit dem selbstgebauten dict basierend auf der WIKI Tabelle. Dann nimm das IPA -> Buchstaben dict um die alternativen Schreibweisen zu generieren und gehe die manuell durch und sortiere aus\n",
    "* ChildLex: Nimm alle Wörter aus dem Korpus und schaue nach welche Wörter mit SAMPA im CELEX Korpus existieren. Diejenigen die es gibt: Easy, da SAMPA existiert und sogar schon in IPA umgewandelt und sogar alternative writings da. Diejenigen die es nicht gibt (vermutlich wenige): Nimm den Online Konverter um Text in SAMPA umzuwandeln (das ist die ground truth! Achte darauf, dass Alphabet so wie beim CELEX Korpus). Wandel diese Wörter dann in IPA um und generiere alternative Schreibweisen\n",
    "* Fibelwörter: Gleicher Ansatz wie bei ChildLex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: Come up with alternative spellings for all orthographic words in a database\n",
    "# Tool 1: IPA -> Graphem Konverter Wiki\n",
    "# Tool 2: SAMPA -> IPA Konverter Wiki\n",
    "\n",
    "# Idea: Take the phonetic sequence (SAMPA) of every word, convert it to an IPA sequence and\n",
    "        # then convert that to all grapheme sequences\n",
    "    \n",
    "# Needs: A dictionary for SAMPA -> IPA (ideally non-ambiguous). A dictionary for IPA -> Text (ambigu.)\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dataset is 37268 samples\n"
     ]
    }
   ],
   "source": [
    "path = '/Users/jannisborn/Desktop/LDS_Data/celex2/german/gpl/gpl.cd'\n",
    "\n",
    "with open(path, 'r') as file:\n",
    "\n",
    "    raw_data = file.read().splitlines()\n",
    "    words = []\n",
    "    phons = []\n",
    "\n",
    "    for ind,raw_line in enumerate(raw_data):\n",
    "\n",
    "        line = raw_line.split(\"\\\\\")\n",
    "\n",
    "        \n",
    "        if line[-2]: # Use only words that HAVE a SAMPA transcript (reduces from 51k to 37345)\n",
    "\n",
    "            # exclude foreign words that have the 'æ' tone (SAMPA '{' ) like in\n",
    "            # exclude foreign words that have the 'ɑ' tone (SAMPA 'A' )\n",
    "            if not 'A' in line[-2] and not '{' in line[-2] and not '~' in line[-2]: \n",
    "                if not ('tS' in line[-2] and not 'tsch' in line[1]):\n",
    "\n",
    "                    words.append(line[1].lower()) # Make spellings lowercase only\n",
    "                    phons.append(line[-2]) # Using SAMPA notation\n",
    "\n",
    "print(\"Size of dataset is\", len(words), \"samples\")\n",
    "\n",
    "\n",
    "## Helper Method:\n",
    "\n",
    "def str_to_num_dataset(X,Y):\n",
    "    \"\"\"\n",
    "    This method receives 2 lists of strings (input X and output Y) and converts it to padded, numerical arrays.\n",
    "    It returns the numerical dataset as well as the dictionaries to retrieve the strings.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Define dictionaries \n",
    "    # Dictionary assignining a unique integer to each input character\n",
    "    try:\n",
    "        u_characters = set(' '.join(X)) \n",
    "    except TypeError:\n",
    "        # Exception for TIMIT dataset (one phoneme is repr. by seq. of chars)\n",
    "        print(\"TypeError occurred.\")\n",
    "        u_characters = set([quant for seq in X for quant in seq])\n",
    "\n",
    "    char2numX = dict(zip(u_characters, range(len(u_characters))))\n",
    "\n",
    "    # Dictionary assignining a unique integer to each phoneme\n",
    "    try:\n",
    "        v_characters = set(' '.join(Y)) \n",
    "    except TypeError:\n",
    "        print(\"TypeError occurred.\")\n",
    "        v_characters = set([quant for seq in Y for quant in seq])\n",
    "    char2numY = dict(zip(v_characters, range(1,len(v_characters)+1))) # Using 0 causes trouble for tf.edit_distance\n",
    "    \n",
    "    # 2. Padding\n",
    "    # Pad inputs\n",
    "    char2numX['<GO>'] = len(char2numX) \n",
    "    char2numX['<PAD>']  = len(char2numX) \n",
    "    mx_l_X = max([len(word) for word in X]) # longest input sequence\n",
    "    # Padd all X for the final form for the LSTM\n",
    "    x = [[char2numX['<PAD>']]*(mx_l_X - len(word)) +[char2numX[char] for char in word] for word in X]\n",
    "    x = np.array(x) \n",
    "\n",
    "    # Pad targets\n",
    "    char2numY['<GO>'] = len(char2numY) # Define number denoting the response onset\n",
    "    char2numY['<PAD>'] = len(char2numY)  \n",
    "    mx_l_Y = max([len(phon_seq) for phon_seq in Y]) # longest output sequence\n",
    "\n",
    "    y = [[char2numY['<GO>']] + [char2numY['<PAD>']]*(mx_l_Y - len(ph_sq)) + [char2numY[phon] for phon in ph_sq] for ph_sq in Y]\n",
    "    y = np.array(y)\n",
    "\n",
    "    return ((x,y) , (char2numX,char2numY))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "((phons_num, words_num), (phon_dict, word_dict)) = str_to_num_dataset(phons,words)\n",
    "#print(len(word_dict),word_dict)\n",
    "#print(len(phon_dict),phon_dict)\n",
    "#print(words_num.shape, phons_num.shape)\n",
    "#print(words_num[321,:], phons_num[321,:])\n",
    "\n",
    "np.savez(\"data/celex.npz\", words=words_num, phons=phons_num, word_dict=word_dict, phon_dict=phon_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of phonetic dict is  40  and the keys are: \n",
      " ['s', 'u', 'U', 'm', 'h', 't', 'k', 'l', 'z', ':', 'y', 'o', 'Y', 'v', 'N', 'n', '@', '#', 'E', 'a', 'O', 'e', 'j', 'I', 'd', 'r', 'p', '+', 'i', 'S', 'Z', 'f', 'g', 'b', '|', 'x', ' ', '/', '<GO>', '<PAD>']\n"
     ]
    }
   ],
   "source": [
    "data = np.load('data/celex.npz')\n",
    "sampa_dict = {key:data['phon_dict'].item().get(key) for key in data['phon_dict'].item()}\n",
    "sampa_keys = list(sampa_dict.keys()) # Has 43 keys originally, 40 after excluding {, ~, A\n",
    "print(\"Length of phonetic dict is \", len(sampa_dict), \" and the keys are: \\n\", sampa_keys) # Has 43 keys\n",
    "\n",
    "\n",
    "# Step 1: Make a sampa_ipa dict. How to: Go to SAMPA -> IPA tabelle, for every SAMPA char, check whether it \n",
    "# is in the CELEX korpus. If yes, look up example word from wiki in korpus and check whether it is the right sampa\n",
    "# sign. If yes, look up on wiktionary example word in IPA and check whether output sign is correct.\n",
    "\n",
    "sampa_ipa = dict()\n",
    "\n",
    "# Vowels\n",
    "sampa_ipa['i'] = 'i'\n",
    "sampa_ipa[':'] = 'ː'\n",
    "sampa_ipa['I'] = 'ɪ'\n",
    "sampa_ipa['e'] = 'e'\n",
    "sampa_ipa['E'] = 'ɛ'\n",
    "sampa_ipa['y'] = 'y'   # meaning a real ü like in kühl\n",
    "sampa_ipa['@'] = 'ə'\n",
    "sampa_ipa['a'] = 'a'\n",
    "sampa_ipa['u'] = 'u'\n",
    "sampa_ipa['U'] = 'ʊ'\n",
    "sampa_ipa['o'] = 'o'\n",
    "sampa_ipa['O'] = 'ɔ'\n",
    "\n",
    "\n",
    "# consonants\n",
    "sampa_ipa['p'] = 'p'\n",
    "sampa_ipa['b'] = 'b'\n",
    "sampa_ipa['t'] = 't'\n",
    "sampa_ipa['d'] = 'd'\n",
    "sampa_ipa['k'] = 'k'\n",
    "sampa_ipa['g'] = 'g'\n",
    "sampa_ipa['f'] = 'f'\n",
    "sampa_ipa['v'] = 'v'\n",
    "sampa_ipa['s'] = 's'\n",
    "sampa_ipa['z'] = 'z'\n",
    "sampa_ipa['S'] = 'ʃ'\n",
    "sampa_ipa['x'] = 'x'\n",
    "sampa_ipa['h'] = 'h'\n",
    "sampa_ipa['m'] = 'm'\n",
    "sampa_ipa['n'] = 'n'\n",
    "sampa_ipa['N'] = 'ŋ'\n",
    "sampa_ipa['l'] = 'l'\n",
    "sampa_ipa['r'] = 'r'\n",
    "sampa_ipa['j'] = 'j'\n",
    "sampa_ipa['Z'] = 'ʒ'\n",
    "sampa_ipa['+'] = ''  # meaning a bit unclear\n",
    "sampa_ipa['#'] = 'ˈ' # following syllabus carries primary intonation\n",
    "sampa_ipa['|'] = 'ø' # meaning a bit unclear\n",
    "sampa_ipa['/'] = 'œ' # usually SAMPA uses 9 instead of / for this \n",
    "sampa_ipa['Y'] = 'ʏ' # meaning more a 'oü' like in Müll\n",
    "\n",
    "# These are 37 keys only, so 6 are missing. Remaining ones are:\n",
    "\n",
    "# <GO>          not needed for alt. writing creation\n",
    "# <PAD>         not needed for alt. writing creation\n",
    "#    (SPACE)    not needed\n",
    "# {             excluded some foreign words\n",
    "# A             excluded some foreign words\n",
    "# ~             excluded some foreign words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now convert the SAMPA words into IPA words:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of IPA samples is 37268 . Some samples are: \n",
      "\n",
      " WORD         ===>        SAMPA        ===>        IPA\n",
      "schwererziehbar  =>  Sve:r#Er#tsi:#ba:r  =>  ʃveːrˈɛrˈtsiːˈbaːr\n",
      "schwerfallen  =>  Sve:r#fal  =>  ʃveːrˈfal\n",
      "schwerfaellig  =>  Sve:r#fal+Ix  =>  ʃveːrˈfalɪx\n",
      "schwerfaelligkeit  =>  Sve:r#fal+Ix#kait  =>  ʃveːrˈfalɪxˈkait\n",
      "schwergewicht  =>  Sve:r#g@#vi:g+t  =>  ʃveːrˈgəˈviːgt\n",
      "schwergewichtig  =>  Sve:r#g@#vi:g+t+Ix  =>  ʃveːrˈgəˈviːgtɪx\n",
      "schwergewichtler  =>  Sve:r#g@#vi:g+t#l@r  =>  ʃveːrˈgəˈviːgtˈlər\n",
      "schwerhoerig  =>  Sve:r#h|:r+Ix  =>  ʃveːrˈhøːrɪx\n",
      "schwerhoerigkeit  =>  Sve:r#h|:r+Ix#kait  =>  ʃveːrˈhøːrɪxˈkait\n",
      "schwerindustrie  =>  Sve:r#IndUstri:  =>  ʃveːrˈɪndʊstriː\n"
     ]
    }
   ],
   "source": [
    "ipa = []\n",
    "for samp in phons:\n",
    "    s = []\n",
    "    for char in samp:\n",
    "        s.append(sampa_ipa[char])\n",
    "    ipa.append(''.join(s))\n",
    "print(\"Amount of IPA samples is\", len(ipa), \". Some samples are: \\n\")\n",
    "print(\" WORD         ===>        SAMPA        ===>        IPA\")\n",
    "for k in range(25100, 25110):\n",
    "    print(words[k],\" => \", phons[k],\" => \", ipa[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now define the second dictionary, mapping IPA signs to graphemes (according to [here](https://de.wiktionary.org/wiki/Verzeichnis:Deutsch/Phoneme_und_Grapheme) ) \n",
    "\n",
    "Go to every phon and write down all graphemes (copying table, excluding very weird graphemes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_values(['i', 'ː', 'ɪ', 'e', 'ɛ', 'y', 'ə', 'a', 'u', 'ʊ', 'o', 'ɔ', 'p', 'b', 't', 'd', 'k', 'g', 'f', 'v', 's', 'z', 'ʃ', 'x', 'h', 'm', 'n', 'ŋ', 'l', 'r', 'j', 'ʒ', '', 'ˈ', 'ø', 'œ', 'ʏ'])\n"
     ]
    }
   ],
   "source": [
    "print(sampa_ipa.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37 46\n",
      "ː\n",
      "\n",
      "ˈ\n",
      "ø\n"
     ]
    }
   ],
   "source": [
    "ipa_graph = dict()\n",
    "ipa_graph['t'] = ['t', 'd', 'tt', 'th', 'dt']\n",
    "ipa_graph['ə'] = ['e']\n",
    "ipa_graph['n'] = ['n', 'nn']\n",
    "ipa_graph['s'] = ['s', 'ss', 'ß', 'c', 'z'] # excluded t for Patience, ce for Renaissance/Farce, zz for Jazz\n",
    "ipa_graph['a'] = ['a', 'ah']\n",
    "ipa_graph['r'] = ['r', 'rh', 'rr'] # excluded rrh for Zirrhose/Myrrhe\n",
    "ipa_graph['l'] = ['l', 'll']\n",
    "ipa_graph['ɛ'] = ['e', 'ae']\n",
    "ipa_graph['f'] = ['f', 'v', 'ff', 'ph']\n",
    "ipa_graph['g'] = ['g', 'gg', 'gh']\n",
    "ipa_graph['ɪ'] = ['i']\n",
    "ipa_graph['k'] = ['k', 'g', 'ck', 'c', 'ch', 'kk' ] # Excluded cch for Zucchini, gg for Flaggschiff, qu for Boutique\n",
    "                  # Even kk is reasonable (Mokka, Akkordeon). qu chars are usually kv ipa (tracked below)\n",
    "ipa_graph['m'] = ['m', 'mm']\n",
    "ipa_graph['b'] = ['b', 'bb']\n",
    "ipa_graph['ʃ'] = ['sch', 's'] # excluded sk for Ski, sh for Sheriff, Show and ch for Recherche \n",
    "ipa_graph['d'] = ['d','dd']\n",
    "ipa_graph['p'] = ['p', 'b', 'pp'] # excluded bb for abebben or schrubben\n",
    "ipa_graph['ŋ'] = ['ng','n']\n",
    "ipa_graph['ɔ'] = ['o'] # excluded ch for Chauffeur (very rare exception)\n",
    "ipa_graph['v'] = ['w', 'v']\n",
    "ipa_graph['ʊ'] = ['u']\n",
    "ipa_graph['z'] = ['s'] # excluded zz for Blizzard, Puzzle and z for zoomen, bulldozer (since only in foreign words)\n",
    "ipa_graph['h'] = ['h']\n",
    "ipa_graph['i'] = ['i', 'y'] # excluding y (Baby/Party/Hockey) would be reasonbale (only 10 words in corpus...)\n",
    "ipa_graph['ʏ'] = ['ue', 'y', 'u'] # The corpus is weird here and writes Druck as drʏk, i.e. \"Drück\" rather than drʊk\n",
    "ipa_graph['x'] = ['ch']\n",
    "ipa_graph['e'] = ['e'] # excluded ee for Kaffee since IPA would be eː\n",
    "ipa_graph['j'] = ['j', 'y']\n",
    "ipa_graph['u'] = ['u'] # excluded ou like in Boutique\n",
    "ipa_graph['o'] = ['o'] # not needed anyways since o always followed by ː\n",
    "ipa_graph['œ'] = ['oe']\n",
    "ipa_graph['y'] = ['y']\n",
    "ipa_graph['ʒ'] = ['g', 'j'] # no wiki entry, self generated. For Garage or Jury\n",
    "\n",
    "\n",
    "# 2 character keys:\n",
    "ipa_graph['ts'] = ['z', 'ts', 'tts', 'tz', 't'] # excluded zz for Pizza/Skizze, c for circa, Penicillin\n",
    "            # but t is reasonable for Aktion, Negation, Infektion, Proportion, ...\n",
    "ipa_graph['aː'] = ['a', 'ah', 'aa']\n",
    "ipa_graph['ai'] = ['ei', 'ai'] # excluded ail for Detail, aill for Medaillon, aille for Medaille and y for Nylon\n",
    "ipa_graph['iː'] = ['ie', 'i', 'ieh', 'ih']\n",
    "ipa_graph['eː'] = ['e', 'ee', 'eh'] # excluded et like in Bidet\n",
    "ipa_graph['ɛː'] = ['ae', 'aeh']\n",
    "ipa_graph['uː'] = ['u', 'uh'] # excluded ou like in Ragout, Limousine and oo like on zoomen/Cartoon\n",
    "ipa_graph['oː'] = ['o', 'oh', 'oo'] # excluded au for aubergine/sauce and eau for plateau, Niveau\n",
    "ipa_graph['yː'] = ['ue', 'ueh', 'y'] # excluded uet like in Debüt and u like in deja-vu\n",
    "ipa_graph['ɔy'] = ['eu', 'aeu', 'oi', 'oy'] # instead of what wiki calls ɔɪ̯\n",
    "ipa_graph['ks'] = ['chs', 'x', 'ks', 'gs'] #excluded gs like in legst/bugsieren and ggs like in eggst (?)\n",
    "ipa_graph['øː'] = ['oe', 'oeh'] # excluded eu like in Ingenieur and eue like in Queue (?)\n",
    "ipa_graph['kv'] = ['qu']\n",
    "\n",
    "\n",
    "print(len(sampa_ipa), len(ipa_graph))\n",
    "# We had 36 keys in sampa_ipa dict, now we have 46 already in ipa_graph due to 2-phoneme-groups\n",
    "# But still there are one-char-values in sampa_ipa which are not keys in ipa_graph. Let us print them:\n",
    "for key in sampa_ipa.values():\n",
    "    if key not in ipa_graph.keys():\n",
    "        print(key)\n",
    "        \n",
    "# Okay 4 are missing:\n",
    "ipa_graph['ː'] = [''] # not needed anyways since ː always occurs after vowel\n",
    "ipa_graph['ˈ'] = [''] # Just a pronounciation symbol, does not carry meaning for spelling\n",
    "# Then the empty string '' is not needed as key\n",
    "# Then ø only occurs followed by a ː\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pretty tough to draw the boundary between which phons rarely but \"regularly\" translate to grapheme strings ([k] to kk like in Akkordeon, Mokka is still regular?) whereas [ʃ] (sch) to ch like in Champagner, Recherche or Lunch is irregular?\n",
    "\n",
    "## Use the IPA->Graphem dict to create alternative writings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_word(word, ipa_graph):\n",
    "    \"\"\"\n",
    "    Splits up an IPA word into a list of lists each with the possible replacement grapheme for each phoneme\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    WORD       {list} in IPA notation\n",
    "    IPA_GRAPH  {dict} mapping IPA symbols to possible grapheme sequences\n",
    "    \n",
    "    Returns:\n",
    "    ---------\n",
    "    CHARS      {list} containing lists with possible grapheme sequences\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    chars = []\n",
    "    \n",
    "    for ind in range(len(word)-1):\n",
    "        \n",
    "        if word[ind:ind+2] in ipa_graph:\n",
    "            chars.append(ipa_graph[word[ind:ind+2]])\n",
    "        else:\n",
    "            chars.append(ipa_graph[word[ind]])\n",
    "            \n",
    "    chars.append(ipa_graph[word[ind+1]])\n",
    "    \n",
    "    return chars\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The cell below generates the alternative writings (takes some time...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently examining word  0\n",
      "0 0\n",
      "2 2\n",
      "6 4\n",
      "359 13\n",
      "1440 22\n",
      "259200 158\n",
      "Currently examining word  200\n",
      "5598720 225\n"
     ]
    }
   ],
   "source": [
    "all_writings = []\n",
    "m = 0\n",
    "\n",
    "for ind,ip in enumerate(ipa):\n",
    "    if ind % 200 == 0:\n",
    "        print(\"Currently examining word \", ind)\n",
    "        \n",
    "    word_lists = split_word(ip, ipa_graph)\n",
    "    alt_write_raw = list(itertools.product(*word_lists))\n",
    "    alt_write = [''.join(a) for a in alt_write_raw]\n",
    "    try:\n",
    "        alt_write.remove(words[ind])\n",
    "    except ValueError:\n",
    "        _ = 1\n",
    "        \n",
    "    all_writings.append(alt_write)\n",
    "    \n",
    "    if len(alt_write) > m:\n",
    "        print(m,ind)\n",
    "        m = len(alt_write)\n",
    "        \n",
    "        \n",
    "print(\"DONE! Alternative writings generated for \", ind, \"words. Resulting list has\", len(all_writings), 'entries.')\n",
    "np.save('celex_all_writings', all_writings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "a = ['1', 'fsdfs']\n",
    "np.save('a', a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "def BAS_P2G_retrieve():\n",
    "    \"\"\"\n",
    "    Shortcut method for quickly retrieving numerical dataset of BAS-Sprecher corpus\n",
    "    In case whole dataset is not copied on remote machine\n",
    "    \"\"\"\n",
    "    data = np.load('data/BAS_P2G.npz')\n",
    "    input_dict = np_dict_to_dict(data['inp_dict'])\n",
    "    target_dict = np_dict_to_dict(data['tar_dict'])\n",
    "\n",
    "    return ( (data['inputs'], data['targets']) , (input_dict, target_dict) )\n",
    "\n",
    "def batch_data(x, y, BATCH_SIZE):\n",
    "    \"\"\"\n",
    "    Receives a batch_size and the entire training data [i.e inputs (x) and labels (y)]\n",
    "    Returns a data iterator\n",
    "    \"\"\"\n",
    "    shuffle = np.random.permutation(len(x))\n",
    "    start = 0\n",
    "    x = x[shuffle]\n",
    "    y = y[shuffle]\n",
    "    while start + BATCH_SIZE <= len(x):\n",
    "        yield x[start:start+BATCH_SIZE], y[start:start+BATCH_SIZE]\n",
    "        start += BATCH_SIZE\n",
    "\n",
    "def np_dict_to_dict(np_dict):\n",
    "    \"\"\"\n",
    "    Converts a dictionary saved via np.save (as structured np array) into an object of type dict\n",
    "\n",
    "    Parameters:\n",
    "    --------------\n",
    "    NP_DICT        : {np.array} structured np.array with dict keys and items\n",
    "\n",
    "    Returns:\n",
    "    --------------\n",
    "    DICT            : {dict} converted NP_DICT\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    return {key:np_dict.item().get(key) for key in np_dict.item()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def extract_celex(path):\n",
    "    \"\"\"\n",
    "    Reads in data from the CELEX corpus\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    PATH        {str} the path to the desired celex file, i.e. gpl.cd \n",
    "                    (contains orthography and phonology)\n",
    "    \n",
    "    Returns:\n",
    "    -----------\n",
    "    WORDS       {list} of words (length 51728) for gpl.cd\n",
    "    PHONS       {list} of phoneme sequences (length 51728) for gpl.cd\n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    with open(path, 'r') as file:\n",
    "\n",
    "        raw_data = file.read().splitlines()\n",
    "        words = []\n",
    "        phons = []\n",
    "        \n",
    "        for ind,raw_line in enumerate(raw_data):\n",
    "            \n",
    "            \n",
    "            line = raw_line.split(\"\\\\\")\n",
    "            words.append(line[1])\n",
    "            phons.append(line[-2]) # Using SAMPA notation\n",
    "                \n",
    "    return words, phons\n",
    "                               \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/Users/jannisborn/Desktop/LDS_Data/celex2/german/gpl/gpl.cd\"\n",
    "words, phons = extract_celex(path)\n",
    "\n",
    "print(words[30], phons[30])\n",
    "\n",
    "((w,p) , (word_dict, phon_dict)) = str_to_num_dataset(words,phons)\n",
    "\n",
    "print(w.shape, p.shape, len(word_dict), len(phon_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('celex.npz', words=w, phons=p, word_dict=word_dict, phon_dict=phon_dict)\n",
    "data = np.load('data/celex.npz')\n",
    "print(data['phons'].shape)\n",
    "print(data['words'].shape)\n",
    "print(data['phon_dict'])\n",
    "print(data['word_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "\"\"\"# Check which words are double in dataset\n",
    "for ind,word in enumerate(ipa):\n",
    "    ipa2 = ipa[:]\n",
    "    del ipa2[ind]\n",
    "    if word in ipa2:\n",
    "        print(word, words[ind],phons[ind])\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def BAS_json(path):\n",
    "    \"\"\"\n",
    "    This method receives a path for the BAS-SprecherInnen corpus and iterates through all JSON files in all subfolders.\n",
    "    It creates and returns a list of words and a list of pronounciations\n",
    "    \"\"\"\n",
    "    \n",
    "    import json, os\n",
    "\n",
    "    words = []\n",
    "    prons = []\n",
    "    ind = 0\n",
    "    # Read in filenames\n",
    "    for dirpath, dirnames, filenames in os.walk(path):\n",
    "        for filename in [f for f in filenames if f.endswith(\".json\")]:\n",
    "\n",
    "            if filename == 'SprecherInnen_DBconfig.json':\n",
    "                continue\n",
    "\n",
    "            # Open the json\n",
    "            with open(os.path.join(dirpath,filename)) as json_file:\n",
    "                data = json.load(json_file)\n",
    "\n",
    "                for item in data['levels'][1]['items']:\n",
    "                    words.append(item['labels'][0]['value'])\n",
    "                    prons.append(item['labels'][1]['value'])\n",
    "\n",
    "    return words,prons\n",
    "\n",
    "\n",
    "def clean_corpus_BAS_Sprecherinnen(words,prons):\n",
    "    \"\"\"\n",
    "    This method receives a list of words and a list of pronunciations of the BAS-Sprech. corpus and returns a cleaned dataset.\n",
    "    Clearning means:    1) Removing multiple occurrences of words       2) Remove misspellings and ambiguities\n",
    "                        3) Remove capitalization at begin of sentence   \n",
    "    Homophon words (Meer, mehr) are kept!\n",
    "\n",
    "    This method required manual inspection (once for each corpus).\n",
    "    Returns a condensed list of words and pronounciations (strings) that can be converted in numerical values next.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # First, we remove multiple occurrences.\n",
    "    # We cannot use set(words), set(prons) since some words are homophon 8(results in diff. lengths)\n",
    "    all_tups = []\n",
    "    for (w,p) in zip(words,prons):\n",
    "        all_tups.append(tuple((w,p)))\n",
    "    set_tup = set(all_tups)\n",
    "    print('Amount of non-unique words in corpus is ', len(all_tups))\n",
    "    unique_tups = dict(set_tup)\n",
    "    print('Amount of unique words in corpus is ', len(set_tup))\n",
    "\n",
    "    # Now we have removed multiple occurrences and we have a dict of tuples (word, pron)\n",
    "\n",
    "    def find_poss_mistakes(unique_tups):\n",
    "        \"\"\"\n",
    "        Receives a list of hopefully unique tupels (word,pron) and collect the tupels\n",
    "        which may have incorrect spelling/pronounciations.\n",
    "        \"\"\"\n",
    "        possible_mistakes = []\n",
    "        for key, val in unique_tups.items():\n",
    "            for keyy,vall in unique_tups.items():\n",
    "                if key != keyy and val == vall:\n",
    "                    # Detect multiple spellings of same pronounciation\n",
    "                    possible_mistakes.append((key,val, keyy, vall))\n",
    "                if key == keyy and val != vall:\n",
    "                    # Detect multiple pronounciations of same spelling\n",
    "                    possible_mistakes.append((key,val, keyy, vall))\n",
    "                    \n",
    "        return possible_mistakes\n",
    "        \n",
    "    poss_mist = find_poss_mistakes(unique_tups)\n",
    "    \"\"\"\n",
    "    print(\"+++ Possible mistakes are +++\")\n",
    "    for k in range(len(poss_mist)):\n",
    "        print(poss_mist[k][0],' -> ',poss_mist[k][1], \n",
    "              poss_mist[k][2],' -> ',poss_mist[k][2])\n",
    "    \"\"\"\n",
    "        \n",
    "    # Remove mistakes (after manual inspection)\n",
    "    unique_tups.pop('BäckerInnen') # removing as a duplicate of Bäckerinnen\n",
    "    unique_tups.pop('nu') # Duplicate of Nu\n",
    "    unique_tups.pop('Abonentinnen') # Misspelled\n",
    "    unique_tups.pop('Mit') # Duplicate of mit\n",
    "    unique_tups.pop('A') # Duplicate of ah\n",
    "    unique_tups.pop('Bei') # Duplicate of bei\n",
    "    unique_tups.pop('backwaren') # Duplicate of Backwaren\n",
    "    unique_tups.pop('-vertreterinnen') # Duplicate of Vertreterinnen\n",
    "    unique_tups.pop('leu') # Duplicate of Leu\n",
    "    unique_tups.pop('teil') # Duplicate of Teilt\n",
    "    unique_tups.pop('Un') # Duplicate of un\n",
    "    unique_tups.pop('Ver') # Duplicate of ver\n",
    "    unique_tups.pop('AutorInnen') # Duplicate of Autorinnen\n",
    "    unique_tups.pop('FreundInnen') # Duplicate of Freundinnen\n",
    "    unique_tups.pop('-pflegerin') # Duplicate of Pflegerin\n",
    "    unique_tups.pop('Neu') # Duplicate of neu\n",
    "    unique_tups.pop('re') # Duplicate of Re\n",
    "    unique_tups.pop('-kolleginnen') # Duplicate of Koleginnen\n",
    "    unique_tups.pop('-trinkerinnen') # Duplicate of Trinkerinnen\n",
    "    unique_tups.pop('Twitter-NutzerInnen') # Duplicate of Twitter-Nutzerinnen\n",
    "    unique_tups.pop('kommissionen') # Duplicate of Koleginnen\n",
    "    # Remaining: (Ihnen, ihnen), (dass,das), (Meer, mehr), (Ihres, ihres), (mal, Mal)\n",
    "\n",
    "    wordss = list(unique_tups.keys())\n",
    "    pronss = list(unique_tups.values())\n",
    "    print('After clearning ', len(wordss), ' different words remain')\n",
    "\n",
    "    return wordss, pronss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/jannisborn/Desktop/LDS_Data/BAS_SprecherInnen'\n",
    "words, prons = BAS_json(path)\n",
    "words, prons = clean_corpus_BAS_Sprecherinnen(words,prons)\n",
    "\n",
    "((x,y) , (char2numX, char2numY)) = str_to_num_dataset(words,prons)\n",
    "np.savez('BAS_G2P.npz', inputs=x,targets=y, inp_dict=char2numX,\n",
    "        tar_dict=char2numY)\n",
    "\n",
    "((x,y) , (char2numX, char2numY)) = str_to_num_dataset(prons,words)\n",
    "np.savez('BAS_P2G.npz', inputs=x,targets=y, inp_dict=char2numX,\n",
    "        tar_dict=char2numY)\n",
    "\n",
    "print(type(x), type(y), type(char2numX),type(char2numY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insight 1: If you have k different classes, then k is the third \n",
    "#    dimension of the logits and then the target matrix must not \n",
    "#    contain values higher than k-1 (i.e. labels are 0, ..., k)\n",
    "\n",
    "# Insight 2: sequence_loss expects unnormalized logits (BEFORE softmax!)\n",
    "# -> This makes testing for me harder, since I cannot simply use values\n",
    "# with a sum over 1 (will get normalized)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
