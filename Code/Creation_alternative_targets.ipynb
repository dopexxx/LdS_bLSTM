{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* CELEX: Nimm die SAMPA ground truth, konvertiere nach IPA mit dem selbstgebauten dict basierend auf der WIKI Tabelle. Dann nimm das IPA -> Buchstaben dict um die alternativen Schreibweisen zu generieren und gehe die manuell durch und sortiere aus\n",
    "* ChildLex: Nimm alle Wörter aus dem Korpus und schaue nach welche Wörter mit SAMPA im CELEX Korpus existieren. Diejenigen die es gibt: Easy, da SAMPA existiert und sogar schon in IPA umgewandelt und sogar alternative writings da. Diejenigen die es nicht gibt (vermutlich wenige): Nimm den Online Konverter um Text in SAMPA umzuwandeln (das ist die ground truth! Achte darauf, dass Alphabet so wie beim CELEX Korpus). Wandel diese Wörter dann in IPA um und generiere alternative Schreibweisen\n",
    "* Fibelwörter: Gleicher Ansatz wie bei ChildLex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jannisborn/anaconda3/envs/tf/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# Task: Come up with alternative spellings for all orthographic words in a database\n",
    "# Tool 1: IPA -> Graphem Konverter Wiki\n",
    "# Tool 2: SAMPA -> IPA Konverter Wiki\n",
    "\n",
    "# Idea: Take the phonetic sequence (SAMPA) of every word, convert it to an IPA sequence and\n",
    "        # then convert that to all grapheme sequences\n",
    "    \n",
    "# Needs: A dictionary for SAMPA -> IPA (ideally non-ambiguous). A dictionary for IPA -> Text (ambigu.)\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/jannisborn/Desktop/LDS_Data/celex2/german/gpl/gpl.cd'\n",
    "\n",
    "def extract_celex(path):\n",
    "    \"\"\"\n",
    "    Reads in data from the CELEX corpus\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    PATH        {str} the path to the desired celex file, i.e. gpl.cd \n",
    "                    (contains orthography and phonology)\n",
    "\n",
    "    Returns:\n",
    "    -----------\n",
    "    2 Tuples, each with 2 variables. \n",
    "        First tuple:\n",
    "    W           {np.array} of words (length 51728) for gpl.cd\n",
    "    P           {np.array} of phoneme sequences (length 51728) for gpl.cd\n",
    "        Second tuple:\n",
    "    WORD_DICT   {dict} allowing to map the numerical array W back to strings\n",
    "    PHON_DICT   {dict} doing the same for the phonetical arrays P\n",
    "\n",
    "    \n",
    "    Call via:\n",
    "    path = \"/Users/jannisborn/Desktop/LDS_Data/celex2/german/gpl/gpl.cd\"\n",
    "    ((w,p) , (word_dict, phon_dict)) = extract_celex(path)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    with open(path, 'r') as file:\n",
    "\n",
    "        raw_data = file.read().splitlines()\n",
    "        words = []\n",
    "        phons = []\n",
    "        m = 0\n",
    "        t = 0\n",
    "        for ind,raw_line in enumerate(raw_data):\n",
    "            \n",
    "            line = raw_line.split(\"\\\\\")\n",
    "\n",
    "            if line[-2]: # Use only words that HAVE a SAMPA transcript (reduces from 51k to 37345)\n",
    "\n",
    "            # exclude foreign words that have the 'æ' tone (SAMPA '{' ) like in PoINte   - 18 words\n",
    "            # exclude foreign words that have the 'ɑ' tone (SAMPA 'A' ) like in NuANce   - 28 words\n",
    "            # exclude foreign words that have a nasal vowel (SAMPA '~' ) like in Jargon  - 22 words\n",
    "                if not 'A' in line[-2] and not '{' in line[-2] and not '~' in line[-2]: \n",
    "\n",
    "                    if not ('tS' in line[-2] and not 'tsch' in line[1]): # exclude 9 foreign words like 'Image', 'Match', 'Punch', 'Sketch'\n",
    "\n",
    "                        if len(line[1]) < 15 and len(line[-2]) < 15 : # exclude extra long words (reduces to 34376)\n",
    "                            \n",
    "                            if len(line[-2]) > m:\n",
    "                                m = len(line[-2])\n",
    "                                print(line[1],line[-2])\n",
    "                                \n",
    "\n",
    "                            words.append(line[1].lower()) # All words are lowercase only\n",
    "                            phons.append(line[-2]) # Using SAMPA notation\n",
    "                            \n",
    "                        else:\n",
    "                            t+=1\n",
    "                            \n",
    "    print(\"Excluded\",t, \"words because they were too long (more than 15 phons)\" )\n",
    "    print(\"Size of dataset is\", len(words), \"samples\")\n",
    "\n",
    "    return words,phons\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def str_to_num_dataset(X,Y,pads=5):\n",
    "    \"\"\"\n",
    "    This method receives 2 lists of strings (input X and output Y) and converts it to padded, numerical arrays.\n",
    "    It returns the numerical dataset as well as the dictionaries to retrieve the strings.\n",
    "    \n",
    "    PADS    {int} specifiying how many additional fields should be padded (to allow long words to have longer alt. writings)\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Define dictionaries \n",
    "    # Dictionary assignining a unique integer to each input character\n",
    "    try:\n",
    "        u_characters = set(' '.join(X)) \n",
    "    except TypeError:\n",
    "        # Exception for TIMIT dataset (one phoneme is repr. by seq. of chars)\n",
    "        print(\"TypeError occurred.\")\n",
    "        u_characters = set([quant for seq in X for quant in seq])\n",
    "\n",
    "    char2numX = dict(zip(u_characters, range(1,len(u_characters)+1)))\n",
    "\n",
    "    # Dictionary assignining a unique integer to each phoneme\n",
    "    try:\n",
    "        v_characters = set(' '.join(Y)) \n",
    "    except TypeError:\n",
    "        print(\"TypeError occurred.\")\n",
    "        v_characters = set([quant for seq in Y for quant in seq])\n",
    "    char2numY = dict(zip(v_characters, range(1,len(v_characters)+1))) # Using 0 causes trouble for tf.edit_distance\n",
    "    \n",
    "    # 2. Padding\n",
    "    # Pad inputs\n",
    "    char2numX['<GO>'] = len(char2numX) + 1\n",
    "    char2numX['<PAD>'] = len(char2numX) + 1\n",
    "    mx_l_X = max([len(word) for word in X]) # longest input sequence\n",
    "    # Padd all X for the final form for the LSTM\n",
    "    x = [[char2numX['<GO>']] + [char2numX['<PAD>']]*(mx_l_X - len(word)) +[char2numX[char] for char in word] for word in X]\n",
    "    x = np.array(x) \n",
    "\n",
    "    # Pad targets\n",
    "    char2numY['<GO>'] = len(char2numY) + 1 # Define number denoting the response onset\n",
    "    char2numY['<PAD>'] = len(char2numY) + 1 \n",
    "    mx_l_Y = max([len(phon_seq) for phon_seq in Y]) # longest output sequence\n",
    "\n",
    "    y = [[char2numY['<GO>']] + pads*[char2numY['<PAD>']] + [char2numY['<PAD>']]*(mx_l_Y - len(ph_sq)) + [char2numY[phon] for phon in ph_sq] for ph_sq in Y]\n",
    "    y = np.array(y)\n",
    "\n",
    "    return ((x,y) , (char2numX,char2numY))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A a:\n",
      "aalen a:l\n",
      "aalglatt a:l#glat\n",
      "Aasgeier a:z#gai@r\n",
      "abaenderlich ap#End@r#lIx\n",
      "abbauwuerdig ap#bau#vYrdIx\n",
      "Abbrucharbeit ap#brEx#arbait\n",
      "Excluded 5593 words because they were too long (more than 15 phons)\n",
      "Size of dataset is 31675 samples\n",
      "29 {'y': 1, 's': 2, 'e': 3, 'w': 4, 'i': 5, 'k': 6, ' ': 7, 'm': 8, 'r': 9, 'q': 10, 'c': 11, 'p': 12, 'o': 13, 'u': 14, 'h': 15, 'd': 16, 'a': 17, 'n': 18, 'g': 19, 'z': 20, 'f': 21, 't': 22, 'b': 23, 'v': 24, 'l': 25, 'x': 26, 'j': 27, '<GO>': 28, '<PAD>': 29}\n",
      "40 {'y': 1, 's': 2, 'e': 3, '#': 4, '@': 5, 'i': 6, 'k': 7, ' ': 8, 'm': 9, 'U': 10, 'I': 11, 'r': 12, 'p': 13, 'S': 14, 'u': 15, 'o': 16, 'h': 17, '|': 18, 'd': 19, ':': 20, '/': 21, 'a': 22, 'E': 23, 'n': 24, 'g': 25, 'O': 26, 'z': 27, 'f': 28, 'Z': 29, 't': 30, 'b': 31, 'N': 32, 'v': 33, 'l': 34, 'Y': 35, 'x': 36, 'j': 37, '+': 38, '<GO>': 39, '<PAD>': 40}\n",
      "(31675, 20) (31675, 15)\n",
      "[28 29 29 29 29 29 29 29 29 29 29 29 29 29 17 23 25 14 21 22] [39 40 40 40 40 40 40 40 22 13  4 34 10 28 30]\n"
     ]
    }
   ],
   "source": [
    "words,phons = extract_celex(path)\n",
    "((phons_num, words_num), (phon_dict, word_dict)) = str_to_num_dataset(phons,words)\n",
    "    \n",
    "print(len(word_dict),word_dict)\n",
    "print(len(phon_dict),phon_dict)\n",
    "print(words_num.shape, phons_num.shape)\n",
    "print(words_num[321,:], phons_num[321,:])\n",
    "\n",
    "np.savez(\"data/celex.npz\", words=words_num, phons=phons_num, word_dict=word_dict, phon_dict=phon_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of phonetic dict is  40  and the keys are: \n",
      " ['y', 's', 'e', '#', '@', 'i', 'k', ' ', 'm', 'U', 'I', 'r', 'p', 'S', 'u', 'o', 'h', '|', 'd', ':', '/', 'a', 'E', 'n', 'g', 'O', 'z', 'f', 'Z', 't', 'b', 'N', 'v', 'l', 'Y', 'x', 'j', '+', '<GO>', '<PAD>']\n"
     ]
    }
   ],
   "source": [
    "data = np.load('data/celex.npz')\n",
    "sampa_dict = {key:data['phon_dict'].item().get(key) for key in data['phon_dict'].item()}\n",
    "sampa_keys = list(sampa_dict.keys()) # Has 43 keys originally, 40 after excluding {, ~, A\n",
    "print(\"Length of phonetic dict is \", len(sampa_dict), \" and the keys are: \\n\", sampa_keys) # Has 43 keys\n",
    "\n",
    "\n",
    "# Step 1: Make a sampa_ipa dict. How to: Go to SAMPA -> IPA tabelle, for every SAMPA char, check whether it \n",
    "# is in the CELEX korpus. If yes, look up example word from wiki in korpus and check whether it is the right sampa\n",
    "# sign. If yes, look up on wiktionary example word in IPA and check whether output sign is correct.\n",
    "\n",
    "sampa_ipa = dict()\n",
    "\n",
    "# Vowels\n",
    "sampa_ipa['i'] = 'i'\n",
    "sampa_ipa[':'] = 'ː'\n",
    "sampa_ipa['I'] = 'ɪ'\n",
    "sampa_ipa['e'] = 'e'\n",
    "sampa_ipa['E'] = 'ɛ'\n",
    "sampa_ipa['y'] = 'y'   # meaning a real ü like in kühl\n",
    "sampa_ipa['@'] = 'ə'\n",
    "sampa_ipa['a'] = 'a'\n",
    "sampa_ipa['u'] = 'u'\n",
    "sampa_ipa['U'] = 'ʊ'\n",
    "sampa_ipa['o'] = 'o'\n",
    "sampa_ipa['O'] = 'ɔ'\n",
    "\n",
    "\n",
    "# consonants\n",
    "sampa_ipa['p'] = 'p'\n",
    "sampa_ipa['b'] = 'b'\n",
    "sampa_ipa['t'] = 't'\n",
    "sampa_ipa['d'] = 'd'\n",
    "sampa_ipa['k'] = 'k'\n",
    "sampa_ipa['g'] = 'g'\n",
    "sampa_ipa['f'] = 'f'\n",
    "sampa_ipa['v'] = 'v'\n",
    "sampa_ipa['s'] = 's'\n",
    "sampa_ipa['z'] = 'z'\n",
    "sampa_ipa['S'] = 'ʃ'\n",
    "sampa_ipa['x'] = 'x'\n",
    "sampa_ipa['h'] = 'h'\n",
    "sampa_ipa['m'] = 'm'\n",
    "sampa_ipa['n'] = 'n'\n",
    "sampa_ipa['N'] = 'ŋ'\n",
    "sampa_ipa['l'] = 'l'\n",
    "sampa_ipa['r'] = 'r'\n",
    "sampa_ipa['j'] = 'j'\n",
    "sampa_ipa['Z'] = 'ʒ'\n",
    "sampa_ipa['+'] = ''  # meaning a bit unclear\n",
    "sampa_ipa['#'] = 'ˈ' # following syllabus carries primary intonation\n",
    "sampa_ipa['|'] = 'ø' # meaning a bit unclear\n",
    "sampa_ipa['/'] = 'œ' # usually SAMPA uses 9 instead of / for this \n",
    "sampa_ipa['Y'] = 'ʏ' # meaning more a 'oü' like in Müll\n",
    "\n",
    "# These are 37 keys only, so 6 are missing. Remaining ones are:\n",
    "\n",
    "# <GO>          not needed for alt. writing creation\n",
    "# <PAD>         not needed for alt. writing creation\n",
    "#    (SPACE)    not needed\n",
    "# {             excluded some foreign words\n",
    "# A             excluded some foreign words\n",
    "# ~             excluded some foreign words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now convert the SAMPA words into IPA words:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of IPA samples is 31675 . Some samples are: \n",
      "\n",
      " WORD         ===>        SAMPA        ===>        IPA\n",
      "toppsegel  =>  tOp#ze:g@l  =>  tɔpˈzeːgəl\n",
      "toppsgast  =>  tOp+s#gast  =>  tɔpsˈgast\n",
      "tor  =>  to:r  =>  toːr\n",
      "tor  =>  to:r  =>  toːr\n",
      "torbogen  =>  to:r#bi:g  =>  toːrˈbiːg\n",
      "torf  =>  tOrf  =>  tɔrf\n",
      "torferde  =>  tOrf#e:rd@  =>  tɔrfˈeːrdə\n",
      "torfgewinnung  =>  tOrf#g@vIn+UN  =>  tɔrfˈgəvɪnʊŋ\n",
      "torfig  =>  tOrf+Ix  =>  tɔrfɪx\n",
      "torfmoor  =>  tOrf#mo:r  =>  tɔrfˈmoːr\n"
     ]
    }
   ],
   "source": [
    "ipa = []\n",
    "for samp in phons:\n",
    "    s = []\n",
    "    for char in samp:\n",
    "        s.append(sampa_ipa[char])\n",
    "    ipa.append(''.join(s))\n",
    "print(\"Amount of IPA samples is\", len(words), \". Some samples are: \\n\")\n",
    "print(\" WORD         ===>        SAMPA        ===>        IPA\")\n",
    "for k in range(25100, 25110):\n",
    "    print(words[k],\" => \", phons[k],\" => \", ipa[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now define the second dictionary, mapping IPA signs to graphemes (according to [here](https://de.wiktionary.org/wiki/Verzeichnis:Deutsch/Phoneme_und_Grapheme) ) \n",
    "\n",
    "Go to every phon and write down all graphemes (copying table, excluding very weird graphemes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_values(['i', 'ː', 'ɪ', 'e', 'ɛ', 'y', 'ə', 'a', 'u', 'ʊ', 'o', 'ɔ', 'p', 'b', 't', 'd', 'k', 'g', 'f', 'v', 's', 'z', 'ʃ', 'x', 'h', 'm', 'n', 'ŋ', 'l', 'r', 'j', 'ʒ', '', 'ˈ', 'ø', 'œ', 'ʏ'])\n"
     ]
    }
   ],
   "source": [
    "print(sampa_ipa.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37 46\n",
      "ː\n",
      "\n",
      "ˈ\n",
      "ø\n"
     ]
    }
   ],
   "source": [
    "ipa_graph = dict()\n",
    "ipa_graph['t'] = ['t', 'd', 'tt'] # excluded dt (mostly in Stadt) and th (Methode) -> hard but not do more than 3 opts.\n",
    "ipa_graph['ə'] = ['e']\n",
    "ipa_graph['n'] = ['n', 'nn']\n",
    "ipa_graph['s'] = ['s', 'ss'] # excluded t for Patience, ce for Farce, zz for Jazz, ß (no occ.), c for Sauce, z for Quiz\n",
    "ipa_graph['a'] = ['a', 'ah']\n",
    "ipa_graph['r'] = ['r', 'rr'] # excluded rrh for Zirrhose/Myrrhe, rh for Rhythmus\n",
    "ipa_graph['l'] = ['l', 'll']\n",
    "ipa_graph['ɛ'] = ['e', 'ae']\n",
    "ipa_graph['f'] = ['f', 'v', 'ff'] # excluded ph for Physik\n",
    "ipa_graph['g'] = ['g', 'gh'] # excluded gg for Bagger\n",
    "ipa_graph['ɪ'] = ['i']\n",
    "ipa_graph['k'] = ['k', 'ck', 'c'] # Excluded cch for Zucchini, gg for Flaggschiff, qu for Boutique, kk for Akkordeon\n",
    "                  #  qu chars are usually kv ipa (tracked below), g for Krieg, ch for Chor\n",
    "ipa_graph['m'] = ['m', 'mm']\n",
    "ipa_graph['b'] = ['b', 'bb']\n",
    "ipa_graph['ʃ'] = ['sch', 's'] # excluded sk for Ski, sh for Sheriff, Show and ch for Recherche \n",
    "ipa_graph['d'] = ['d','dd']\n",
    "ipa_graph['p'] = ['p', 'b', 'pp'] # excluded bb for abebben or schrubben\n",
    "ipa_graph['ŋ'] = ['ng','n']\n",
    "ipa_graph['ɔ'] = ['o'] # excluded ch for Chauffeur (very rare exception)\n",
    "ipa_graph['v'] = ['w', 'v']\n",
    "ipa_graph['ʊ'] = ['u']\n",
    "ipa_graph['z'] = ['s'] # excluded zz for Blizzard, Puzzle and z for zoomen, bulldozer (since only in foreign words)\n",
    "ipa_graph['h'] = ['h']\n",
    "ipa_graph['i'] = ['i'] # excluding y (Baby/Party/Hockey) only 10 words in corpus\n",
    "ipa_graph['ʏ'] = ['ue', 'u'] # The corpus is weird here and writes Druck as drʏk, i.e. \"Drück\" rather than drʊk\n",
    "ipa_graph['x'] = ['ch']\n",
    "ipa_graph['e'] = ['e'] # excluded ee for Kaffee since IPA would be eː\n",
    "ipa_graph['j'] = ['j', 'y']\n",
    "ipa_graph['u'] = ['u'] # excluded ou like in Boutique\n",
    "ipa_graph['o'] = ['o'] # not needed anyways since o always followed by ː\n",
    "ipa_graph['œ'] = ['oe']\n",
    "ipa_graph['y'] = ['y']\n",
    "ipa_graph['ʒ'] = ['g', 'j'] # no wiki entry, self generated. For Garage or Jury\n",
    "\n",
    "\n",
    "# 2 character keys:\n",
    "ipa_graph['ts'] = ['z', 'ts', 'tz'] # excluded zz for Pizza/Skizze, c for circa, Penicillin, tts for trittst\n",
    "            # t for Aktion, Negation, Infektion, Proportion, ...\n",
    "ipa_graph['aː'] = ['a', 'ah', 'aa']\n",
    "ipa_graph['ai'] = ['ei', 'ai'] # excluded ail for Detail, aill for Medaillon, aille for Medaille and y for Nylon\n",
    "ipa_graph['iː'] = ['ie', 'i', 'ih'] # excluded ieh for Entziehung\n",
    "ipa_graph['eː'] = ['e', 'ee', 'eh'] # excluded et like in Bidet\n",
    "ipa_graph['ɛː'] = ['ae', 'aeh']\n",
    "ipa_graph['uː'] = ['u', 'uh'] # excluded ou like in Ragout, Limousine and oo like on zoomen/Cartoon\n",
    "ipa_graph['oː'] = ['o', 'oh', 'oo'] # excluded au for aubergine/sauce and eau for plateau, Niveau\n",
    "ipa_graph['yː'] = ['ue', 'ueh', 'y'] # excluded uet like in Debüt and u like in deja-vu\n",
    "ipa_graph['ɔy'] = ['eu', 'aeu'] # instead of what wiki calls ɔɪ̯, excluded oi for Boiler and oy for Boykott\n",
    "ipa_graph['ks'] = ['chs', 'x', 'ks'] #excluded gs like in legst/bugsieren, ggs like in eggst (?), cks like in zwecks, gs (legst)\n",
    "ipa_graph['øː'] = ['oe', 'oeh'] # excluded eu like in Ingenieur and eue like in Queue (?)\n",
    "ipa_graph['kv'] = ['qu']\n",
    "\n",
    "\n",
    "print(len(sampa_ipa), len(ipa_graph))\n",
    "# We had 36 keys in sampa_ipa dict, now we have 46 already in ipa_graph due to 2-phoneme-groups\n",
    "# But still there are one-char-values in sampa_ipa which are not keys in ipa_graph. Let us print them:\n",
    "for key in sampa_ipa.values():\n",
    "    if key not in ipa_graph.keys():\n",
    "        print(key)\n",
    "        \n",
    "# Okay 4 are missing:\n",
    "ipa_graph['ː'] = [''] # not needed anyways since ː always occurs after vowel\n",
    "ipa_graph['ˈ'] = [''] # Just a pronounciation symbol, does not carry meaning for spelling\n",
    "# Then the empty string '' is not needed as key\n",
    "# Then ø only occurs followed by a ː\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pretty tough to draw the boundary between which phons rarely but \"regularly\" translate to grapheme strings ([k] to kk like in Akkordeon, Mokka is still regular?) whereas [ʃ] (sch) to ch like in Champagner, Recherche or Lunch is irregular?\n",
    "\n",
    "## Use the IPA->Graphem dict to create alternative writings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_word(word, ipa_graph):\n",
    "    \"\"\"\n",
    "    Splits up an IPA word into a list of lists each with the possible replacement grapheme for each phoneme\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    WORD       {list} in IPA notation\n",
    "    IPA_GRAPH  {dict} mapping IPA symbols to possible grapheme sequences\n",
    "    \n",
    "    Returns:\n",
    "    ---------\n",
    "    CHARS      {list} containing lists with possible grapheme sequences\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    chars = []\n",
    "    single_key = True\n",
    "    for ind in range(len(word)-1):\n",
    "        \n",
    "        if single_key: \n",
    "            if word[ind:ind+2] in ipa_graph:\n",
    "                chars.append(ipa_graph[word[ind:ind+2]])\n",
    "                single_key = False\n",
    "            else:\n",
    "                chars.append(ipa_graph[word[ind]])\n",
    "        else:\n",
    "            single_key = True\n",
    "            \n",
    "    chars.append(ipa_graph[word[ind+1]])\n",
    "    \n",
    "    return chars\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The cell below generates the alternative writings (takes some time...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1,0,0,1,0])\n",
    "d = dict()\n",
    "d[str(a)] = 12\n",
    "\n",
    "e = np.random.random((2,2,2,2,2))\n",
    "f = [np.asscalar(aa) for aa in a]\n",
    "print(f)\n",
    "d[tuple(a)] = 121\n",
    "print(d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "d[np.str(a)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "e[a[0],a[1],a[2],a[3], a[4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "d[tuple(a)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_writings = []\n",
    "m = 0\n",
    "\n",
    "for ind,ip in enumerate(ipa):\n",
    "    if ind % 200 == 0:\n",
    "        print(\"Currently examining word \", ind)\n",
    "        \n",
    "    word_lists = split_word(ip, ipa_graph)\n",
    "    alt_write_raw = list(itertools.product(*word_lists))\n",
    "    alt_write = [''.join(a) for a in alt_write_raw]\n",
    "    try:\n",
    "        alt_write.remove(words[ind])\n",
    "    except ValueError:\n",
    "        _ = 1\n",
    "        \n",
    "    all_writings.append(alt_write)\n",
    "    \n",
    "    if len(alt_write) > m:\n",
    "        print(m,ind)\n",
    "        m = len(alt_write)\n",
    "        \n",
    "        \n",
    "print(\"DONE! Alternative writings generated for \", ind, \"words. Resulting list has\", len(all_writings), 'entries.')\n",
    "np.save('celex_all_writings', all_writings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract childlex data from downloaded CSV and clean it up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of words in childlex 9769\n",
      "Set of characters in childlex words is ['a', 'e', 'c', 'h', 'z', 'n', 'd', 'f', 'm', 'l', 'i', 'r', 'g', 's', 't', 'u', 'o', 'b', 'k', 'p', 'q', 'w', 'v', 'x', 'j', 'y']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "path = 'data/childlex_6-8_lemmata.csv'\n",
    "forbidden = \"_,.!?*:=&%- /\\\\–•»«…›‹()[]{}’'\"\n",
    "\n",
    "\n",
    "def rem_um(word):\n",
    "    \"\"\"\n",
    "    Converts a word with German umlauts (ü,ä,ö) into a word without\n",
    "    \"\"\"\n",
    "        \n",
    "    umlautfree = str()\n",
    "    for char in word:\n",
    "        if char == 'ä':\n",
    "            umlautfree += 'ae'\n",
    "        elif char == 'ü':\n",
    "            umlautfree += 'ue'\n",
    "        elif char == 'ö':\n",
    "            umlautfree += 'oe'\n",
    "        elif char == 'ß':\n",
    "            umlautfree += 'ss'\n",
    "        else:\n",
    "            umlautfree += char\n",
    "    return umlautfree\n",
    "\n",
    "\n",
    "with open(path, 'r') as csvfile:\n",
    "    \n",
    "    raw_file = csv.reader(csvfile,delimiter=';')\n",
    "    raw_data = []\n",
    "    \n",
    "    \n",
    "    for line in raw_file:\n",
    "        umlautfree = rem_um(line[0].lower())\n",
    "        \n",
    "       # data cleanup\n",
    "        has_digit = any(char.isdigit() for char in umlautfree)\n",
    "        has_sonderz = any(char in forbidden for char in umlautfree)\n",
    "        \n",
    "        if not has_digit and not has_sonderz:\n",
    "        \n",
    "            raw_data.append(umlautfree)\n",
    "        \n",
    "        \n",
    "    \n",
    "childlex_words_all = raw_data\n",
    "print(\"Amount of words in childlex\", len(childlex_words_all))\n",
    "    \n",
    "chars = []\n",
    "for word in childlex_words_all:\n",
    "    for char in word:\n",
    "        if char not in chars:\n",
    "            chars.append(char)\n",
    "    \n",
    "print(\"Set of characters in childlex words is\", chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look up phonetic sequences in CELEX Database. \n",
    "#### Problem: We do not have phonetic sequences yet. Idea: Retrieve as many phonetic sequences as possible from the CELEX database, convert this database into numerical values and save it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "childlex_phons = []\n",
    "childlex_words = []\n",
    "for word in childlex_words_all:\n",
    "    if word in words:\n",
    "        ind = words.index(word)\n",
    "        childlex_phons.append(phons[ind])\n",
    "        childlex_words.append(words[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From the 9769 words in CHILDLEX database 5885 were succesfully retrieved\n"
     ]
    }
   ],
   "source": [
    "a =  childlex_phons.count('')\n",
    "print(\"From the\", len(childlex_words_all),\"words in CHILDLEX database\", len(childlex_words),\"were succesfully retrieved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29 {'y': 1, 's': 2, 'e': 3, 'i': 4, 'w': 5, 'k': 6, ' ': 7, 'm': 8, 'r': 9, 'q': 10, 'p': 11, 'c': 12, 'o': 13, 'u': 14, 'h': 15, 'd': 16, 'a': 17, 'n': 18, 'g': 19, 'z': 20, 'f': 21, 't': 22, 'b': 23, 'v': 24, 'l': 25, 'x': 26, 'j': 27, '<GO>': 28, '<PAD>': 29}\n",
      "40 {'y': 1, 's': 2, 'e': 3, '#': 4, '@': 5, 'i': 6, 'k': 7, ' ': 8, 'm': 9, 'U': 10, 'I': 11, 'r': 12, 'p': 13, 'S': 14, 'h': 15, 'u': 16, '|': 17, 'o': 18, 'd': 19, ':': 20, '/': 21, 'a': 22, 'E': 23, 'n': 24, 'g': 25, 'O': 26, 'z': 27, 'f': 28, 'Z': 29, 't': 30, 'b': 31, 'N': 32, 'v': 33, 'l': 34, 'Y': 35, 'x': 36, 'j': 37, '+': 38, '<GO>': 39, '<PAD>': 40}\n",
      "(5885, 20) (5885, 15)\n",
      "[28 29 29 29 29 29 29 29 29 29 29 29 29 29 17 14  2  2  3 18] [39 40 40 40 40 40 40 40 40 22 16  2 38  5 24]\n"
     ]
    }
   ],
   "source": [
    "# Save the childlex dataset\n",
    "((phons_num, words_num), (phon_dict, word_dict)) = str_to_num_dataset(childlex_phons,childlex_words)\n",
    "print(len(word_dict),word_dict)\n",
    "print(len(phon_dict),phon_dict)\n",
    "print(words_num.shape, phons_num.shape)\n",
    "print(words_num[321,:], phons_num[321,:])\n",
    "np.savez(\"data/childlex.npz\", words=words_num, phons=phons_num, word_dict=word_dict, phon_dict=phon_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract data from the FIBEL dataset\n",
    "##### Focus on the \"Mia and Mo\" Fibel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has 175 from which 115 could be retrieved from celex\n"
     ]
    }
   ],
   "source": [
    "fibel_path = \"/Users/jannisborn/Dropbox/GitHub/LSTM/LdS_bLSTM/Code/data/Fibelwörter.txt\"\n",
    "\n",
    "\n",
    "with open(fibel_path, 'r') as txtfile:\n",
    "    fibel_words = []\n",
    "    for line in txtfile.read().split(','):\n",
    "        # Use lowercase letters only, remove leading whitespacetake care of line breaks.\n",
    "        if '\\n' in line:    \n",
    "            for item in line.split('\\n'):\n",
    "                if item.lstrip().lower() not in fibel_words:\n",
    "                    fibel_words.append(rem_um(item.lstrip().lower()))\n",
    "        elif line.lstrip().lower() not in fibel_words:\n",
    "            fibel_words.append(rem_um(line.lstrip().lower()))\n",
    "        \n",
    "lektions_inds = [9,14,20,28,36,46,58,77,99,121,154,174]\n",
    "\n",
    "fibel_phons = []\n",
    "k=0\n",
    "for w in fibel_words:\n",
    "    if w in words:\n",
    "        ind = words.index(w)\n",
    "        fibel_phons.append(phons[ind])\n",
    "    else:\n",
    "        fibel_phons.append('NO')\n",
    "        k +=1\n",
    "print(\"The dataset has \"+str(len(fibel_words))+\" from which \"+str(len(fibel_words)-k)+\" could be retrieved from celex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate the SAMPA spellings for the missing words. First step: Copy the IPA values from wiktionary + convert IPA into SAMPA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YAY, all words have phonetic transcript\n"
     ]
    }
   ],
   "source": [
    "ipa_sampa = dict(zip(sampa_ipa.values(), sampa_ipa.keys()))\n",
    "\n",
    "fibel_phons[fibel_words.index('mia')] = ''.join([ipa_sampa[w] for w in 'miːa'])\n",
    "fibel_phons[fibel_words.index('mo')] = ''.join([ipa_sampa[w] for w in 'mo'])\n",
    "fibel_phons[fibel_words.index('mimi')] = ''.join([ipa_sampa[w] for w in 'mimi'])\n",
    "fibel_phons[fibel_words.index('im')] = ''.join([ipa_sampa[w] for w in 'ɪm'])\n",
    "fibel_phons[fibel_words.index('am')] = ''.join([ipa_sampa[w] for w in 'am'])\n",
    "fibel_phons[fibel_words.index('momo')] = ''.join([ipa_sampa[w] for w in 'momo'])\n",
    "fibel_phons[fibel_words.index('omi')] = ''.join([ipa_sampa[w] for w in 'oːmiː'])\n",
    "fibel_phons[fibel_words.index('radio')] = ''.join([ipa_sampa[w] for w in 'reɪdɪəʊ'])\n",
    "fibel_phons[fibel_words.index('sissi')] = ''.join([ipa_sampa[w] for w in 'zɪsɪː'])\n",
    "fibel_phons[fibel_words.index('susi')] = ''.join([ipa_sampa[w] for w in 'susi'])\n",
    "fibel_phons[fibel_words.index('oli')] = ''.join([ipa_sampa[w] for w in 'ɔliː'])\n",
    "fibel_phons[fibel_words.index('salami')] = ''.join([ipa_sampa[w] for w in 'zaˈlaːmi'])\n",
    "fibel_phons[fibel_words.index('ist')] = ''.join([ipa_sampa[w] for w in 'ɪst'])\n",
    "fibel_phons[fibel_words.index('tim')] = ''.join([ipa_sampa[w] for w in 'tɪm'])\n",
    "fibel_phons[fibel_words.index('tom')] = ''.join([ipa_sampa[w] for w in 'tɔm'])\n",
    "fibel_phons[fibel_words.index('mario')] = ''.join([ipa_sampa[w] for w in 'ˈmaːrio'])\n",
    "fibel_phons[fibel_words.index('isa')] = ''.join([ipa_sampa[w] for w in 'iːza'])\n",
    "fibel_phons[fibel_words.index('maria')] = ''.join([ipa_sampa[w] for w in 'maˈriːa'])\n",
    "fibel_phons[fibel_words.index('rosarot')] = ''.join([ipa_sampa[w] for w in 'ˈroːzaˈroːt'])\n",
    "fibel_phons[fibel_words.index('nimm')] = ''.join([ipa_sampa[w] for w in 'nɪm'])\n",
    "fibel_phons[fibel_words.index('nimmt')] = ''.join([ipa_sampa[w] for w in 'nɪmt'])\n",
    "fibel_phons[fibel_words.index('nina')] = ''.join([ipa_sampa[w] for w in 'niːna'])\n",
    "fibel_phons[fibel_words.index('rosinen')] = ''.join([ipa_sampa[w] for w in 'roˈziːnən'])\n",
    "fibel_phons[fibel_words.index('anna')] = ''.join([ipa_sampa[w] for w in 'ˈana'])\n",
    "fibel_phons[fibel_words.index('bananen')] = ''.join([ipa_sampa[w] for w in 'baˈnaːnən'])\n",
    "fibel_phons[fibel_words.index('birnen')] = ''.join([ipa_sampa[w] for w in 'bɪrnən'])\n",
    "fibel_phons[fibel_words.index('nuesse')] = ''.join([ipa_sampa[w] for w in 'nʏsə'])\n",
    "fibel_phons[fibel_words.index('orangen')] = ''.join([ipa_sampa[w] for w in 'oˈraŋʒn'])\n",
    "fibel_phons[fibel_words.index('weintrauben')] = ''.join([ipa_sampa[w] for w in 'ˈvaintraʊbn'])\n",
    "fibel_phons[fibel_words.index('maroni')] = ''.join([ipa_sampa[w] for w in 'maˈroːni'])\n",
    "fibel_phons[fibel_words.index('erna')] = ''.join([ipa_sampa[w] for w in 'ˈɛrna'])\n",
    "fibel_phons[fibel_words.index('die')] = ''.join([ipa_sampa[w] for w in 'diː'])\n",
    "fibel_phons[fibel_words.index('das')] = ''.join([ipa_sampa[w] for w in 'das'])\n",
    "fibel_phons[fibel_words.index('sind')] = ''.join([ipa_sampa[w] for w in 'zɪnt'])\n",
    "fibel_phons[fibel_words.index('domino')] = ''.join([ipa_sampa[w] for w in 'ˈdoːmino'])\n",
    "fibel_phons[fibel_words.index('indianer')] = ''.join([ipa_sampa[w] for w in 'ɪnˈdiaːnər'])\n",
    "fibel_phons[fibel_words.index('tonio')] = ''.join([ipa_sampa[w] for w in 'tɔnioː'])\n",
    "fibel_phons[fibel_words.index('sagt')] = ''.join([ipa_sampa[w] for w in 'zaːkt'])\n",
    "fibel_phons[fibel_words.index('italien')] = ''.join([ipa_sampa[w] for w in 'iˈtaːliən'])\n",
    "fibel_phons[fibel_words.index('ute')] = ''.join([ipa_sampa[w] for w in 'uːtə'])\n",
    "fibel_phons[fibel_words.index('umarmt')] = ''.join([ipa_sampa[w] for w in 'ʊmˈarmt'])\n",
    "fibel_phons[fibel_words.index('runter')] = ''.join([ipa_sampa[w] for w in 'rʊntər'])\n",
    "fibel_phons[fibel_words.index('turnen')] = ''.join([ipa_sampa[w] for w in 'tʊrnən'])\n",
    "fibel_phons[fibel_words.index('dem')] = ''.join([ipa_sampa[w] for w in 'deːm'])\n",
    "fibel_phons[fibel_words.index('rennt')] = ''.join([ipa_sampa[w] for w in 'rɛnt'])\n",
    "fibel_phons[fibel_words.index('ene')] = ''.join([ipa_sampa[w] for w in 'eːnə'])\n",
    "fibel_phons[fibel_words.index('mene')] = ''.join([ipa_sampa[w] for w in 'meːnə'])\n",
    "fibel_phons[fibel_words.index('mu')] = ''.join([ipa_sampa[w] for w in 'muː'])\n",
    "fibel_phons[fibel_words.index('turnt')] = ''.join([ipa_sampa[w] for w in 'ˈtʊrnt'])\n",
    "fibel_phons[fibel_words.index('miauen')] = ''.join([ipa_sampa[w] for w in 'miˈaʊən'])\n",
    "fibel_phons[fibel_words.index('raus')] = ''.join([ipa_sampa[w] for w in 'raʊs'])\n",
    "fibel_phons[fibel_words.index('mauert')] = ''.join([ipa_sampa[w] for w in 'tɔm'])\n",
    "fibel_phons[fibel_words.index('saust')] = ''.join([ipa_sampa[w] for w in 'zaʊst'])\n",
    "fibel_phons[fibel_words.index('rosaroten')] = ''.join([ipa_sampa[w] for w in 'roːzaˈroːtn'])\n",
    "fibel_phons[fibel_words.index('martin')] = ''.join([ipa_sampa[w] for w in 'ˈmartiːn'])\n",
    "fibel_phons[fibel_words.index('martins')] = ''.join([ipa_sampa[w] for w in 'ˈmartiːns'])\n",
    "fibel_phons[fibel_words.index('eine')] = ''.join([ipa_sampa[w] for w in 'ˈaɪnə'])\n",
    "fibel_phons[fibel_words.index('miaut')] = ''.join([ipa_sampa[w] for w in 'miˈaʊt'])\n",
    "fibel_phons[fibel_words.index('otto')] = ''.join([ipa_sampa[w] for w in 'ˈɔto'])\n",
    "fibel_phons[fibel_words.index('isst')] = ''.join([ipa_sampa[w] for w in 'ɪst'])\n",
    "\n",
    "try:\n",
    "    fibel_phons.index('NO')\n",
    "except ValueError:\n",
    "    print(\"YAY, all words have phonetic transcript\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Fibel dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "((phons_num, words_num), (phon_dict, word_dict)) = str_to_num_dataset(childlex_phons,childlex_words)\n",
    "np.savez(\"data/childlex.npz\", words=words_num, phons=phons_num, word_dict=word_dict, phon_dict=phon_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insight 1: If you have k different classes, then k is the third \n",
    "#    dimension of the logits and then the target matrix must not \n",
    "#    contain values higher than k-1 (i.e. labels are 0, ..., k)\n",
    "\n",
    "# Insight 2: sequence_loss expects unnormalized logits (BEFORE softmax!)\n",
    "# -> This makes testing for me harder, since I cannot simply use values\n",
    "# with a sum over 1 (will get normalized)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
